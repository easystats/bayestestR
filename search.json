[{"path":[]},{"path":"https://easystats.github.io/bayestestR/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"https://easystats.github.io/bayestestR/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others’ private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"https://easystats.github.io/bayestestR/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"https://easystats.github.io/bayestestR/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"https://easystats.github.io/bayestestR/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement dom.makowski@gmail.com. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"https://easystats.github.io/bayestestR/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"https://easystats.github.io/bayestestR/CODE_OF_CONDUCT.html","id":"id_1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"https://easystats.github.io/bayestestR/CODE_OF_CONDUCT.html","id":"id_2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"https://easystats.github.io/bayestestR/CODE_OF_CONDUCT.html","id":"id_3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"https://easystats.github.io/bayestestR/CODE_OF_CONDUCT.html","id":"id_4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"https://easystats.github.io/bayestestR/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.1, available https://www.contributor-covenant.org/version/2/1/code_of_conduct.html. Community Impact Guidelines inspired [Mozilla’s code conduct enforcement ladder][https://github.com/mozilla/inclusion]. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https://www.contributor-covenant.org/translations.","code":""},{"path":"https://easystats.github.io/bayestestR/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contribution Guidelines","title":"Contribution Guidelines","text":"easystats guidelines 0.1.0 people much welcome contribute code, documentation, testing suggestions. package aims beginner-friendly. Even ’re new open-source way life, new coding github stuff, encourage try submitting pull requests (PRs). “’d like help, ’m good enough programming yet” ’s alright, don’t worry! can always dig code, documentation tests. always typos fix, docs improve, details add, code lines document, tests add… Even smaller PRs appreciated. “’d like help, don’t know start” can look around issue section find features / ideas / bugs start working . can also open new issue just say ’re , interested helping . might ideas adapted skills. “’m sure suggestion idea worthwile” Enough impostor syndrom! suggestions opinions good, even ’s just thought , ’s always good receive feedback. “waste time ? get credit?” Software contributions getting valued academic world, good time collaborate us! Authors substantial contributions added within authors list. ’re also keen including eventual academic publications. Anyway, starting important! enter whole new world, new fantastic point view… fork repo, changes submit . work together make best :)","code":""},{"path":"https://easystats.github.io/bayestestR/CONTRIBUTING.html","id":"code","dir":"","previous_headings":"","what":"Code","title":"Contribution Guidelines","text":"Please document comment code, purpose step (code line) stated clear understandable way. submitting change, please read R style guide particular easystats convention code-style keep consistency code formatting. Regarding style guide, note exception: put readability clarity everything. Thus, like underscores full names (prefer model_performance modelperf interpret_odds_logistic intoddslog). start code, make sure ’re dev branch (“advanced”). , can create new branch named feature (e.g., feature_lightsaber) changes. Finally, submit branch merged dev branch. , every now , dev branch merge master, new package version.","code":""},{"path":"https://easystats.github.io/bayestestR/CONTRIBUTING.html","id":"checks-to-do-before-submission","dir":"","previous_headings":"","what":"Checks to do before submission","title":"Contribution Guidelines","text":"Make sure documentation (roxygen) good Make sure add tests new functions Run: styler::style_pkg(): Automatic style formatting lintr::lint_package(): Style checks devtools::check(): General checks","code":""},{"path":"https://easystats.github.io/bayestestR/CONTRIBUTING.html","id":"useful-materials","dir":"","previous_headings":"","what":"Useful Materials","title":"Contribution Guidelines","text":"Understanding GitHub flow","code":""},{"path":"https://easystats.github.io/bayestestR/PULL_REQUEST_TEMPLATE.html","id":null,"dir":"","previous_headings":"","what":"Description","title":"Description","text":"PR aims adding feature…","code":""},{"path":"https://easystats.github.io/bayestestR/PULL_REQUEST_TEMPLATE.html","id":"proposed-changes","dir":"","previous_headings":"","what":"Proposed Changes","title":"Description","text":"changed foo function …","code":""},{"path":"https://easystats.github.io/bayestestR/articles/bayes_factors.html","id":"the-bayes-factor","dir":"Articles","previous_headings":"","what":"The Bayes Factor","title":"Bayes Factors","text":"Bayes Factors (BFs) indices relative evidence one “model” another. role hypothesis testing index, Bayesian framework \\(p\\)-value classical/frequentist framework. significance-based testing, \\(p\\)-values used assess unlikely observed data null hypothesis true, Bayesian model selection framework, Bayes factors assess evidence different models, model corresponding specific hypothesis. According Bayes’ theorem, can update prior probabilities model \\(M\\) (\\(P(M)\\)) posterior probabilities (\\(P(M|D)\\)) observing datum \\(D\\) accounting probability observing datum given model (\\(P(D|M)\\), also known likelihood): \\[ P(M|D) = \\frac{P(D|M)\\times P(M)}{P(D)} \\] Using equation, can compare probability-odds two models: \\[ \\underbrace{\\frac{P(M_1|D)}{P(M_2|D)}}_{\\text{Posterior Odds}} = \\underbrace{\\frac{P(D|M_1)}{P(D|M_2)}}_{\\text{Likelihood Ratio}} \\times \\underbrace{\\frac{P(M_1)}{P(M_2)}}_{\\text{Prior Odds}} \\] likelihood ratio (middle term) Bayes factor - factor prior odds updated observing data posterior odds. Thus, Bayes factors can calculated two ways: ratio quantifying relative probability observed data two models. (contexts, probabilities also called marginal likelihoods.) \\[ BF_{12}=\\frac{P(D|M_1)}{P(D|M_2)} \\] degree shift prior beliefs relative credibility two models (since can computed dividing posterior odds prior odds). \\[ BF_{12}=\\frac{Posterior~Odds_{12}}{Prior~Odds_{12}} \\] provide functions computing Bayes factors two different contexts: testing single parameters (coefficients) within model comparing statistical models ","code":""},{"path":"https://easystats.github.io/bayestestR/articles/bayes_factors.html","id":"bayesfactor_parameters","dir":"Articles","previous_headings":"","what":"Testing Models’ Parameters with Bayes Factors","title":"Bayes Factors","text":"Bayes factor single parameter can used answer question: “Given observed data, null hypothesis absence effect become less credible?” Bayesian analysis Students’ (1908) Sleep data set. Let’s use Students’ (1908) Sleep data set (data(\"sleep\")). data comes study participants administered drug researchers assessed extra hours sleep participants slept afterwards. try answering following research question using Bayes factors: Given observed data, hypothesis drug (effect group) effect numbers hours extra sleep (variable extra) become less credible?  boxplot suggests second group higher number hours extra sleep. much? Let’s fit simple Bayesian linear model, prior \\(b_{group} \\sim N(0, 3)\\) (.e. prior follows Gaussian/normal distribution \\(mean = 0\\) \\(SD = 3\\)), using rstanarm package:","code":"set.seed(123) library(rstanarm)  model <- stan_glm(   formula = extra ~ group,   data = sleep,   prior = normal(0, 3, autoscale = FALSE),   chains = 10, iter = 5000, warmup = 1000 )"},{"path":"https://easystats.github.io/bayestestR/articles/bayes_factors.html","id":"testing-against-a-null-region","dir":"Articles","previous_headings":"Testing Models’ Parameters with Bayes Factors","what":"Testing against a null-region","title":"Bayes Factors","text":"One way operationlizing null-hypothesis setting null region, effect falls within interval practically equivalent null (Kruschke, 2010). case, means defining range effects consider equal drug effect . can compute prior probability drug’s effect falling outside null-region, prior probability drug’s effect falling within null-region get prior odds. Say effect smaller hour extra sleep practically equivalent effect , define prior odds : \\[ \\frac {P(b_{drug} \\notin [-1, 1])} {P(b_{drug} \\[-1, 1])} \\] Given prior normal distribution centered 0 hours scale (SD) 3 hours, priors look like :  prior odds 2.8. looking posterior distribution, can now compute posterior probability drug’s effect falling outside null-region, posterior probability drug’s effect falling within null-region get posterior odds: \\[ \\frac {P(b_{drug} \\notin [-1,1] | Data)} {P(b_{drug} \\[-1,1] | Data)} \\]  can see center posterior distribution shifted away 0 (~1.5). Likewise, posterior odds 2.5, seems favor effect non-null. , mean data support alternative null? Hard say, since even data observed, priors already favored alternative - need take priors account ! Let’s compute Bayes factor change prior odds posterior odds: \\(BF_{10} = Odds_{posterior} / Odds_{prior} = 0.9\\)! BF indicates data provide 1/0.9 = 1.1 times evidence effect drug practically nothing drug clinically significant effect. Thus, although center distribution shifted away 0, posterior distribution seems favor non-null effect drug, seems given observed data, probability mass overall shifted closer null interval, making values null interval probable! (see Non-overlapping Hypotheses Morey & Rouder, 2011) can achieved function bayesfactor_parameters(), computes Bayes factor model’s parameters: can also plot using see package:  Note interpretation guides Bayes factors can found effectsize package:","code":"My_first_BF <- bayesfactor_parameters(model, null = c(-1, 1)) My_first_BF > Bayes Factor (Null-Interval) >  > Parameter   |    BF > ------------------- > (Intercept) | 0.098 > group2      | 0.888 >  > * Evidence Against The Null: [-1.000, 1.000] library(see) plot(My_first_BF) effectsize::interpret_bf(exp(My_first_BF$log_BF[2]), include_value = TRUE) > [1] \"anecdotal evidence (BF = 1/1.13) against\" > (Rules: jeffreys1961)"},{"path":"https://easystats.github.io/bayestestR/articles/bayes_factors.html","id":"testing-against-the-point-null-0","dir":"Articles","previous_headings":"Testing Models’ Parameters with Bayes Factors","what":"Testing against the point-null (0)","title":"Bayes Factors","text":"don’t know region practically equivalent 0? just want null exactly zero? problem - width null region shrinks point, change prior probability posterior probability null can estimated comparing density null value two distributions.1 ratio called Savage-Dickey ratio, added benefit also approximation Bayes factor comparing estimated model model parameter interest restricted point-null: “[…] Bayes factor \\(H_0\\) versus \\(H_1\\) obtained analytically integrating model parameter \\(\\theta\\). However, Bayes factor may likewise obtained considering \\(H_1\\), dividing height posterior \\(\\theta\\) height prior \\(\\theta\\), point interest.” (Wagenmakers, Lodewyckx, Kuriyal, & Grasman, 2010)","code":"My_second_BF <- bayesfactor_parameters(model, null = 0) My_second_BF > Bayes Factor (Savage-Dickey density ratio) >  > Parameter |   BF > ---------------- > group2    | 1.26 >  > * Evidence Against The Null: 0 plot(My_second_BF)"},{"path":"https://easystats.github.io/bayestestR/articles/bayes_factors.html","id":"directional-hypotheses","dir":"Articles","previous_headings":"Testing Models’ Parameters with Bayes Factors","what":"Directional hypotheses","title":"Bayes Factors","text":"can also compute Bayes factors directional hypotheses (“one sided”), prior hypotheses direction effect. can done setting order restriction prior distribution (results order restriction posterior distribution) alternative (Morey & Wagenmakers, 2014). example, prior hypothesis drug positive effect number sleep hours, alternative restricted region right null (point interval):  can see, given priori assumption direction effect (effect positive), presence effect 2.8 times likely absence effect, given observed data (data 2.8 time probable \\(H_1\\) \\(H_0\\)). indicates , given observed data, priori hypothesis, posterior mass shifted away null value, giving evidence null (note Bayes factor 2.8 still considered quite weak evidence). Thanks flexibility Bayesian framework, also possible compute Bayes factor dividing hypotheses - , null alternative complementary, opposing one-sided hypotheses (Morey & Wagenmakers, 2014). example, compared alternative \\(H_A\\): drug positive effects null \\(H_0\\): drug effect. can also compare instead alternative complementary hypothesis: \\(H_{-}\\): drug negative effects.  can see test produces even stronger (conclusive) evidence one-sided vs. point-null test! indeed, rule thumb, specific two hypotheses , distinct one another, power Bayes factor ! 2 Thanks transitivity Bayes factors, can also use bayesfactor_parameters() compare even types hypotheses, trickery. example: \\[ \\underbrace{BF_{0<b<1\\text{ vs. }b=0}}_{\\text{range vs. point}} = \\underbrace{BF_{b<0\\text{ vs. }b=0}}_{\\text{directional vs. point}} / \\underbrace{BF_{b<0\\text{ vs. }0<b<1}}_{\\text{directional vs. range}} \\] NOTE: See Testing Contrasts appendix .","code":"test_group2_right <- bayesfactor_parameters(model, direction = \">\") test_group2_right > Bayes Factor (Savage-Dickey density ratio) >  > Parameter |   BF > ---------------- > group2    | 2.42 >  > * Evidence Against The Null: 0 > *                 Direction: Right-Sided test plot(test_group2_right) test_group2_dividing <- bayesfactor_parameters(model, null = c(-Inf, 0)) test_group2_dividing > Bayes Factor (Null-Interval) >  > Parameter |    BF > ----------------- > group2    | 20.75 >  > * Evidence Against The Null: [-Inf, 0.000] plot(test_group2_dividing)"},{"path":"https://easystats.github.io/bayestestR/articles/bayes_factors.html","id":"si","dir":"Articles","previous_headings":"Testing Models’ Parameters with Bayes Factors","what":"Support intervals","title":"Bayes Factors","text":"far ’ve seen Bayes factors quantify relative support competing hypotheses. However, can also ask: Upon observing data, credibility parameter’s values increased (decreased)? example, ’ve seen point null become somewhat less credible observing data, might also ask values gained credibility given observed data?. resulting range values called support interval indicates values supported data (Wagenmakers, Gronau, Dablander, & Etz, 2018). can comparing prior posterior distributions checking posterior densities higher prior densities. bayestestR, can achieved si() function: argument BF = 1 indicates want interval contain values gained support factor least 1 (, support ). Visually, can see credibility values within interval increased (likewise credibility values outside interval decreased):  can also see support interval (just barely) excludes point null (0) - whose credibility ’ve already seen decreased observed data. emphasizes relationship support interval Bayes factor: “interpretation intervals analogous frequentist confidence interval contains parameter values rejected tested level \\(\\alpha\\). instance, BF = 1/3 support interval encloses values theta updating factor stronger 3 .” (Wagenmakers et al., 2018) Thus, choice BF (level support interval indicate) depends want interval represent: \\(BF = 1\\) contains values whose credibility merely decreased observing data. \\(BF > 1\\) contains values received impressive support data. \\(BF < 1\\) contains values whose credibility impressively decreased observing data. Testing values outside interval produce Bayes factor larger \\(1/BF\\) support alternative.","code":"my_first_si <- si(   posterior = data.frame(group2 = posterior),   prior = data.frame(group2 = prior),   BF = 1 )  print(my_first_si) > Support Interval >  > Parameter |    BF = 1 SI > ------------------------ > group2    | [0.15, 3.04] plot(my_first_si)"},{"path":"https://easystats.github.io/bayestestR/articles/bayes_factors.html","id":"bayesfactor_models","dir":"Articles","previous_headings":"","what":"Comparing Models using Bayes Factors","title":"Bayes Factors","text":"Bayes factors can also used compare statistical models. statistical context, answer following question: model observed data probable? words, model likely produced observed data? usually done comparing marginal likelihoods two models. case, Bayes factor measure relative evidence one model . Let’s use Bayes factors model comparison find model best describes length iris’ sepal using iris data set.","code":""},{"path":"https://easystats.github.io/bayestestR/articles/bayes_factors.html","id":"for-bayesian-models-brms-and-rstanarm","dir":"Articles","previous_headings":"Comparing Models using Bayes Factors","what":"For Bayesian models (brms and rstanarm)","title":"Bayes Factors","text":"Note: order compute Bayes factors Bayesian models, non-default arguments must added upon fitting: brmsfit models must fitted save_pars = save_pars(= TRUE) stanreg models must fitted defined diagnostic_file. Let’s first fit 5 Bayesian regressions brms predict Sepal.Length: can now compare models bayesfactor_models() function, using denominator argument specify model rest models compared (case, intercept-model): can see Species + Petal.Length model best model - \\(BF=2\\times 10^{53}\\) compared null (intercept ). Due transitive property Bayes factors, can easily change reference model full Species * Petal.Length model: can see, Species + Petal.Length model also favored compared Species * Petal.Length model, though several orders magnitude less - supported 23.38 times !) can also change reference model Species model: Notice , Bayesian framework compared models need nested models, happened compared Petal.Length-model Species-model (something done frequentist framework, compared models must nested one another). can also get matrix Bayes factors pairwise model comparisons: NOTE: order correctly precisely estimate Bayes Factors, always need 4 P’s: Proper Priors 3, Plentiful Posterior 4.","code":"library(brms)  # intercept only model m0 <- brm(Sepal.Length ~ 1,   data = iris,   prior =     set_prior(\"student_t(3, 6, 6)\", class = \"Intercept\") +       set_prior(\"student_t(3, 0, 6)\", class = \"sigma\"),   chains = 10, iter = 5000, warmup = 1000,   save_pars = save_pars(all = TRUE) )  # Petal.Length only m1 <- brm(Sepal.Length ~ Petal.Length,   data = iris,   prior =     set_prior(\"student_t(3, 6, 6)\", class = \"Intercept\") +       set_prior(\"student_t(3, 0, 6)\", class = \"sigma\") +       set_prior(\"normal(0, 1)\", coef = \"Petal.Length\"),   chains = 10, iter = 5000, warmup = 1000,   save_pars = save_pars(all = TRUE) )  # Species only m2 <- brm(Sepal.Length ~ Species,   data = iris,   prior =     set_prior(\"student_t(3, 6, 6)\", class = \"Intercept\") +       set_prior(\"student_t(3, 0, 6)\", class = \"sigma\") +       set_prior(\"normal(0, 3)\", coef = c(\"Speciesversicolor\", \"Speciesvirginica\")),   chains = 10, iter = 5000, warmup = 1000,   save_pars = save_pars(all = TRUE) )  # Species + Petal.Length model m3 <- brm(Sepal.Length ~ Species + Petal.Length,   data = iris,   prior =     set_prior(\"student_t(3, 6, 6)\", class = \"Intercept\") +       set_prior(\"student_t(3, 0, 6)\", class = \"sigma\") +       set_prior(\"normal(0, 1)\", coef = \"Petal.Length\") +       set_prior(\"normal(0, 3)\", coef = c(\"Speciesversicolor\", \"Speciesvirginica\")),   chains = 10, iter = 5000, warmup = 1000,   save_pars = save_pars(all = TRUE) )  # full interactive model m4 <- brm(Sepal.Length ~ Species * Petal.Length,   data = iris,   prior =     set_prior(\"student_t(3, 6, 6)\", class = \"Intercept\") +       set_prior(\"student_t(3, 0, 6)\", class = \"sigma\") +       set_prior(\"normal(0, 1)\", coef = \"Petal.Length\") +       set_prior(\"normal(0, 3)\", coef = c(\"Speciesversicolor\", \"Speciesvirginica\")) +       set_prior(\"normal(0, 2)\", coef = c(\"Speciesversicolor:Petal.Length\", \"Speciesvirginica:Petal.Length\")),   chains = 10, iter = 5000, warmup = 1000,   save_pars = save_pars(all = TRUE) ) library(bayestestR)  comparison <- bayesfactor_models(m1, m2, m3, m4, denominator = m0) comparison > Bayes Factors for Model Comparison >  >     Model                        BF > [1] Petal.Length           1.27e+44 > [2] Species                8.34e+27 > [3] Species + Petal.Length 2.29e+53 > [4] Species * Petal.Length 9.79e+51 >  > * Against Denominator: [5] (Intercept only) > *   Bayes Factor Type: marginal likelihoods (bridgesampling) update(comparison, reference = 4) > Bayes Factors for Model Comparison >  >     Model                        BF > [1] Petal.Length           1.30e-08 > [2] Species                8.52e-25 > [3] Species + Petal.Length    23.38 > [5] (Intercept only)       1.02e-52 >  > * Against Denominator: [4] Species * Petal.Length > *   Bayes Factor Type: marginal likelihoods (bridgesampling) update(comparison, reference = 2) > Bayes Factors for Model Comparison >  >     Model                        BF > [1] Petal.Length           1.53e+16 > [3] Species + Petal.Length 2.74e+25 > [4] Species * Petal.Length 1.17e+24 > [5] (Intercept only)       1.20e-28 >  > * Against Denominator: [2] Species > *   Bayes Factor Type: marginal likelihoods (bridgesampling) as.matrix(comparison) > # Bayes Factors for Model Comparison  >  >               Numerator > Denominator >  >                 |      [1] |      [2] |      [3] |      [4] |      [5] > --------------------------------------------------------------------------------- > [1] Petal.Length           |        1 | 6.54e-17 | 1.80e+09 | 7.68e+07 | 7.85e-45 > [2] Species                | 1.53e+16 |        1 | 2.74e+25 | 1.17e+24 | 1.20e-28 > [3] Species + Petal.Length | 5.57e-10 | 3.64e-26 |        1 |    0.043 | 4.37e-54 > [4] Species * Petal.Length | 1.30e-08 | 8.52e-25 |    23.38 |        1 | 1.02e-52 > [5] (Intercept only)       | 1.27e+44 | 8.34e+27 | 2.29e+53 | 9.79e+51 |        1"},{"path":"https://easystats.github.io/bayestestR/articles/bayes_factors.html","id":"for-frequentist-models-via-the-bic-approximation","dir":"Articles","previous_headings":"Comparing Models using Bayes Factors","what":"For Frequentist models via the BIC approximation","title":"Bayes Factors","text":"also possible compute Bayes factors comparison frequentist models. done comparing BIC measures, allowing Bayesian comparison nested well non-nested frequentist models (Wagenmakers, 2007). Let’s try linear mixed-effects models:","code":"library(lme4)  # define models with increasing complexity m0 <- lmer(Sepal.Length ~ (1 | Species), data = iris) m1 <- lmer(Sepal.Length ~ Petal.Length + (1 | Species), data = iris) m2 <- lmer(Sepal.Length ~ Petal.Length + (Petal.Length | Species), data = iris) m3 <- lmer(Sepal.Length ~ Petal.Length + Petal.Width + (Petal.Length | Species), data = iris) m4 <- lmer(Sepal.Length ~ Petal.Length * Petal.Width + (Petal.Length | Species), data = iris)  # model comparison bayesfactor_models(m1, m2, m3, m4, denominator = m0) > Bayes Factors for Model Comparison >  >      Model                                                       BF > [m1] Petal.Length + (1 | Species)                          3.82e+25 > [m2] Petal.Length + (Petal.Length | Species)               4.96e+24 > [m3] Petal.Length + Petal.Width + (Petal.Length | Species) 4.03e+23 > [m4] Petal.Length * Petal.Width + (Petal.Length | Species) 9.06e+22 >  > * Against Denominator: [m0] 1 + (1 | Species) > *   Bayes Factor Type: BIC approximation"},{"path":"https://easystats.github.io/bayestestR/articles/bayes_factors.html","id":"bayesfactor_restricted","dir":"Articles","previous_headings":"Comparing Models using Bayes Factors","what":"Order restricted models","title":"Bayes Factors","text":"stated discussing one-sided hypothesis tests, can create new models imposing order restrictions given model. example, consider following model, predict length iris’ sepal length petal, well species, priors: - \\(b_{petal} \\sim N(0,2)\\) - \\(b_{versicolors}\\ \\&\\  b_{virginica} \\sim N(0,1.2)\\) priors unrestricted - , values \\(-\\infty\\) \\(\\infty\\) parameters model non-zero credibility (matter small; true prior posterior distribution). Subsequently, priori ordering parameters relating iris species can ordering, priori setosa can larger sepals virginica, also possible virginica larger sepals setosa! make sense let priors cover possibilities? depends prior knowledge hypotheses. example, even novice botanist assume unlikely petal length negatively associated sepal length - iris longer petals likely larger, thus also longer sepal. expert botanist perhaps assume setosas smaller sepals versicolors virginica. priors can formulated restricted priors (Morey, 2015; Morey & Rouder, 2011): novice botanist: \\(b_{petal} > 0\\) expert botanist: \\(b_{versicolors} > 0\\ \\&\\ b_{virginica} > 0\\) testing restrictions prior posterior samples, can see probabilities restricted distributions change observing data. can achieved bayesfactor_restricted(), compute Bayes factor restricted model vs unrestricted model. Let’s first specify restrictions logical conditions: Let’s test hypotheses: can see novice botanist’s hypothesis gets Bayes factor ~2, indicating data provides twice much evidence model petal length restricted positively associated sepal length model restriction. expert botanist? seems failed miserably, BF favoring unrestricted model many many times . possible? seems controlling petal length, versicolor virginica actually shorter sepals!  Note Bayes factors compare restricted model unrestricted model. wanted compare restricted model null model, use transitive property Bayes factors like : \\[ BF_{\\text{restricted vs. NULL}} = \\frac {BF_{\\text{restricted vs. un-restricted}}} {BF_{\\text{un-restricted vs NULL}}} \\] restrictions prior distribution, appropriate testing pre-planned (priori) hypotheses, used post hoc comparisons (Morey, 2015). NOTE: See Specifying Correct Priors Factors 2 Levels appendix .","code":"iris_model <- stan_glm(Sepal.Length ~ Species + Petal.Length,   data = iris,   prior = normal(0, c(2, 1.2, 1.2), autoscale = FALSE),   chains = 10, iter = 5000, warmup = 1000 ) botanist_hypotheses <- c(   \"Petal.Length > 0\",   \"(Speciesversicolor > 0) & (Speciesvirginica > 0)\" ) model_prior <- unupdate(iris_model)  botanist_BFs <- bayesfactor_restricted(   posterior = iris_model,   prior = model_prior,   hypothesis = botanist_hypotheses )  print(botanist_BFs) > Bayes Factor (Order-Restriction) >  > Hypothesis                                       P(Prior) P(Posterior)       BF > Petal.Length > 0                                     0.50            1     2.02 > (Speciesversicolor > 0) & (Speciesvirginica > 0)     0.24            0 0.00e+00 >  > * Bayes factors for the restricted model vs. the un-restricted model."},{"path":"https://easystats.github.io/bayestestR/articles/bayes_factors.html","id":"bayesian-model-averaging","dir":"Articles","previous_headings":"","what":"Bayesian Model Averaging","title":"Bayes Factors","text":"previous section, discussed direct comparison two models determine effect supported data. However, many cases many models consider, perhaps straightforward models compare determine effect supported data. cases, can use Bayesian model averaging (BMA) determine support provided data parameter term across many models.","code":""},{"path":"https://easystats.github.io/bayestestR/articles/bayes_factors.html","id":"bayesfactor_inclusion","dir":"Articles","previous_headings":"Bayesian Model Averaging","what":"Inclusion Bayes factors","title":"Bayes Factors","text":"Inclusion Bayes factors answer question: observed data probable models particular predictor, models without particular predictor? words, average, models predictor \\(X\\) likely produced observed data models without predictor \\(X\\)?5 Since model prior probability, possible sum prior probability models include predictor interest (prior inclusion probability), models include predictor (prior exclusion probability). data observed, model assigned posterior probability, can similarly consider sums posterior models’ probabilities obtain posterior inclusion probability posterior exclusion probability. , change prior inclusion odds posterior inclusion odds Inclusion Bayes factor [“\\(BF_{Inclusion}\\)”; Clyde, Ghosh, & Littman (2011)]. Lets use brms example : examine interaction term’s inclusion Bayes factor, can see across 5 models, model term average (1/0.171) 5.84 times less supported model without term. Note Species, factor represented model several parameters, gets single Bayes factor - inclusion Bayes factors given per predictor! can also compare matched models - averaging done across models (1) include interactions predictor interest; (2) interaction predictors, averaging done across models contain main effects interaction predictor comprised (see explanation might want ).","code":"bayesfactor_inclusion(comparison) > Inclusion Bayes Factors (Model Averaged) >  >                      P(prior) P(posterior) Inclusion BF > Petal.Length             0.60         1.00     1.91e+25 > Species                  0.60         1.00     1.25e+09 > Petal.Length:Species     0.20         0.04        0.171 >  > * Compared among: all models > *    Priors odds: uniform-equal bayesfactor_inclusion(comparison, match_models = TRUE) > Inclusion Bayes Factors (Model Averaged) >  >                      P(prior) P(posterior) Inclusion BF > Petal.Length             0.40         0.96     2.74e+25 > Species                  0.40         0.96     1.80e+09 > Petal.Length:Species     0.20         0.04        0.043 >  > * Compared among: matched models only > *    Priors odds: uniform-equal"},{"path":"https://easystats.github.io/bayestestR/articles/bayes_factors.html","id":"comparison-with-jasp","dir":"Articles","previous_headings":"Bayesian Model Averaging","what":"Comparison with JASP","title":"Bayes Factors","text":"bayesfactor_inclusion() meant provide Bayes Factors per predictor, similar JASP’s Effects option. Let’s compare two: Across models:  Across matched models:  Nuisance Effects: ’ll add dose null model JASP, R:","code":"library(BayesFactor) data(ToothGrowth) ToothGrowth$dose <- as.factor(ToothGrowth$dose)  BF_ToothGrowth <- anovaBF(len ~ dose * supp, ToothGrowth, progress = FALSE)  bayesfactor_inclusion(BF_ToothGrowth) > Inclusion Bayes Factors (Model Averaged) >  >           P(prior) P(posterior) Inclusion BF > supp          0.60         1.00       140.99 > dose          0.60         1.00     3.21e+14 > dose:supp     0.20         0.72        10.12 >  > * Compared among: all models > *    Priors odds: uniform-equal bayesfactor_inclusion(BF_ToothGrowth, match_models = TRUE) > Inclusion Bayes Factors (Model Averaged) >  >           P(prior) P(posterior) Inclusion BF > supp          0.40         0.28        59.19 > dose          0.40         0.28     1.36e+14 > dose:supp     0.20         0.72         2.57 >  > * Compared among: matched models only > *    Priors odds: uniform-equal BF_ToothGrowth_against_dose <- BF_ToothGrowth[3:4] / BF_ToothGrowth[2] # OR: # update(bayesfactor_models(BF_ToothGrowth), #        subset = c(4, 5), #        reference = 3) BF_ToothGrowth_against_dose > Bayes factor analysis > -------------- > [1] supp + dose             : 59  ±4.5% > [2] supp + dose + supp:dose : 152 ±1.5% >  > Against denominator: >   len ~ dose  > --- > Bayes factor type: BFlinearModel, JZS bayesfactor_inclusion(BF_ToothGrowth_against_dose) > Inclusion Bayes Factors (Model Averaged) >  >           P(prior) P(posterior) Inclusion BF > dose          1.00         1.00              > supp          0.67         1.00       105.74 > dose:supp     0.33         0.72         5.06 >  > * Compared among: all models > *    Priors odds: uniform-equal"},{"path":"https://easystats.github.io/bayestestR/articles/bayes_factors.html","id":"weighted_posteriors","dir":"Articles","previous_headings":"Bayesian Model Averaging","what":"Averaging posteriors","title":"Bayes Factors","text":"Similar can average evidence predictor across models, can also average posterior estimate across models. useful situations Bayes factors seem support null effect, yet HDI alternative excludes null value (also see si() described ). example, looking Motor Trend Car Road Tests (data(mtcars)), naturally predict miles/gallon (mpg) transition type () weight (wt), number carburetors (carb)? good predictor? can determine comparing following models: seems model without carb predictor \\(1/BF=1.2\\) times likely model carb predictor. might assume latter model, HDI include point-null value 0 effect, also indicate credibility null posterior. However, case: can ? estimating HDI effect carb full model, acting assumption model correct. However, ’ve just seen, models practically tied. case limit estimation effect just one model? (Bergh, Haaf, Ly, Rouder, & Wagenmakers, 2019). Using Bayesian Model Averaging, can combine posteriors samples several models, weighted models’ marginal likelihood (done via bayesfactor_models() function). parameter part models missing others, assumed fixed 0 (can also seen method applying shrinkage estimates). results posterior distribution across several models, can now treat like posterior distribution, estimate HDI. bayestestR, can weighted_posteriors() function:  can see across models consideration, posterior carb effect almost equally weighted alternative model null model - represented half posterior mass concentrated 0 - makes sense models almost equally supported data. can also see across models, now HDI contain 0. Thus resolved conflict Bayes factor HDI (Rouder, Haaf, & Vandekerckhove, 2018)! Note: Parameters might play different roles across different models. example, parameter plays different role model Y ~ + B (main effect) model Y ~ + B + :B (simple effect). many cases centering predictors (mean subtracting continuous variables, orthogonal coding factors) can cases reduce issue.","code":"mod <- stan_glm(mpg ~ wt + am,   data = mtcars,   prior = normal(0, c(10, 10), autoscale = FALSE),   chains = 10, iter = 5000, warmup = 1000,   diagnostic_file = file.path(tempdir(), \"df1.csv\"),   refresh = 0 )  mod_carb <- stan_glm(mpg ~ wt + am + carb,   data = mtcars,   prior = normal(0, c(10, 10, 20), autoscale = FALSE),   chains = 10, iter = 5000, warmup = 1000,   diagnostic_file = file.path(tempdir(), \"df0.csv\"),   refresh = 0 )  BF_carb <- bayesfactor_models(mod_carb, denominator = mod, verbose = FALSE) BF_carb > Bayes Factors for Model Comparison >  >     Model             BF > [1] wt + am + carb 0.811 >  > * Against Denominator: [2] wt + am > *   Bayes Factor Type: marginal likelihoods (bridgesampling) hdi(mod_carb, ci = 0.95) > Highest Density Interval >  > Parameter   |        95% HDI > ---------------------------- > (Intercept) | [28.09, 40.15] > wt          | [-5.51, -1.74] > am          | [-1.01,  5.57] > carb        | [-2.02, -0.35] BMA_draws <- weighted_posteriors(mod, mod_carb)  BMA_hdi <- hdi(BMA_draws, ci = 0.95) BMA_hdi > Highest Density Interval >  > Parameter   |        95% HDI > ---------------------------- > (Intercept) | [29.18, 42.48] > wt          | [-6.64, -2.15] > am          | [-2.74,  5.03] > carb        | [-1.68,  0.00] plot(BMA_hdi)"},{"path":[]},{"path":"https://easystats.github.io/bayestestR/articles/bayes_factors.html","id":"testing-contrasts-with-emmeans-modelbased","dir":"Articles","previous_headings":"Appendices","what":"Testing contrasts (with emmeans / modelbased)","title":"Bayes Factors","text":"Besides testing parameter bayesfactor_parameters() can used test estimate based prior posterior distribution estimate. One way achieve mix bayesfactor_parameters() + emmeans test Bayesian contrasts. example, sleep example , can estimate group means difference : strong evidence mean group 1 0, group 2 0, hardly evidence difference 0. Conflict? Uncertainty? Bayesian way! can also use easystats’ modelbased package compute Bayes factors contrasts: NOTE: See Specifying Correct Priors Factors 2 Levels section .","code":"library(emmeans)  (group_diff <- emmeans(model, pairwise ~ group)) > $emmeans >  group emmean lower.HPD upper.HPD >  1       0.79     -0.48       2.0 >  2       2.28      1.00       3.5 >  > Point estimate displayed: median  > HPD interval probability: 0.95  >  > $contrasts >  contrast        estimate lower.HPD upper.HPD >  group1 - group2    -1.47      -3.2     0.223 >  > Point estimate displayed: median  > HPD interval probability: 0.95 # pass the original model via prior bayesfactor_parameters(group_diff, prior = model) > Bayes Factor (Savage-Dickey density ratio) >  > Parameter       |    BF > ----------------------- > 1               | 0.287 > 2               | 19.97 > group1 - group2 |  1.26 >  > * Evidence Against The Null: 0 library(modelbased)  estimate_contrasts(model, test = \"bf\", bf_prior = model)"},{"path":"https://easystats.github.io/bayestestR/articles/bayes_factors.html","id":"contr_bayes","dir":"Articles","previous_headings":"Appendices","what":"Specifying correct priors for factors","title":"Bayes Factors","text":"section introduces biased priors obtained using common effects factor coding (contr.sum) dummy factor coding (contr.treatment), solution using orthonormal factor coding (contr.equalprior) (outlined Rouder, Morey, Speckman, & Province, 2012, sec. 7.2). Special care taken working factors 3 levels.","code":""},{"path":"https://easystats.github.io/bayestestR/articles/bayes_factors.html","id":"contrasts-and-marginal-means","dir":"Articles","previous_headings":"Appendices > Specifying correct priors for factors","what":"Contrasts (and marginal means)","title":"Bayes Factors","text":"effects factor coding commonly used factorial analysis carries hidden bias applies Bayesian priors. example, want test pairwise differences 3 levels factor, expect priori differences distribution, … example, test prior pairwise differences 3 species iris dataset.  Notice , though prior estimate 3 pairwise contrasts ~0, scale HDI much narrower prior setosa - versicolor contrast! happened??? caused inherent bias priors introduced effects coding (’s even worse default treatment coding, prior intercept usually drastically different effect’s parameters). since affects priors, bias also bias Bayes factors / understating evidence contrasts others! solution use equal-prior factor coding, -la contr.equalprior* family, can either specify factor coding per-factor: can set globally: Let’s estimate prior differences:  can see using contr.equalprior_pairs coding scheme, equal priors pairwise contrasts, width corresponding normal(0, c(1, 1), autoscale = FALSE) prior set! solutions problem priors. can read Solomon Kurz’s blog post.","code":"df <- iris contrasts(df$Species) <- contr.sum  fit_sum <- stan_glm(Sepal.Length ~ Species,   data = df,   prior = normal(0, c(1, 1), autoscale = FALSE),   prior_PD = TRUE, # sample priors   family = gaussian(),   chains = 10, iter = 5000, warmup = 1000,   refresh = 0 ) (pairs_sum <- pairs(emmeans(fit_sum, ~Species))) >  contrast               estimate lower.HPD upper.HPD >  setosa - versicolor      -0.017      -2.8       2.7 >  setosa - virginica       -0.027      -4.0       4.6 >  versicolor - virginica    0.001      -4.2       4.5 >  > Point estimate displayed: median  > HPD interval probability: 0.95 ggplot(stack(insight::get_parameters(pairs_sum)), aes(x = values, fill = ind)) +   geom_density(size = 1) +   facet_grid(ind ~ .) +   labs(x = \"prior difference values\") +   theme(legend.position = \"none\") contrasts(df$Species) <- contr.equalprior_pairs options(contrasts = c(\"contr.equalprior_pairs\", \"contr.poly\")) fit_bayes <- stan_glm(Sepal.Length ~ Species,   data = df,   prior = normal(0, c(1, 1), autoscale = FALSE),   prior_PD = TRUE, # sample priors   family = gaussian(),   chains = 10, iter = 5000, warmup = 1000,   refresh = 0 ) (pairs_bayes <- pairs(emmeans(fit_bayes, ~Species))) >  contrast               estimate lower.HPD upper.HPD >  setosa - versicolor      0.0000     -2.10      1.89 >  setosa - virginica       0.0228     -1.93      1.99 >  versicolor - virginica   0.0021     -2.06      1.89 >  > Point estimate displayed: median  > HPD interval probability: 0.95 ggplot(stack(insight::get_parameters(pairs_bayes)), aes(x = values, fill = ind)) +   geom_density(size = 1) +   facet_grid(ind ~ .) +   labs(x = \"prior difference values\") +   theme(legend.position = \"none\")"},{"path":"https://easystats.github.io/bayestestR/articles/bayes_factors.html","id":"order-restrictions","dir":"Articles","previous_headings":"Appendices > Specifying correct priors for factors","what":"Order restrictions","title":"Bayes Factors","text":"bias also affect order restrictions involving 3 levels. example, want test order restriction among , B, C, priori probability obtaining order > C > B 1/6 (reach back intro stats year 1), … example, interested following order restrictions iris dataset (line separate restriction): default factor coding, looks like : happened??? comparison 2 levels prior ~0.5, expected. comparison 3 levels different priors, depending order restriction - .e. orders priori likely others!!! , solved using equal prior factor coding ().","code":"hyp <- c(   # comparing 2 levels   \"setosa < versicolor\",   \"setosa < virginica\",   \"versicolor < virginica\",    # comparing 3 (or more) levels   \"setosa    < virginica  & virginica  < versicolor\",   \"virginica < setosa     & setosa     < versicolor\",   \"setosa    < versicolor & versicolor < virginica\" ) contrasts(df$Species) <- contr.sum  fit_sum <- stan_glm(Sepal.Length ~ Species,   data = df,   prior = normal(0, c(1, 1), autoscale = FALSE),   family = gaussian(),   chains = 10, iter = 5000, warmup = 1000, )  em_sum <- emmeans(fit_sum, ~Species) # the posterior marginal means  bayesfactor_restricted(em_sum, fit_sum, hypothesis = hyp) >  > SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1). > Chain 1:  > Chain 1: Gradient evaluation took 2.5e-05 seconds > Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.25 seconds. > Chain 1: Adjust your expectations accordingly! > Chain 1:  > Chain 1:  > Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) > Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) > Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) > Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) > Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) > Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) > Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) > Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) > Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) > Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) > Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) > Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) > Chain 1:  > Chain 1:  Elapsed Time: 0.040254 seconds (Warm-up) > Chain 1:                0.070296 seconds (Sampling) > Chain 1:                0.11055 seconds (Total) > Chain 1:  >  > SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2). > Chain 2:  > Chain 2: Gradient evaluation took 1.4e-05 seconds > Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. > Chain 2: Adjust your expectations accordingly! > Chain 2:  > Chain 2:  > Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) > Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) > Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) > Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) > Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) > Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) > Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) > Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) > Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) > Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) > Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) > Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) > Chain 2:  > Chain 2:  Elapsed Time: 0.042379 seconds (Warm-up) > Chain 2:                0.063181 seconds (Sampling) > Chain 2:                0.10556 seconds (Total) > Chain 2:  >  > SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3). > Chain 3:  > Chain 3: Gradient evaluation took 1.4e-05 seconds > Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. > Chain 3: Adjust your expectations accordingly! > Chain 3:  > Chain 3:  > Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) > Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) > Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) > Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) > Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) > Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) > Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) > Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) > Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) > Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) > Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) > Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) > Chain 3:  > Chain 3:  Elapsed Time: 0.039851 seconds (Warm-up) > Chain 3:                0.060459 seconds (Sampling) > Chain 3:                0.10031 seconds (Total) > Chain 3:  >  > SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4). > Chain 4:  > Chain 4: Gradient evaluation took 1.5e-05 seconds > Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds. > Chain 4: Adjust your expectations accordingly! > Chain 4:  > Chain 4:  > Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) > Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) > Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) > Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) > Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) > Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) > Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) > Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) > Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) > Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) > Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) > Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) > Chain 4:  > Chain 4:  Elapsed Time: 0.042198 seconds (Warm-up) > Chain 4:                0.061165 seconds (Sampling) > Chain 4:                0.103363 seconds (Total) > Chain 4: > Bayes Factor (Order-Restriction) >  > Hypothesis                                       P(Prior) P(Posterior)       BF > setosa < versicolor                                  0.51            1     1.97 > setosa < virginica                                   0.49            1     2.02 > versicolor < virginica                               0.49            1     2.03 > setosa    < virginica  & virginica  < versicolor     0.11            0 0.00e+00 > virginica < setosa     & setosa     < versicolor     0.20            0 0.00e+00 > setosa    < versicolor & versicolor < virginica      0.20            1     5.09 >  > * Bayes factors for the restricted model vs. the un-restricted model. contrasts(df$Species) <- contr.equalprior_pairs  fit_bayes <- stan_glm(Sepal.Length ~ Species,   data = df,   prior = normal(0, c(1, 1), autoscale = FALSE),   family = gaussian(),   chains = 10, iter = 5000, warmup = 1000 ) em_bayes <- emmeans(fit_sum, ~Species) # the posterior marginal means bayesfactor_restricted(em_bayes, fit_sum, hypothesis = hyp) > Bayes Factor (Order-Restriction) >  > Hypothesis                                       P(Prior) P(Posterior)       BF > setosa < versicolor                                  0.49            1     2.06 > setosa < virginica                                   0.49            1     2.03 > versicolor < virginica                               0.51            1     1.96 > setosa    < virginica  & virginica  < versicolor     0.17            0 0.00e+00 > virginica < setosa     & setosa     < versicolor     0.16            0 0.00e+00 > setosa    < versicolor & versicolor < virginica      0.16            1     6.11 >  > * Bayes factors for the restricted model vs. the un-restricted model."},{"path":"https://easystats.github.io/bayestestR/articles/bayes_factors.html","id":"conclusion","dir":"Articles","previous_headings":"Appendices > Specifying correct priors for factors","what":"Conclusion","title":"Bayes Factors","text":"comparing results two factor coding schemes, find: 1. cases, estimated (posterior) means quite similar (identical). 2. priors Bayes factors differ two schemes. 3. contr.equalprior*, prior distribution difference order 3 () means balanced. Read equal prior contrasts contr.equalprior docs!","code":""},{"path":[]},{"path":"https://easystats.github.io/bayestestR/articles/bayestestR.html","id":"why-use-the-bayesian-framework","dir":"Articles","previous_headings":"","what":"Why use the Bayesian Framework?","title":"Get Started with Bayesian Analysis","text":"Bayesian framework statistics quickly gaining popularity among scientists, associated general shift towards open honest science. Reasons prefer approach : reliability (Etz & Vandekerckhove, 2016) accuracy (noisy data small samples) (Kruschke, Aguinis, & Joo, 2012) possibility introducing prior knowledge analysis (Andrews & Baguley, 2013; Kruschke et al., 2012) critically, intuitive nature results straightforward interpretation (Kruschke, 2010; Wagenmakers et al., 2018) general, frequentist approach associated focus null hypothesis testing, misuse p-values shown critically contribute reproducibility crisis social psychological sciences (Chambers, Feredoes, Muthukumaraswamy, & Etchells, 2014; Szucs & Ioannidis, 2016). emerging consensus generalization Bayesian approach one way overcoming issues (Benjamin et al., 2018; Etz & Vandekerckhove, 2016). agree Bayesian framework right way go, might wonder exactly framework. ’s fuss ?","code":""},{"path":"https://easystats.github.io/bayestestR/articles/bayestestR.html","id":"what-is-the-bayesian-framework","dir":"Articles","previous_headings":"","what":"What is the Bayesian Framework?","title":"Get Started with Bayesian Analysis","text":"Adopting Bayesian framework shift paradigm change methodology. Indeed, common statistical procedures (t-tests, correlations, ANOVAs, regressions, etc.) can achieved using Bayesian framework. key difference frequentist framework (“classical” approach statistics, p t values, well weird degrees freedom), effects fixed (unknown) data random. words, assumes unknown parameter unique value trying estimate/guess using sample data. hand, Bayesian framework, instead estimating “true effect”, probability different effects given observed data computed, resulting distribution possible values parameters, called posterior distribution. uncertainty Bayesian inference can summarized, instance, median distribution, well range values posterior distribution includes 95% probable values (95% credible interval). Cum grano salis, considered counterparts point-estimate confidence interval frequentist framework. illustrate difference interpretation, Bayesian framework allows say “given observed data, effect 95% probability falling within range”, frequentist (less intuitive) alternative “repeatedly computing confidence intervals data sort, 95% probability effect falls within given range”. essence, Bayesian sampling algorithms (MCMC sampling) return probability distribution (posterior) effect compatible observed data. Thus, effect can described characterizing posterior distribution relation centrality (point-estimates), uncertainty, well existence significance words, putting maths behind aside moment, can say : frequentist approach tries estimate real effect. instance, “real” value correlation x y. Hence, frequentist models return point-estimate (.e., single value distribution) “real” correlation (e.g., \\(r = 0.42\\)) estimated number obscure assumptions (minimum, considering data sampled random “parent”, usually normal distribution). Bayesian framework assumes thing. data . Based observed data (prior belief result), Bayesian sampling algorithm (MCMC sampling one example) returns probability distribution (called posterior) effect compatible observed data. correlation x y, return distribution says, example, “probable effect 0.42, data also compatible correlations 0.12 0.74 certain probabilities”. characterize statistical significance effects, need p-values, indices. simply describe posterior distribution effect. example, can report median, 89% Credible Interval indices. Accurate depiction regular Bayesian user estimating credible interval. Note: Altough purpose package advocate use Bayesian statistics, please note serious arguments supporting frequentist indices (see instance thread). always, world black white (p < .001). … work?","code":""},{"path":[]},{"path":"https://easystats.github.io/bayestestR/articles/bayestestR.html","id":"bayestestr-installation","dir":"Articles","previous_headings":"A simple example","what":"bayestestR installation","title":"Get Started with Bayesian Analysis","text":"can install bayestestR along whole easystats suite running following: Let’s also install load rstanarm, allows fitting Bayesian models, well bayestestR, describe .","code":"install.packages(\"remotes\") remotes::install_github(\"easystats/easystats\") install.packages(\"rstanarm\") library(rstanarm)"},{"path":"https://easystats.github.io/bayestestR/articles/bayestestR.html","id":"traditional-linear-regression","dir":"Articles","previous_headings":"A simple example","what":"Traditional linear regression","title":"Get Started with Bayesian Analysis","text":"Let’s start fitting simple frequentist linear regression (lm() function stands linear model) two numeric variables, Sepal.Length Petal.Length famous iris dataset, included default R. analysis suggests statistically significant (whatever means) positive (coefficient 0.41) linear relationship two variables. Fitting interpreting frequentist models easy obvious people use instead Bayesian framework… right? anymore.","code":"model <- lm(Sepal.Length ~ Petal.Length, data = iris) summary(model) Call: lm(formula = Sepal.Length ~ Petal.Length, data = iris)  Residuals:     Min      1Q  Median      3Q     Max  -1.2468 -0.2966 -0.0152  0.2768  1.0027   Coefficients:              Estimate Std. Error t value Pr(>|t|)     (Intercept)    4.3066     0.0784    54.9   <2e-16 *** Petal.Length   0.4089     0.0189    21.6   <2e-16 *** --- Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  Residual standard error: 0.41 on 148 degrees of freedom Multiple R-squared:  0.76,  Adjusted R-squared:  0.758  F-statistic:  469 on 1 and 148 DF,  p-value: <2e-16"},{"path":"https://easystats.github.io/bayestestR/articles/bayestestR.html","id":"bayesian-linear-regression","dir":"Articles","previous_headings":"A simple example","what":"Bayesian linear regression","title":"Get Started with Bayesian Analysis","text":"Summary Posterior Distribution ’s ! just fitted Bayesian version model simply using stan_glm() function instead lm() described posterior distributions parameters! conclusion draw, example, similar. effect (median effect’s posterior distribution) 0.41, can also considered significant Bayesian sense (later). , ready learn ? Check next tutorial! , want even , can check articles describing functionality package offer! https://easystats.github.io/bayestestR/articles/","code":"model <- stan_glm(Sepal.Length ~ Petal.Length, data = iris) posteriors <- describe_posterior(model) # for a nicer table print_md(posteriors, digits = 2)"},{"path":[]},{"path":"https://easystats.github.io/bayestestR/articles/credible_interval.html","id":"what-is-a-credible-interval","dir":"Articles","previous_headings":"","what":"What is a Credible Interval?","title":"Credible Intervals (CI)","text":"Credible intervals important concept Bayesian statistics. core purpose describe summarise uncertainty related unknown parameters trying estimate. regard, appear quite similar frequentist Confidence Intervals. However, goal similar, statistical definition meaning different. Indeed, latter obtained complex algorithm full rarely-tested assumptions approximations, credible intervals fairly straightforward compute. Bayesian inference returns distribution possible effect values (posterior), credible interval just range containing particular percentage probable values. instance, 95% credible interval simply central portion posterior distribution contains 95% values. Note drastically improve interpretability Bayesian interval compared frequentist one. Indeed, Bayesian framework allows us say “given observed data, effect 95% probability falling within range”, compared less straightforward, frequentist alternative (95% Confidence* Interval) “95% probability computing confidence interval data sort, effect falls within range”.","code":""},{"path":"https://easystats.github.io/bayestestR/articles/credible_interval.html","id":"vs--95-ci","dir":"Articles","previous_headings":"","what":"89% vs. 95% CI","title":"Credible Intervals (CI)","text":"Using 89% another popular choice, used default long time (read story change). start? Naturally, came choosing CI level report default, people started using 95%, arbitrary convention used frequentist world. However, authors suggested 95% might appropriate Bayesian posterior distributions, potentially lacking stability enough posterior samples drawn (Kruschke, 2014). proposition use 90% instead 95%. However, recently, McElreath (2014, 2018) suggested use arbitrary thresholds first place, use 89%? Moreover, 89 highest prime number exceed already unstable 95% threshold. anything? Nothing, reminds us total arbitrariness conventions (McElreath, 2018). Thus, CIs computed 89% intervals (ci = 0.89), deemed stable , instance, 95% intervals (Kruschke, 2014). effective sample size (ESS; see ) least 10.000 recommended one wants compute precise 95% intervals (Kruschke, 2014, p. 183ff). Unfortunately, default number posterior samples Bayes packages (e.g., rstanarm brms) 4.000 (thus, might want increase fitting model). However, 95% advantages . instance, shares (case normal posterior distribution) intuitive relationship standard deviation conveys accurate image (artificial) bounds distribution. Also, wider, makes analyses conservative (.e., probability covering 0 larger 95% CI lower ranges 89%), good thing context reproducibility crisis. add mess, software use different default, instance 90%. Ultimately, user make informed decision, based needs goals, justify choice.","code":""},{"path":"https://easystats.github.io/bayestestR/articles/credible_interval.html","id":"different-types-of-cis","dir":"Articles","previous_headings":"","what":"Different types of CIs","title":"Credible Intervals (CI)","text":"reader might notice bayestestR provides two methods compute credible intervals, Highest Density Interval (HDI) (hdi()) Equal-tailed Interval (ETI) (eti()). methods can also changed via method argument ci() function. difference? Let’s see:  exactly … also case types distributions?  difference strong one. Contrary HDI, points within interval higher probability density points outside interval, ETI equal-tailed. means 90% interval 5% distribution either side limits. indicates 5th percentile 95th percentile. symmetric distributions, two methods computing credible intervals, ETI HDI, return similar results. case skewed distributions. Indeed, possible parameter values ETI lower credibility (less probable) parameter values outside ETI. property seems undesirable summary credible values distribution. hand, ETI range change transformations applied distribution (instance, log-odds probabilities transformation): lower higher bounds transformed distribution correspond transformed lower higher bounds original distribution. contrary, applying transformations distribution change resulting HDI. Thus, instance, exponentiated credible intervals required, calculating ETI recommended.","code":"library(bayestestR) library(ggplot2)  # Generate a normal distribution posterior <- distribution_normal(1000)  # Compute HDI and ETI ci_hdi <- ci(posterior, method = \"HDI\") ci_eti <- ci(posterior, method = \"ETI\")  # Plot the distribution and add the limits of the two CIs out <- estimate_density(posterior, extend = TRUE) ggplot(out, aes(x = x, y = y)) +   geom_area(fill = \"orange\") +   theme_classic() +   # HDI in blue   geom_vline(xintercept = ci_hdi$CI_low, color = \"royalblue\", size = 3) +   geom_vline(xintercept = ci_hdi$CI_high, color = \"royalblue\", size = 3) +   # Quantile in red   geom_vline(xintercept = ci_eti$CI_low, color = \"red\", size = 1) +   geom_vline(xintercept = ci_eti$CI_high, color = \"red\", size = 1) # Generate a beta distribution posterior <- distribution_beta(1000, 6, 2)  # Compute HDI and Quantile CI ci_hdi <- ci(posterior, method = \"HDI\") ci_eti <- ci(posterior, method = \"ETI\")  # Plot the distribution and add the limits of the two CIs out <- estimate_density(posterior, extend = TRUE) ggplot(out, aes(x = x, y = y)) +   geom_area(fill = \"orange\") +   theme_classic() +   # HDI in blue   geom_vline(xintercept = ci_hdi$CI_low, color = \"royalblue\", size = 3) +   geom_vline(xintercept = ci_hdi$CI_high, color = \"royalblue\", size = 3) +   # ETI in red   geom_vline(xintercept = ci_eti$CI_low, color = \"red\", size = 1) +   geom_vline(xintercept = ci_eti$CI_high, color = \"red\", size = 1)"},{"path":"https://easystats.github.io/bayestestR/articles/credible_interval.html","id":"the-support-interval","dir":"Articles","previous_headings":"","what":"The Support Interval","title":"Credible Intervals (CI)","text":"Unlike HDI ETI, look posterior distribution, Support Interval (SI) provides information regarding change credibility values prior posterior - words, indicates values parameter gained support observed data factor greater equal k (Wagenmakers, Gronau, Dablander, & Etz, 2018).  blue lines values received support data (\\(BF = 1~SI\\)), red lines values received least moderate support (\\(BF = 3~SI\\)) data. perspective Savage-Dickey Bayes factor, testing point null hypothesis value within Support Interval yield Bayes factor smaller 1/BF.","code":"prior <- distribution_normal(40000, mean = 0, sd = 1) posterior <- distribution_normal(40000, mean = .5, sd = .3)  si_1 <- si(posterior, prior, BF = 1) si_3 <- si(posterior, prior, BF = 3)  ggplot(mapping = aes(x = x, y = y)) +   theme_classic() +   # The posterior   geom_area(     fill = \"orange\",     data = estimate_density(posterior, extend = TRUE)   ) +   # The prior   geom_area(     color = \"black\", fill = NA, size = 1, linetype = \"dashed\",     data = estimate_density(prior, extend = TRUE)   ) +   # BF = 1 SI in blue   geom_vline(xintercept = si_1$CI_low, color = \"royalblue\", size = 1) +   geom_vline(xintercept = si_1$CI_high, color = \"royalblue\", size = 1) +   # BF = 3 SI in red   geom_vline(xintercept = si_3$CI_low, color = \"red\", size = 1) +   geom_vline(xintercept = si_3$CI_high, color = \"red\", size = 1)"},{"path":[]},{"path":"https://easystats.github.io/bayestestR/articles/example1.html","id":"loading-the-packages","dir":"Articles","previous_headings":"","what":"Loading the packages","title":"1. Initiation to Bayesian models","text":"’ve installed necessary packages, can load rstanarm (fit models), bayestestR (compute useful indices), insight (access parameters).","code":"library(rstanarm) library(bayestestR) library(insight)"},{"path":"https://easystats.github.io/bayestestR/articles/example1.html","id":"simple-linear-regression-model","dir":"Articles","previous_headings":"","what":"Simple linear (regression) model","title":"1. Initiation to Bayesian models","text":"begin conducting simple linear regression test relationship Petal.Length (predictor, independent, variable) Sepal.Length (response, dependent, variable) iris dataset included default R.","code":""},{"path":"https://easystats.github.io/bayestestR/articles/example1.html","id":"fitting-the-model","dir":"Articles","previous_headings":"Simple linear (regression) model","what":"Fitting the model","title":"1. Initiation to Bayesian models","text":"Let’s start fitting frequentist version model, just reference point: can also zoom parameters interest us: model, linear relationship Petal.Length Sepal.Length positive significant (\\(\\beta = 0.41, t(148) = 21.6, p < .001\\)). means one-unit increase Petal.Length (predictor), can expect Sepal.Length (response) increase 0.41. effect can visualized plotting predictor values x axis response values y using ggplot2 package:  Now let’s fit Bayesian version model using stan_glm function rstanarm package: can see sampling algorithm run.","code":"model <- lm(Sepal.Length ~ Petal.Length, data = iris) summary(model) >  > Call: > lm(formula = Sepal.Length ~ Petal.Length, data = iris) >  > Residuals: >     Min      1Q  Median      3Q     Max  > -1.2468 -0.2966 -0.0152  0.2768  1.0027  >  > Coefficients: >              Estimate Std. Error t value Pr(>|t|)     > (Intercept)    4.3066     0.0784    54.9   <2e-16 *** > Petal.Length   0.4089     0.0189    21.6   <2e-16 *** > --- > Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 >  > Residual standard error: 0.41 on 148 degrees of freedom > Multiple R-squared:  0.76,    Adjusted R-squared:  0.758  > F-statistic:  469 on 1 and 148 DF,  p-value: <2e-16 get_parameters(model) >      Parameter Estimate > 1  (Intercept)     4.31 > 2 Petal.Length     0.41 library(ggplot2) # Load the package  # The ggplot function takes the data as argument, and then the variables # related to aesthetic features such as the x and y axes. ggplot(iris, aes(x = Petal.Length, y = Sepal.Length)) +   geom_point() + # This adds the points   geom_smooth(method = \"lm\") # This adds a regression line model <- stan_glm(Sepal.Length ~ Petal.Length, data = iris)"},{"path":"https://easystats.github.io/bayestestR/articles/example1.html","id":"extracting-the-posterior","dir":"Articles","previous_headings":"Simple linear (regression) model","what":"Extracting the posterior","title":"1. Initiation to Bayesian models","text":"done, let us extract parameters (.e., coefficients) model. can see, parameters take form lengthy dataframe two columns, corresponding intercept effect Petal.Length. columns contain posterior distributions two parameters. simple terms, posterior distribution set different plausible values parameter. Contrast result saw frequentist linear regression mode using lm, results single values effect model, distribution values. one important differences two frameworks.","code":"posteriors <- get_parameters(model)  head(posteriors) # Show the first 6 rows >   (Intercept) Petal.Length > 1         4.4         0.39 > 2         4.4         0.40 > 3         4.3         0.41 > 4         4.3         0.40 > 5         4.3         0.40 > 6         4.3         0.41"},{"path":"https://easystats.github.io/bayestestR/articles/example1.html","id":"about-posterior-draws","dir":"Articles","previous_headings":"Simple linear (regression) model > Extracting the posterior","what":"About posterior draws","title":"1. Initiation to Bayesian models","text":"Let’s look length posteriors. size 4000, less? First , observations (rows) usually referred posterior draws. underlying idea Bayesian sampling algorithm (e.g., Monte Carlo Markov Chains - MCMC) draw hidden true posterior distribution. Thus, posterior draws can estimate underlying true posterior distribution. Therefore, draws , better estimation posterior distribution. However, increased draws also means longer computation time. look documentation (?sampling) rstanarm’s \"sampling\" algorithm used default model , can see several parameters influence number posterior draws. default, 4 chains (can see distinct sampling runs), create 2000 iter (draws). However, half iterations kept, half used warm-(convergence algorithm). Thus, total posterior draws equals 4 chains * (2000 iterations - 1000 warm-) = 4000. can change , instance: case, expected, 2 chains * (1000 iterations - 250 warm-) = 1500 posterior draws. let’s keep first model default setup (draws).","code":"nrow(posteriors) # Size (number of rows) > [1] 4000 model <- stan_glm(Sepal.Length ~ Petal.Length, data = iris, chains = 2, iter = 1000, warmup = 250)  nrow(get_parameters(model)) # Size (number of rows) [1] 1500"},{"path":"https://easystats.github.io/bayestestR/articles/example1.html","id":"visualizing-the-posterior-distribution","dir":"Articles","previous_headings":"Simple linear (regression) model > Extracting the posterior","what":"Visualizing the posterior distribution","title":"1. Initiation to Bayesian models","text":"Now ’ve understood values come , let’s look . start visualizing posterior distribution parameter interest, effect Petal.Length.  distribution represents probability (y axis) different effects (x axis). central values probable extreme values. can see, distribution ranges 0.35 0.50, bulk around 0.41. Congrats! ’ve just described first posterior distribution. heart Bayesian analysis. don’t need p-values, t-values, degrees freedom. Everything need contained within posterior distribution. description consistent values obtained frequentist regression (resulted \\(\\beta\\) 0.41). reassuring! Indeed, cases, Bayesian analysis drastically differ frequentist results interpretation. Rather, makes results interpretable intuitive, easier understand describe. can now go ahead precisely characterize posterior distribution.","code":"ggplot(posteriors, aes(x = Petal.Length)) +   geom_density(fill = \"orange\")"},{"path":"https://easystats.github.io/bayestestR/articles/example1.html","id":"describing-the-posterior","dir":"Articles","previous_headings":"Simple linear (regression) model","what":"Describing the Posterior","title":"1. Initiation to Bayesian models","text":"Unfortunately, often practical report whole posterior distributions graphs. need find concise way summarize . recommend describe posterior distribution 3 elements: point-estimate one-value summary (similar \\(beta\\) frequentist regressions). credible interval representing associated uncertainty. indices significance, giving information relative importance effect.","code":""},{"path":"https://easystats.github.io/bayestestR/articles/example1.html","id":"point-estimate","dir":"Articles","previous_headings":"Simple linear (regression) model > Describing the Posterior","what":"Point-estimate","title":"1. Initiation to Bayesian models","text":"single value can best represent posterior distribution? Centrality indices, mean, median, mode usually used point-estimates. ’s difference ? Let’s answer first inspecting mean: close frequentist \\(\\beta\\). , know, mean quite sensitive outliers extremes values. Maybe median robust? Well, close mean (identical rounding values). Maybe take mode, , peak posterior distribution? Bayesian framework, value called Maximum Posteriori (MAP). Let’s see: close! Let’s visualize values posterior distribution:  Well, values give similar results. Thus, choose median, value direct meaning probabilistic perspective: 50% chance true effect higher 50% chance effect lower (divides distribution two equal parts).","code":"mean(posteriors$Petal.Length) > [1] 0.41 median(posteriors$Petal.Length) > [1] 0.41 map_estimate(posteriors$Petal.Length) > MAP Estimate: 0.41 ggplot(posteriors, aes(x = Petal.Length)) +   geom_density(fill = \"orange\") +   # The mean in blue   geom_vline(xintercept = mean(posteriors$Petal.Length), color = \"blue\", size = 1) +   # The median in red   geom_vline(xintercept = median(posteriors$Petal.Length), color = \"red\", size = 1) +   # The MAP in purple   geom_vline(xintercept = map_estimate(posteriors$Petal.Length), color = \"purple\", size = 1)"},{"path":"https://easystats.github.io/bayestestR/articles/example1.html","id":"uncertainty","dir":"Articles","previous_headings":"Simple linear (regression) model > Describing the Posterior","what":"Uncertainty","title":"1. Initiation to Bayesian models","text":"Now point-estimate, describe uncertainty. compute range: make sense include extreme values? Probably . Thus, compute credible interval. Long story short, ’s kind similar frequentist confidence interval, easier interpret easier compute — makes sense. compute credible interval based Highest Density Interval (HDI). give us range containing 89% probable effect values. Note use 89% CIs instead 95% CIs (frequentist framework), 89% level gives stable results (Kruschke, 2014) reminds us arbitrariness conventions (McElreath, 2018). Nice, can conclude effect 89% chance falling within [0.38, 0.44] range. just computed two important pieces information describing effects.","code":"range(posteriors$Petal.Length) > [1] 0.33 0.48 hdi(posteriors$Petal.Length, ci = 0.89) > 89% HDI: [0.38, 0.44]"},{"path":"https://easystats.github.io/bayestestR/articles/example1.html","id":"effect-significance","dir":"Articles","previous_headings":"Simple linear (regression) model > Describing the Posterior","what":"Effect significance","title":"1. Initiation to Bayesian models","text":"However, many scientific fields sufficient simply describe effects. Scientists also want know effect significance practical statistical terms, words, whether effect important. instance, effect different 0? assess significance effect. can ? Well, particular case, eloquent: possible effect values (.e., whole posterior distribution) positive 0.35, already substantial evidence effect zero. still, want objective decision criterion, say yes effect ‘significant’. One approach, similar frequentist framework, see Credible Interval contains 0. case, mean effect ‘significant’. index fine-grained, ? Can better? Yes!","code":""},{"path":"https://easystats.github.io/bayestestR/articles/example1.html","id":"a-linear-model-with-a-categorical-predictor","dir":"Articles","previous_headings":"","what":"A linear model with a categorical predictor","title":"1. Initiation to Bayesian models","text":"Imagine moment interested weight chickens varies depending two different feed types. example, start selecting chickwts dataset (available base R) two feed types interest us (peculiar interests): meat meals sunflowers.","code":""},{"path":"https://easystats.github.io/bayestestR/articles/example1.html","id":"data-preparation-and-model-fitting","dir":"Articles","previous_headings":"A linear model with a categorical predictor","what":"Data preparation and model fitting","title":"1. Initiation to Bayesian models","text":"Let’s run another Bayesian regression predict weight two types feed type.","code":"library(datawizard)  # We keep only rows for which feed is meatmeal or sunflower data <- data_filter(chickwts, feed %in% c(\"meatmeal\", \"sunflower\")) model <- stan_glm(weight ~ feed, data = data)"},{"path":"https://easystats.github.io/bayestestR/articles/example1.html","id":"posterior-description","dir":"Articles","previous_headings":"A linear model with a categorical predictor","what":"Posterior description","title":"1. Initiation to Bayesian models","text":"represents posterior distribution difference meatmeal sunflowers. seems difference positive (since values concentrated right side 0). Eating sunflowers makes fat (least, ’re chicken). , much? Let us compute median CI: makes fat around 51 grams (median). However, uncertainty quite high: 89% chance difference two feed types 14 91. effect different 0?","code":"posteriors <- get_parameters(model)  ggplot(posteriors, aes(x = feedsunflower)) +   geom_density(fill = \"red\") median(posteriors$feedsunflower) > [1] 52 hdi(posteriors$feedsunflower) > 95% HDI: [1.55, 99.59]"},{"path":"https://easystats.github.io/bayestestR/articles/example1.html","id":"rope-percentage","dir":"Articles","previous_headings":"A linear model with a categorical predictor","what":"ROPE Percentage","title":"1. Initiation to Bayesian models","text":"Testing whether distribution different 0 doesn’t make sense, 0 single value (probability distribution different single value infinite). However, one way assess significance define area around 0, consider practically equivalent zero (.e., absence , negligible, effect). called Region Practical Equivalence (ROPE), one way testing significance parameters. can define region? Driing driiiing – easystats team speaking. can help? – Prof. Sanders. expert chicks… mean chickens. Just calling let know based expert knowledge, effect -20 20 negligible. Bye. Well, ’s convenient. Now know can define ROPE [-20, 20] range. effects within range considered null (negligible). can now compute proportion 89% probable values (89% CI) null, .e., outside range. 5% 89% CI can considered null. lot? Based guidelines, yes, much. Based particular definition ROPE, conclude effect significant (probability negligible high). said, honest, doubts Prof. Sanders. don’t really trust definition ROPE. objective way defining ? Prof. Sanders giving default values define Region Practical Equivalence (ROPE). Yes! One practice instance use tenth (1/10 = 0.1) standard deviation (SD) response variable, can considered “negligible” effect size (Cohen, 1988). Let’s redefine ROPE region within [-6.2, 6.2] range. Note can directly obtained rope_range function :) Let’s recompute percentage ROPE: reasonable definition ROPE, observe 89% posterior distribution effect overlap ROPE. Thus, can conclude effect significant (sense important enough noted).","code":"rope(posteriors$feedsunflower, range = c(-20, 20), ci = 0.89) > # Proportion of samples inside the ROPE [-20.00, 20.00]: >  > inside ROPE > ----------- > 4.57 % rope_value <- 0.1 * sd(data$weight) rope_range <- c(-rope_value, rope_value) rope_range > [1] -6.2  6.2 rope_value <- rope_range(model) rope_value > [1] -6.2  6.2 rope(posteriors$feedsunflower, range = rope_range, ci = 0.89) > # Proportion of samples inside the ROPE [-6.17, 6.17]: >  > inside ROPE > ----------- > 0.00 %"},{"path":"https://easystats.github.io/bayestestR/articles/example1.html","id":"probability-of-direction-pd","dir":"Articles","previous_headings":"A linear model with a categorical predictor","what":"Probability of Direction (pd)","title":"1. Initiation to Bayesian models","text":"Maybe interested whether effect non-negligible. Maybe just want know effect positive negative. case, can simply compute proportion posterior positive, matter “size” effect. can conclude effect positive probability 98%. call index Probability Direction (pd). can, fact, computed easily following: Interestingly, happens index usually highly correlated frequentist p-value. almost roughly infer corresponding p-value simple transformation: ran model frequentist framework, approximately observe effect p-value 0.04. true?","code":"# select only positive values n_positive <- nrow(data_filter(posteriors, feedsunflower > 0))  n_positive / nrow(posteriors) * 100 > [1] 98 p_direction(posteriors$feedsunflower) > Probability of Direction: 0.98 pd <- 97.82 onesided_p <- 1 - pd / 100 twosided_p <- onesided_p * 2 twosided_p > [1] 0.044"},{"path":"https://easystats.github.io/bayestestR/articles/example1.html","id":"comparison-to-frequentist","dir":"Articles","previous_headings":"A linear model with a categorical predictor > Probability of Direction (pd)","what":"Comparison to frequentist","title":"1. Initiation to Bayesian models","text":"frequentist model tells us difference positive significant (\\(\\beta = 52, p = 0.04\\)). Although arrived similar conclusion, Bayesian framework allowed us develop profound intuitive understanding effect, uncertainty estimation.","code":"summary(lm(weight ~ feed, data = data)) >  > Call: > lm(formula = weight ~ feed, data = data) >  > Residuals: >     Min      1Q  Median      3Q     Max  > -123.91  -25.91   -6.92   32.09  103.09  >  > Coefficients: >               Estimate Std. Error t value Pr(>|t|)     > (Intercept)      276.9       17.2   16.10  2.7e-13 *** > feedsunflower     52.0       23.8    2.18     0.04 *   > --- > Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 >  > Residual standard error: 57 on 21 degrees of freedom > Multiple R-squared:  0.185,   Adjusted R-squared:  0.146  > F-statistic: 4.77 on 1 and 21 DF,  p-value: 0.0405"},{"path":"https://easystats.github.io/bayestestR/articles/example1.html","id":"all-with-one-function","dir":"Articles","previous_headings":"","what":"All with one function","title":"1. Initiation to Bayesian models","text":"yet, agree, bit tedious extract compute indices. told can , , one function? Behold, describe_posterior! function computes adored mentioned indices, can run directly model: Tada! ! median, CI, pd ROPE percentage! Understanding describing posterior distributions just one aspect Bayesian modelling. ready ?! Click see next example.","code":"describe_posterior(model, test = c(\"p_direction\", \"rope\", \"bayesfactor\")) > Summary of Posterior Distribution >  > Parameter     | Median |           95% CI |     pd |          ROPE | % in ROPE |  Rhat |      ESS |       BF > ------------------------------------------------------------------------------------------------------------ > (Intercept)   | 276.84 | [241.50, 312.32] |   100% | [-6.17, 6.17] |        0% | 1.000 | 35566.00 | 2.27e+14 > feedsunflower |  52.03 | [  2.38, 100.48] | 97.99% | [-6.17, 6.17] |     1.01% | 1.000 | 34922.00 |    0.758"},{"path":[]},{"path":[]},{"path":"https://easystats.github.io/bayestestR/articles/example2.html","id":"frequentist-version","dir":"Articles","previous_headings":"Correlations","what":"Frequentist version","title":"2. Confirmation of Bayesian skills","text":", let us begin frequentist correlation two continuous variables, width length sepals flowers. data available R iris dataset (used previous tutorial). compute Pearson’s correlation test, store results object called result, display : can see output, test actually compared two hypotheses: - null hypothesis (h0; correlation), - alternative hypothesis (h1; non-null correlation). Based p-value, null hypothesis rejected: correlation two variables negative non-significant (\\(r = -.12, p > .05\\)).","code":"result <- cor.test(iris$Sepal.Width, iris$Sepal.Length) result >  >   Pearson's product-moment correlation >  > data:  iris$Sepal.Width and iris$Sepal.Length > t = -1, df = 148, p-value = 0.2 > alternative hypothesis: true correlation is not equal to 0 > 95 percent confidence interval: >  -0.273  0.044 > sample estimates: >   cor  > -0.12"},{"path":"https://easystats.github.io/bayestestR/articles/example2.html","id":"bayesian-correlation","dir":"Articles","previous_headings":"Correlations","what":"Bayesian correlation","title":"2. Confirmation of Bayesian skills","text":"compute Bayesian correlation test, need BayesFactor package (can install running install.packages(\"BayesFactor\")). can load package, compute correlation using correlationBF() function, store result. Now, let us run describe_posterior() function : see many things , important indices now median posterior distribution, -.11. () quite close frequentist correlation. , previously, describe credible interval, pd ROPE percentage, focus another index provided Bayesian framework, Bayes Factor (BF).","code":"library(BayesFactor) result <- correlationBF(iris$Sepal.Width, iris$Sepal.Length) describe_posterior(result) > Summary of Posterior Distribution >  > Parameter | Median |        95% CI |     pd |          ROPE | % in ROPE |    BF |         Prior > ----------------------------------------------------------------------------------------------- > rho       |  -0.11 | [-0.27, 0.04] | 92.25% | [-0.05, 0.05] |    20.42% | 0.509 | Beta (3 +- 3)"},{"path":"https://easystats.github.io/bayestestR/articles/example2.html","id":"bayes-factor-bf","dir":"Articles","previous_headings":"Correlations","what":"Bayes Factor (BF)","title":"2. Confirmation of Bayesian skills","text":"said previously correlation test actually compares two hypotheses, null (absence effect) alternative one (presence effect). Bayes factor (BF) allows comparison determines two models observed data probable: model effect interest, null model without effect interest. , context correlation example, null hypothesis correlation two variables (\\(h0: \\rho = 0\\); \\(\\rho\\) stands Bayesian correlation coefficient), alternative hypothesis correlation different 0 - positive negative (\\(h1: \\rho \\neq 0\\)). can use bayesfactor_models() specifically compute Bayes factor comparing models: got BF 0.51. mean? Bayes factors continuous measures relative evidence, Bayes factor greater 1 giving evidence favour one models (often referred numerator), Bayes factor smaller 1 giving evidence favour model (denominator). Yes, heard right, evidence favour null! ’s one reason Bayesian framework sometimes considered superior frequentist framework. Remember stats lessons, p-value can used reject h0, accept . Bayes factor, can measure evidence - favour - null. words, frequentist framework, p-value significant, can conclude evidence effect absent, evidence absence effect. Bayesian framework, can latter. important since sometimes hypotheses effect. BFs representing evidence alternative null can reversed using \\(BF_{01}=1/BF_{10}\\) (01 10 correspond h0 h1 h1 h0, respectively) provide evidence null alternative. improves human readability1 cases BF alternative null smaller 1 (.e., support null). case, BF = 1/0.51 = 2, indicates data 2 times probable null compared alternative hypothesis, , though favouring null, considered anecdotal evidence null. can thus conclude anecdotal evidence favour absence correlation two variables (rmedian = 0.11, BF = 0.51), much informative statement can frequentist statistics. ’s !","code":"bayesfactor_models(result) > Bayes Factors for Model Comparison >  >     Model         BF > [2] (rho != 0) 0.509 >  > * Against Denominator: [1] (rho = 0) > *   Bayes Factor Type: JZS (BayesFactor)"},{"path":"https://easystats.github.io/bayestestR/articles/example2.html","id":"visualise-the-bayes-factor","dir":"Articles","previous_headings":"Correlations","what":"Visualise the Bayes factor","title":"2. Confirmation of Bayesian skills","text":"general, pie charts absolute -go data visualisation, brain’s perceptive system heavily distorts information presented way2. Nevertheless, one exception: pizza charts. intuitive way interpreting strength evidence provided BFs amount surprise. Wagenmakers’ pizza poking analogy. great <www.bayesianspectacles.org> blog. “pizza plots” can directly created see visualisation companion package easystats (can install running install.packages(\"see\")):  , seeing pizza, much surprised outcome blinded poke?","code":"library(see)  plot(bayesfactor_models(result)) +   scale_fill_pizza()"},{"path":"https://easystats.github.io/bayestestR/articles/example2.html","id":"t-tests","dir":"Articles","previous_headings":"","what":"t-tests","title":"2. Confirmation of Bayesian skills","text":"“know know nothing, especially versicolor virginica differ terms Sepal.Width” - Socrates. Time finally answer crucial question!","code":""},{"path":"https://easystats.github.io/bayestestR/articles/example2.html","id":"versicolor-vs--virginica","dir":"Articles","previous_headings":"t-tests","what":"Versicolor vs. virginica","title":"2. Confirmation of Bayesian skills","text":"Bayesian t-tests can performed similar way correlations. particularly interested two levels Species factor, versicolor virginica. start filtering iris non-relevant observations corresponding setosa specie, visualise observations distribution Sepal.Width variable.  seems (visually) virgnica flowers , average, slightly higer width sepals. Let’s assess difference statistically using ttestBF() function BayesFactor package.","code":"library(datawizard) library(ggplot2)  # Select only two relevant species data <- droplevels(data_filter(iris, Species != \"setosa\"))  # Visualise distributions and observations ggplot(data, aes(x = Species, y = Sepal.Width, fill = Species)) +   geom_violindot(fill_dots = \"black\", size_dots = 1) +   scale_fill_material() +   theme_modern()"},{"path":"https://easystats.github.io/bayestestR/articles/example2.html","id":"compute-the-bayesian-t-test","dir":"Articles","previous_headings":"t-tests","what":"Compute the Bayesian t-test","title":"2. Confirmation of Bayesian skills","text":"indices, can say difference Sepal.Width virginica versicolor probability 100% negative [pd sign median] (median = -0.19, 89% CI [-0.29, -0.092]). data provides strong evidence null hypothesis (BF = 18). Keep mind see another way investigating question.","code":"result <- BayesFactor::ttestBF(formula = Sepal.Width ~ Species, data = data) describe_posterior(result) > Summary of Posterior Distribution >  > Parameter  | Median |         95% CI |     pd |          ROPE | % in ROPE |    BF |              Prior > ------------------------------------------------------------------------------------------------------ > Difference |  -0.19 | [-0.32, -0.06] | 99.75% | [-0.03, 0.03] |        0% | 17.72 | Cauchy (0 +- 0.71)"},{"path":"https://easystats.github.io/bayestestR/articles/example2.html","id":"logistic-model","dir":"Articles","previous_headings":"","what":"Logistic Model","title":"2. Confirmation of Bayesian skills","text":"hypothesis one uses t-test can also tested using binomial model (e.g., logistic model). Indeed, possible reformulate following hypothesis, “important difference variable two groups” hypothesis “variable able discriminate (classify) two groups”. However, models much powerful t-test. case difference Sepal.Width virginica versicolor, question becomes, well can classify two species using Sepal.Width.","code":""},{"path":"https://easystats.github.io/bayestestR/articles/example2.html","id":"fit-the-model","dir":"Articles","previous_headings":"Logistic Model","what":"Fit the model","title":"2. Confirmation of Bayesian skills","text":"","code":"library(rstanarm)  model <- stan_glm(Species ~ Sepal.Width,   data = data, family = \"binomial\",   chains = 10, iter = 5000, warmup = 1000,   refresh = 0 )"},{"path":"https://easystats.github.io/bayestestR/articles/example2.html","id":"visualise-the-model","dir":"Articles","previous_headings":"Logistic Model","what":"Visualise the model","title":"2. Confirmation of Bayesian skills","text":"Using modelbased package.","code":"library(modelbased)  vizdata <- estimate_relation(model)  ggplot(vizdata, aes(x = Sepal.Width, y = Predicted)) +   geom_ribbon(aes(ymin = CI_low, ymax = CI_high), alpha = 0.5) +   geom_line() +   ylab(\"Probability of being virginica\") +   theme_modern()"},{"path":"https://easystats.github.io/bayestestR/articles/example2.html","id":"performance-and-parameters","dir":"Articles","previous_headings":"Logistic Model","what":"Performance and Parameters","title":"2. Confirmation of Bayesian skills","text":", can extract indices interest posterior distribution using old pal describe_posterior().","code":"describe_posterior(model, test = c(\"pd\", \"ROPE\", \"BF\")) > Summary of Posterior Distribution >  > Parameter   | Median |          95% CI |     pd |          ROPE | % in ROPE |  Rhat |      ESS |    BF > ------------------------------------------------------------------------------------------------------ > (Intercept) |  -6.12 | [-10.45, -2.25] | 99.92% | [-0.18, 0.18] |        0% | 1.000 | 26540.00 | 12.99 > Sepal.Width |   2.13 | [  0.79,  3.63] | 99.94% | [-0.18, 0.18] |        0% | 1.000 | 26693.00 | 13.98 library(performance)  model_performance(model) > # Indices of model performance >  > ELPD    | ELPD_SE |   LOOIC | LOOIC_SE |    WAIC |    R2 |  RMSE | Sigma | Log_loss | Score_log | Score_spherical > ----------------------------------------------------------------------------------------------------------------- > -66.284 |   3.052 | 132.568 |    6.104 | 132.562 | 0.099 | 0.477 | 1.000 |    0.643 |   -35.436 |           0.014"},{"path":"https://easystats.github.io/bayestestR/articles/example2.html","id":"visualise-the-indices","dir":"Articles","previous_headings":"Logistic Model","what":"Visualise the indices","title":"2. Confirmation of Bayesian skills","text":".","code":"library(see)  plot(rope(result))"},{"path":"https://easystats.github.io/bayestestR/articles/example2.html","id":"diagnostic-indices","dir":"Articles","previous_headings":"Logistic Model","what":"Diagnostic Indices","title":"2. Confirmation of Bayesian skills","text":"diagnostic indices Rhat ESS.","code":""},{"path":"https://easystats.github.io/bayestestR/articles/example3.html","id":"mixed-models","dir":"Articles","previous_headings":"","what":"Mixed Models","title":"3. Become a Bayesian master","text":"CONTINUED.","code":""},{"path":"https://easystats.github.io/bayestestR/articles/example3.html","id":"priors","dir":"Articles","previous_headings":"Mixed Models","what":"Priors","title":"3. Become a Bayesian master","text":"CONTINUED.","code":""},{"path":"https://easystats.github.io/bayestestR/articles/example3.html","id":"whats-next","dir":"Articles","previous_headings":"","what":"What’s next?","title":"3. Become a Bayesian master","text":"journey become true Bayesian master yet . merely beginning. now time leave bayestestR universe apply Bayesian framework variety statistical contexts: Marginal means Contrast analysis Testing Contrasts Bayesian Models ‘emmeans’ ‘bayestestR’","code":""},{"path":[]},{"path":"https://easystats.github.io/bayestestR/articles/guidelines.html","id":"how-to-describe-and-report-the-parameters-of-a-model","dir":"Articles","previous_headings":"Reporting Guidelines","what":"How to describe and report the parameters of a model","title":"Reporting Guidelines","text":"Bayesian analysis returns posterior distribution parameter (effect). minimally describe distributions, recommend reporting point-estimate centrality well information characterizing estimation uncertainty (dispersion). Additionally, one can also report indices effect existence /significance. Based previous comparison point-estimates indices effect existence, can draw following recommendations.","code":""},{"path":"https://easystats.github.io/bayestestR/articles/guidelines.html","id":"centrality","dir":"Articles","previous_headings":"Reporting Guidelines > How to describe and report the parameters of a model","what":"Centrality","title":"Reporting Guidelines","text":"suggest reporting median index centrality, robust compared mean MAP estimate. However, case severely skewed posterior distribution, MAP estimate good alternative.","code":""},{"path":"https://easystats.github.io/bayestestR/articles/guidelines.html","id":"uncertainty","dir":"Articles","previous_headings":"Reporting Guidelines > How to describe and report the parameters of a model","what":"Uncertainty","title":"Reporting Guidelines","text":"95% 89% Credible Intervals (CI) two reasonable ranges characterize uncertainty related estimation (see discussion differences two values). also recommend computing CIs based HDI rather quantiles, favouring probable central values. Note CI based quantile (equal-tailed interval) might appropriate case transformations (instance transforming log-odds probabilities). Otherwise, intervals originally cover null might cover transformation (see ).","code":""},{"path":"https://easystats.github.io/bayestestR/articles/guidelines.html","id":"existence","dir":"Articles","previous_headings":"Reporting Guidelines > How to describe and report the parameters of a model","what":"Existence","title":"Reporting Guidelines","text":"Reviewer 2 (circa long time ago galaxy far away). Bayesian framework can neatly delineate quantify different aspects hypothesis testing, effect existence significance. straightforward index describe existence effect Probability Direction (pd), representing certainty associated probable direction (positive negative) effect. index easy understand, simple interpret, straightforward compute, robust model characteristics, independent scale data. Moreover, strongly correlated frequentist p-value, can thus used draw parallels give reference readers non-familiar Bayesian statistics. two-sided p-value respectively .1, .05, .01 .001 correspond approximately pd* 95%, 97.5%, 99.5% 99.95%. Thus, convenience, suggest following reference values interpretation helpers: pd <= 95% ~ p > .1: uncertain pd > 95% ~ p < .1: possibly existing pd > 97%: likely existing pd > 99%: probably existing pd > 99.9%: certainly existing","code":""},{"path":"https://easystats.github.io/bayestestR/articles/guidelines.html","id":"significance","dir":"Articles","previous_headings":"Reporting Guidelines > How to describe and report the parameters of a model","what":"Significance","title":"Reporting Guidelines","text":"percentage ROPE index significance (primary meaning), informing us whether parameter related non-negligible change (terms magnitude) outcome. suggest reporting percentage full posterior distribution (full ROPE) instead given proportion CI ROPE, appears sensitive (especially delineate highly significant effects). Rather using binary, --nothing decision criterion, suggested original equivalence test, recommend using percentage continuous index significance. However, based simulation data, suggest following reference values interpretation helpers: > 99% ROPE: negligible (can accept null hypothesis) > 97.5% ROPE: probably negligible <= 97.5% & >= 2.5% ROPE: undecided significance < 2.5% ROPE: probably significant < 1% ROPE: significant (can reject null hypothesis) Note extra caution required interpretation highly depends parameters sample size ROPE range (see ).","code":""},{"path":"https://easystats.github.io/bayestestR/articles/guidelines.html","id":"template-sentence","dir":"Articles","previous_headings":"Reporting Guidelines > How to describe and report the parameters of a model","what":"Template Sentence","title":"Reporting Guidelines","text":"Based suggestions, template sentence minimal reporting parameter based posterior distribution : “effect X probability pd negative (Median = median, 89% CI [ HDIlow , HDIhigh ] can considered significant [ROPE% ROPE]).”","code":""},{"path":"https://easystats.github.io/bayestestR/articles/guidelines.html","id":"how-to-compare-different-models","dir":"Articles","previous_headings":"Reporting Guidelines","what":"How to compare different models","title":"Reporting Guidelines","text":"Although can also used assess effect existence significance, Bayes factor (BF) versatile index can used directly compare different models (data generation processes). Bayes factor ratio informs us much (less) likely observed data two compared models - usually model versus model without effect. Depending specifications null model (whether point-estimate (e.g., 0) interval), Bayes factor used context effect existence significance. general, Bayes factor greater 1 taken evidence favour one model (nominator), Bayes factor smaller 1 taken evidence favour model (denominator). Several rules thumb exist help interpretation (see ), > 3 one common threshold categorize non-anecdotal evidence.","code":""},{"path":"https://easystats.github.io/bayestestR/articles/guidelines.html","id":"template-sentence-1","dir":"Articles","previous_headings":"Reporting Guidelines > How to compare different models","what":"Template Sentence","title":"Reporting Guidelines","text":"reporting Bayes factors (BF), one can use following sentence: “moderate evidence favour absence effect x (BF = BF).”","code":""},{"path":"https://easystats.github.io/bayestestR/articles/guidelines.html","id":"suggestions","dir":"Articles","previous_headings":"","what":"Suggestions","title":"Reporting Guidelines","text":"advice, opinion , encourage let us know opening discussion thread making pull request.","code":""},{"path":"https://easystats.github.io/bayestestR/articles/indicesExistenceComparison.html","id":"indices-of-effect-existence-and-significance-in-the-bayesian-framework","dir":"Articles","previous_headings":"","what":"Indices of Effect Existence and Significance in the Bayesian Framework","title":"In-Depth 2: Comparison of Indices of Effect Existence and Significance","text":"comparison different Bayesian indices (pd, BFs, ROPE etc.) accessible . , case don’t wish read full article, following table summarizes key takeaways!","code":""},{"path":"https://easystats.github.io/bayestestR/articles/indicesExistenceComparison.html","id":"suggestions","dir":"Articles","previous_headings":"","what":"Suggestions","title":"In-Depth 2: Comparison of Indices of Effect Existence and Significance","text":"advice, opinion , encourage let us know opening discussion thread making pull request.","code":""},{"path":"https://easystats.github.io/bayestestR/articles/mediation.html","id":"mediation-analysis-in-brms-and-rstanarm","dir":"Articles","previous_headings":"","what":"Mediation Analysis in brms and rstanarm","title":"Mediation Analysis using Bayesian Regression Models","text":"mediation() summary function, especially mediation analysis, .e. multivariate response models casual mediation effects. models m2 m3, treat treatment effect job_seek mediator effect. brms model (m2), f1 describes mediator model f2 describes outcome model. similar rstanarm model. mediation() returns data frame information direct effect (median value posterior samples treatment outcome model), mediator effect (median value posterior samples mediator outcome model), indirect effect (median value multiplication posterior samples mediator outcome model posterior samples treatment mediation model) total effect (median value sums posterior samples used direct indirect effect). proportion mediated indirect effect divided total effect. simplest call just needs model-object. Typically, mediation() finds treatment mediator variables automatically. work, use treatment mediator arguments specify related variable names. values, 89% credible intervals calculated default. Use ci calculate different interval.","code":"library(bayestestR) library(mediation) library(brms) library(rstanarm)  # load sample data data(jobs)  set.seed(123) # linear models, for mediation analysis b1 <- lm(job_seek ~ treat + econ_hard + sex + age, data = jobs) b2 <- lm(depress2 ~ treat + job_seek + econ_hard + sex + age, data = jobs)  # mediation analysis, for comparison with brms m1 <- mediate(b1, b2, sims = 1000, treat = \"treat\", mediator = \"job_seek\") # Fit Bayesian mediation model in brms f1 <- bf(job_seek ~ treat + econ_hard + sex + age) f2 <- bf(depress2 ~ treat + job_seek + econ_hard + sex + age) m2 <- brm(f1 + f2 + set_rescor(FALSE), data = jobs, cores = 4) # Fit Bayesian mediation model in rstanarm m3 <- stan_mvmer(   list(     job_seek ~ treat + econ_hard + sex + age + (1 | occp),     depress2 ~ treat + job_seek + econ_hard + sex + age + (1 | occp)   ),   data = jobs,   cores = 4,   refresh = 0 ) # for brms mediation(m2) #> # Causal Mediation Analysis for Stan Model #>  #>   Treatment: treat #>   Mediator : job_seek #>   Response : depress2 #>  #> Effect                 | Estimate |          95% ETI #> ---------------------------------------------------- #> Direct Effect (ADE)    |   -0.040 | [-0.124,  0.046] #> Indirect Effect (ACME) |   -0.015 | [-0.041,  0.008] #> Mediator Effect        |   -0.240 | [-0.294, -0.185] #> Total Effect           |   -0.055 | [-0.145,  0.034] #>  #> Proportion mediated: 28.14% [-181.46%, 237.75%]  # for rstanarm mediation(m3) #> # Causal Mediation Analysis for Stan Model #>  #>   Treatment: treat #>   Mediator : job_seek #>   Response : depress2 #>  #> Effect                 | Estimate |          95% ETI #> ---------------------------------------------------- #> Direct Effect (ADE)    |   -0.040 | [-0.129,  0.048] #> Indirect Effect (ACME) |   -0.018 | [-0.042,  0.006] #> Mediator Effect        |   -0.241 | [-0.296, -0.187] #> Total Effect           |   -0.057 | [-0.151,  0.033] #>  #> Proportion mediated: 30.59% [-221.09%, 282.26%]"},{"path":"https://easystats.github.io/bayestestR/articles/mediation.html","id":"comparison-to-the-mediation-package","dir":"Articles","previous_headings":"","what":"Comparison to the mediation package","title":"Mediation Analysis using Bayesian Regression Models","text":"comparison mediation package. Note summary()-output mediation package shows indirect effect first, followed direct effect. want calculate mean instead median values posterior samples, use centrality-argument. Furthermore, print()-method, allows print digits. can see, results similar mediation package produces non-Bayesian models.","code":"summary(m1) #>  #> Causal Mediation Analysis  #>  #> Quasi-Bayesian Confidence Intervals #>  #>                Estimate 95% CI Lower 95% CI Upper p-value #> ACME            -0.0157      -0.0387         0.01    0.19 #> ADE             -0.0438      -0.1315         0.04    0.35 #> Total Effect    -0.0595      -0.1530         0.02    0.21 #> Prop. Mediated   0.2137      -2.0277         2.70    0.32 #>  #> Sample Size Used: 899  #>  #>  #> Simulations: 1000  mediation(m2, ci = 0.95) #> # Causal Mediation Analysis for Stan Model #>  #>   Treatment: treat #>   Mediator : job_seek #>   Response : depress2 #>  #> Effect                 | Estimate |          95% ETI #> ---------------------------------------------------- #> Direct Effect (ADE)    |   -0.040 | [-0.124,  0.046] #> Indirect Effect (ACME) |   -0.015 | [-0.041,  0.008] #> Mediator Effect        |   -0.240 | [-0.294, -0.185] #> Total Effect           |   -0.055 | [-0.145,  0.034] #>  #> Proportion mediated: 28.14% [-181.46%, 237.75%]  mediation(m3, ci = 0.95) #> # Causal Mediation Analysis for Stan Model #>  #>   Treatment: treat #>   Mediator : job_seek #>   Response : depress2 #>  #> Effect                 | Estimate |          95% ETI #> ---------------------------------------------------- #> Direct Effect (ADE)    |   -0.040 | [-0.129,  0.048] #> Indirect Effect (ACME) |   -0.018 | [-0.042,  0.006] #> Mediator Effect        |   -0.241 | [-0.296, -0.187] #> Total Effect           |   -0.057 | [-0.151,  0.033] #>  #> Proportion mediated: 30.59% [-221.09%, 282.26%] m <- mediation(m2, centrality = \"mean\", ci = 0.95) print(m, digits = 4) #> # Causal Mediation Analysis for Stan Model #>  #>   Treatment: treat #>   Mediator : job_seek #>   Response : depress2 #>  #> Effect                 | Estimate |            95% ETI #> ------------------------------------------------------ #> Direct Effect (ADE)    |  -0.0395 | [-0.1237,  0.0456] #> Indirect Effect (ACME) |  -0.0158 | [-0.0405,  0.0083] #> Mediator Effect        |  -0.2401 | [-0.2944, -0.1846] #> Total Effect           |  -0.0553 | [-0.1454,  0.0341] #>  #> Proportion mediated: 28.60% [-181.01%, 238.20%]"},{"path":"https://easystats.github.io/bayestestR/articles/mediation.html","id":"comparison-to-sem-from-the-lavaan-package","dir":"Articles","previous_headings":"","what":"Comparison to SEM from the lavaan package","title":"Mediation Analysis using Bayesian Regression Models","text":"Finally, also compare results SEM model, using lavaan. example demonstrate “translate” model different packages modeling approached. summary output lavaan longer, can find related numbers quite easily: direct effect treatment treat (c1), -0.040 indirect effect treatment indirect_treat, -0.016 mediator effect job_seek job_seek (b), -0.240 total effect total_treat, -0.056","code":"library(lavaan) data(jobs) set.seed(1234)  model <- \" # direct effects              depress2 ~ c1*treat + c2*econ_hard + c3*sex + c4*age + b*job_seek             # mediation              job_seek ~ a1*treat + a2*econ_hard + a3*sex + a4*age             # indirect effects (a*b)              indirect_treat := a1*b              indirect_econ_hard := a2*b              indirect_sex := a3*b              indirect_age := a4*b             # total effects              total_treat := c1 + (a1*b)              total_econ_hard := c2 + (a2*b)              total_sex := c3 + (a3*b)              total_age := c4 + (a4*b)          \" m4 <- sem(model, data = jobs) summary(m4) #> lavaan 0.6.14 ended normally after 1 iteration #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        11 #>  #>   Number of observations                           899 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                 0.000 #>   Degrees of freedom                                 0 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Regressions: #>                    Estimate  Std.Err  z-value  P(>|z|) #>   depress2 ~                                           #>     treat     (c1)   -0.040    0.043   -0.929    0.353 #>     econ_hard (c2)    0.149    0.021    7.156    0.000 #>     sex       (c3)    0.107    0.041    2.604    0.009 #>     age       (c4)    0.001    0.002    0.332    0.740 #>     job_seek   (b)   -0.240    0.028   -8.524    0.000 #>   job_seek ~                                           #>     treat     (a1)    0.066    0.051    1.278    0.201 #>     econ_hard (a2)    0.053    0.025    2.167    0.030 #>     sex       (a3)   -0.008    0.049   -0.157    0.875 #>     age       (a4)    0.005    0.002    1.983    0.047 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|) #>    .depress2          0.373    0.018   21.201    0.000 #>    .job_seek          0.524    0.025   21.201    0.000 #>  #> Defined Parameters: #>                    Estimate  Std.Err  z-value  P(>|z|) #>     indirect_treat   -0.016    0.012   -1.264    0.206 #>     indirct_cn_hrd   -0.013    0.006   -2.100    0.036 #>     indirect_sex      0.002    0.012    0.157    0.875 #>     indirect_age     -0.001    0.001   -1.932    0.053 #>     total_treat      -0.056    0.045   -1.244    0.214 #>     total_econ_hrd    0.136    0.022    6.309    0.000 #>     total_sex         0.109    0.043    2.548    0.011 #>     total_age        -0.000    0.002   -0.223    0.824  # just to have the numbers right at hand and you don't need to scroll up mediation(m2, ci = 0.95) #> # Causal Mediation Analysis for Stan Model #>  #>   Treatment: treat #>   Mediator : job_seek #>   Response : depress2 #>  #> Effect                 | Estimate |          95% ETI #> ---------------------------------------------------- #> Direct Effect (ADE)    |   -0.040 | [-0.124,  0.046] #> Indirect Effect (ACME) |   -0.015 | [-0.041,  0.008] #> Mediator Effect        |   -0.240 | [-0.294, -0.185] #> Total Effect           |   -0.055 | [-0.145,  0.034] #>  #> Proportion mediated: 28.14% [-181.46%, 237.75%]"},{"path":"https://easystats.github.io/bayestestR/articles/overview_of_vignettes.html","id":"function-overview","dir":"Articles","previous_headings":"","what":"Function Overview","title":"Overview of Vignettes","text":"Function Reference","code":""},{"path":"https://easystats.github.io/bayestestR/articles/overview_of_vignettes.html","id":"get-started","dir":"Articles","previous_headings":"","what":"Get Started","title":"Overview of Vignettes","text":"Get Started Bayesian Analysis","code":""},{"path":"https://easystats.github.io/bayestestR/articles/overview_of_vignettes.html","id":"examples","dir":"Articles","previous_headings":"","what":"Examples","title":"Overview of Vignettes","text":"Initiation Bayesian models Confirmation Bayesian skills Become Bayesian master","code":""},{"path":"https://easystats.github.io/bayestestR/articles/overview_of_vignettes.html","id":"articles","dir":"Articles","previous_headings":"","what":"Articles","title":"Overview of Vignettes","text":"Credible Intervals (CI)) Region Practical Equivalence (ROPE) Probability Direction (pd) Bayes Factors","code":""},{"path":"https://easystats.github.io/bayestestR/articles/overview_of_vignettes.html","id":"in-depths","dir":"Articles","previous_headings":"","what":"In-Depths","title":"Overview of Vignettes","text":"Comparison Point-Estimates Indices Effect Existence Significance Bayesian Framework Mediation Analysis using Bayesian Regression Models","code":""},{"path":"https://easystats.github.io/bayestestR/articles/overview_of_vignettes.html","id":"guidelines","dir":"Articles","previous_headings":"","what":"Guidelines","title":"Overview of Vignettes","text":"Reporting Guidelines","code":""},{"path":"https://easystats.github.io/bayestestR/articles/probability_of_direction.html","id":"what-is-the-pd","dir":"Articles","previous_headings":"","what":"What is the pd?","title":"Probability of Direction (pd)","text":"Probability Direction (pd) index effect existence, ranging 50% 100%, representing certainty effect goes particular direction (.e., positive negative). Beyond simplicity interpretation, understanding computation, index also presents interesting properties: independent model: solely based posterior distributions require additional information data model. robust scale response variable predictors. strongly correlated frequentist p-value, can thus used draw parallels give reference readers non-familiar Bayesian statistics. However, index relevant assess magnitude importance effect (meaning “significance”), better achieved indices ROPE percentage. fact, indices significance existence totally independent. can effect pd 99.99%, whole posterior distribution concentrated within [0.0001, 0.0002] range. case, effect positive high certainty, also significant (.e., small). Indices effect existence, pd, particularly useful exploratory research clinical studies, focus make sure effect interest opposite direction (clinical studies, treatment harmful). However, effect’s direction confirmed, focus shift toward significance, including precise estimation magnitude, relevance importance.","code":""},{"path":"https://easystats.github.io/bayestestR/articles/probability_of_direction.html","id":"relationship-with-the-p-value","dir":"Articles","previous_headings":"","what":"Relationship with the p-value","title":"Probability of Direction (pd)","text":"cases, seems pd direct correspondence frequentist one-sided p-value formula: \\[p_{one-sided} = 1-p_d\\] Similarly, two-sided p-value (commonly reported one) equivalent formula: \\[p_{two-sided} = 2*(1-p_d)\\] Thus, two-sided p-value respectively .1, .05, .01 .001 correspond approximately pd 95%, 97.5%, 99.5% 99.95% . Correlation frequentist p-value probability direction (pd) ’s like p-value, must bad p-value bad [insert reference reproducibility crisis]. fact, aspect reproducibility crisis might misunderstood. Indeed, p-value intrinsically bad wrong. Instead, misuse, misunderstanding misinterpretation fuels decay situation. instance, fact pd highly correlated p-value suggests latter index effect existence significance (.e., “worth interest”). Bayesian version, pd, intuitive meaning makes obvious fact thresholds arbitrary. Additionally, mathematical interpretative transparency pd, reconceptualisation index effect existence, offers valuable insight characterization Bayesian results. Moreover, concomitant proximity frequentist p-value makes perfect metric ease transition psychological research adoption Bayesian framework.","code":""},{"path":"https://easystats.github.io/bayestestR/articles/probability_of_direction.html","id":"methods-of-computation","dir":"Articles","previous_headings":"","what":"Methods of computation","title":"Probability of Direction (pd)","text":"simple direct way compute pd 1) look median’s sign, 2) select portion posterior sign 3) compute percentage portion represents. “simple” method straightforward, precision directly tied number posterior draws. second approach relies density estimation. starts estimating density function (many methods available), computing area curve (AUC) density curve side 0. density-based method hypothetically considered precise, strongly depends method used estimate density function.","code":""},{"path":"https://easystats.github.io/bayestestR/articles/probability_of_direction.html","id":"methods-comparison","dir":"Articles","previous_headings":"","what":"Methods comparison","title":"Probability of Direction (pd)","text":"Let’s compare 4 available methods, direct method 3 density-based methods differing density estimation algorithm (see estimate_density).","code":""},{"path":"https://easystats.github.io/bayestestR/articles/probability_of_direction.html","id":"correlation","dir":"Articles","previous_headings":"Methods comparison","what":"Correlation","title":"Probability of Direction (pd)","text":"Let’s start testing proximity similarity results obtained different methods. methods give highly correlated give similar results. means method choice drastic game changer used tweak results much.","code":"library(bayestestR) library(logspline) library(KernSmooth)  # Compute the correlations data <- data.frame() for (the_mean in runif(25, 0, 4)) {   for (the_sd in runif(25, 0.5, 4)) {     x <- rnorm(100, the_mean, abs(the_sd))     data <- rbind(       data,       data.frame(         \"direct\" = pd(x),         \"kernel\" = pd(x, method = \"kernel\"),         \"logspline\" = pd(x, method = \"logspline\"),         \"KernSmooth\" = pd(x, method = \"KernSmooth\")       )     )   } } data <- as.data.frame(sapply(data, as.numeric))  # Visualize the correlations bayesplot::mcmc_pairs(data) +   theme_classic() > NULL"},{"path":"https://easystats.github.io/bayestestR/articles/probability_of_direction.html","id":"accuracy","dir":"Articles","previous_headings":"Methods comparison","what":"Accuracy","title":"Probability of Direction (pd)","text":"test accuracy methods, start computing direct pd dense distribution (large amount observations). baseline, “true” pd. , iteratively draw smaller samples parent distribution, compute pd different methods. closer estimate reference one, better.  “Kernel” based density methods seems consistently underestimate pd. Interestingly, “direct” method appears reliable, even case small number posterior draws.","code":"data <- data.frame() for (i in 1:25) {   the_mean <- runif(1, 0, 4)   the_sd <- abs(runif(1, 0.5, 4))   parent_distribution <- rnorm(100000, the_mean, the_sd)   true_pd <- pd(parent_distribution)    for (j in 1:25) {     sample_size <- round(runif(1, 25, 5000))     subsample <- sample(parent_distribution, sample_size)     data <- rbind(       data,       data.frame(         \"sample_size\" = sample_size,         \"true\" = true_pd,         \"direct\" = pd(subsample) - true_pd,         \"kernel\" = pd(subsample, method = \"kernel\") - true_pd,         \"logspline\" = pd(subsample, method = \"logspline\") - true_pd,         \"KernSmooth\" = pd(subsample, method = \"KernSmooth\") - true_pd       )     )   } } data <- as.data.frame(sapply(data, as.numeric)) library(datawizard) # for reshape_longer  data <- reshape_longer(data, select = 3:6, names_to = \"Method\", values_to = \"Distance\")  ggplot(data, aes(x = sample_size, y = Distance, color = Method, fill = Method)) +   geom_point(alpha = 0.3, stroke = 0, shape = 16) +   geom_smooth(alpha = 0.2) +   geom_hline(yintercept = 0) +   theme_classic() +   xlab(\"\\nDistribution Size\")"},{"path":"https://easystats.github.io/bayestestR/articles/probability_of_direction.html","id":"can-the-pd-be-100","dir":"Articles","previous_headings":"Methods comparison","what":"Can the pd be 100%?","title":"Probability of Direction (pd)","text":"p = 0.000 coined one term avoid reporting results (Lilienfeld et al., 2015), even often displayed statistical software. rationale every probability distribution, value probability exactly 0. always infinitesimal probability associated data point, p = 0.000 returned software due approximations related, among , finite memory hardware. One apply rationale pd: since data points non-null probability density, pd (particular portion probability density) can never 100%. entirely valid point, people using direct method might argue pd based posterior draws, rather theoretical, hidden, true posterior distribution (approximated posterior draws). posterior draws represent finite sample pd = 100% valid statement.","code":""},{"path":"https://easystats.github.io/bayestestR/articles/region_of_practical_equivalence.html","id":"what-is-the-rope","dir":"Articles","previous_headings":"","what":"What is the ROPE?","title":"Region of Practical Equivalence (ROPE)","text":"Unlike frequentist approach, Bayesian inference based statistical significance, effects tested “zero”. Indeed, Bayesian framework offers probabilistic view parameters, allowing assessment uncertainty related . Thus, rather concluding effect present simply differs zero, conclude probability outside specific range can considered “practically effect” (.e., negligible magnitude) sufficient. range called region practical equivalence (ROPE). Indeed, statistically, probability posterior distribution different 0 make much sense (probability different single point infinite). Therefore, idea underlining ROPE let user define area around null value enclosing values equivalent null value practical purposes (J. Kruschke, 2014; J. K. Kruschke, 2010; J. K. Kruschke, Aguinis, & Joo, 2012).","code":""},{"path":"https://easystats.github.io/bayestestR/articles/region_of_practical_equivalence.html","id":"equivalence-test","dir":"Articles","previous_headings":"","what":"Equivalence Test","title":"Region of Practical Equivalence (ROPE)","text":"ROPE, region corresponding “null” hypothesis, used equivalence test, test whether parameter significant (sense important enough cared ). test usually based “HDI+ROPE decision rule” (J. Kruschke, 2014; J. K. Kruschke & Liddell, 2018) check whether parameter values accepted rejected explicitly formulated “null hypothesis” (.e., ROPE). words, checks percentage Credible Interval (CI) null region (ROPE). percentage sufficiently low, null hypothesis rejected. percentage sufficiently high, null hypothesis accepted.","code":""},{"path":"https://easystats.github.io/bayestestR/articles/region_of_practical_equivalence.html","id":"credible-interval-in-rope-vs-full-posterior-in-rope","dir":"Articles","previous_headings":"","what":"Credible interval in ROPE vs full posterior in ROPE","title":"Region of Practical Equivalence (ROPE)","text":"Using ROPE HDI Credible Interval, Kruschke (2018) suggests using percentage 95% HDI falls within ROPE decision rule. However, 89% HDI considered better choice (J. Kruschke, 2014; R. McElreath, 2014; Richard McElreath, 2018), bayestestR provides default percentage 89% HDI falls within ROPE. However, simulation studies data suggest using percentage full posterior distribution, instead CI, might sensitive (especially delineate highly significant effects). Thus, recommend user considers using full ROPE percentage (setting ci = 1), return portion entire posterior distribution ROPE.","code":""},{"path":"https://easystats.github.io/bayestestR/articles/region_of_practical_equivalence.html","id":"what-percentage-in-rope-to-accept-or-to-reject","dir":"Articles","previous_headings":"","what":"What percentage in ROPE to accept or to reject?","title":"Region of Practical Equivalence (ROPE)","text":"HDI completely outside ROPE, “null hypothesis” parameter “rejected”. ROPE completely covers HDI, .e., credible values parameter inside region practical equivalence, null hypothesis accepted. Else, ’s unclear whether null hypothesis accepted rejected. full ROPE used (.e., 100% HDI), null hypothesis rejected accepted percentage posterior within ROPE smaller 2.5% greater 97.5%. Desirable results low proportions inside ROPE (closer zero better).","code":""},{"path":"https://easystats.github.io/bayestestR/articles/region_of_practical_equivalence.html","id":"how-to-define-the-rope-range","dir":"Articles","previous_headings":"","what":"How to define the ROPE range?","title":"Region of Practical Equivalence (ROPE)","text":"Kruschke (2018) suggests ROPE set, default, range -0.1 0.1 standardized parameter (negligible effect size according Cohen, 1988). linear models (lm), can generalised : \\[[-0.1*SD_{y}, 0.1*SD_{y}]\\]. logistic models, parameters expressed log odds ratio can converted standardized difference formula: \\[\\pi/\\sqrt{3}\\] (see effectsize package, resulting range -0.18 -0.18. models binary outcome, strongly recommended manually specify rope argument. Currently, default applied logistic models. t-tests, standard deviation response used, similarly linear models (see ). correlations, -0.05, 0.05 used, .e., half value negligible correlation suggested Cohen’s (1988) rules thumb. models, -0.1, 0.1 used determine ROPE limits, strongly advised specify manually.","code":""},{"path":"https://easystats.github.io/bayestestR/articles/region_of_practical_equivalence.html","id":"sensitivity-to-parameters-scale","dir":"Articles","previous_headings":"","what":"Sensitivity to parameter’s scale","title":"Region of Practical Equivalence (ROPE)","text":"important consider unit (.e., scale) predictors using index based ROPE, correct interpretation ROPE representing region practical equivalence zero dependent scale predictors. Indeed, unlike indices (pd), percentage ROPE depend unit parameter. words, ROPE represents fixed portion response’s scale, proximity coefficient depends scale coefficient . instance, consider simple regression growth ~ time, modelling development Wookies babies, negligible change (ROPE) less 54 cm. time variable expressed days, find coefficient (representing growth day) 10 cm (median posterior coefficient 10). consider negligible. However, decide express time variable years, coefficient scaled transformation (now represent growth year). coefficient now around 3550 cm (10 * 355), now consider significant.   can see pd percentage ROPE linear relationship Sepal.Length Sepal.Width respectively 92.95% 15.95%, corresponding uncertain significant effect. happen scale predictor?   can see, simply dividing predictor 100, drastically changed conclusion related percentage ROPE (became close 0): effect now interpreted significant. Thus, recommend paying close attention unit predictors selecting ROPE range (e.g., coefficient correspond small effect?), reporting reading ROPE results.","code":"library(rstanarm) library(bayestestR) library(see)  data <- iris # Use the iris data model <- stan_glm(Sepal.Length ~ Sepal.Width, data = data) # Fit model # Compute indices pd <- p_direction(model) percentage_in_rope <- rope(model, ci = 1)  # Visualise the pd plot(pd) pd > Probability of Direction >  > Parameter   |     pd > -------------------- > (Intercept) |   100% > Sepal.Width | 91.65% # Visualise the percentage in ROPE plot(percentage_in_rope) percentage_in_rope > # Proportion of samples inside the ROPE [-0.08, 0.08]: >  > Parameter   | inside ROPE > ------------------------- > (Intercept) |      0.00 % > Sepal.Width |     16.28 % data$Sepal.Width_scaled <- data$Sepal.Width / 100 # Divide predictor by 100 model <- stan_glm(Sepal.Length ~ Sepal.Width_scaled, data = data) # Fit model # Compute indices pd <- p_direction(model) percentage_in_rope <- rope(model, ci = 1)  # Visualise the pd plot(pd) pd > Probability of Direction >  > Parameter          |     pd > --------------------------- > (Intercept)        |   100% > Sepal.Width_scaled | 91.65% # Visualise the percentage in ROPE plot(percentage_in_rope) percentage_in_rope > # Proportion of samples inside the ROPE [-0.08, 0.08]: >  > Parameter          | inside ROPE > -------------------------------- > (Intercept)        |      0.00 % > Sepal.Width_scaled |      0.10 %"},{"path":"https://easystats.github.io/bayestestR/articles/region_of_practical_equivalence.html","id":"multicollinearity-non-independent-covariates","dir":"Articles","previous_headings":"","what":"Multicollinearity: Non-independent covariates","title":"Region of Practical Equivalence (ROPE)","text":"parameters show strong correlations, .e., covariates independent, joint parameter distributions may shift towards away ROPE. Collinearity invalidates ROPE hypothesis testing based univariate marginals, probabilities conditional independence. problematic parameters partial overlap ROPE region. case collinearity, (joint) distributions parameters may either get increased decreased ROPE, means inferences based ROPE inappropriate (J. Kruschke, 2014). equivalence_test() rope() functions perform simple check pairwise correlations parameters, can collinearity two variables, first step check assumptions hypothesis testing look different pair plots. even sophisticated check projection predictive variable selection (Piironen & Vehtari, 2017).","code":""},{"path":[]},{"path":"https://easystats.github.io/bayestestR/articles/web_only/indicesEstimationComparison.html","id":"introduction","dir":"Articles > Web_only","previous_headings":"Effect Point-Estimates in the Bayesian Framework","what":"Introduction","title":"In-Depth 1: Comparison of Point-Estimates","text":"One main difference Bayesian frequentist frameworks former returns probability distribution effect (.e., model parameter interest, regression slope) instead single value. However, still need demand - reporting use analysis - single value (point-estimate) best characterises underlying posterior distribution. three main indices used literature effect estimation: - mean - median - MAP (Maximum Posteriori) estimate (roughly corresponding mode - “peak” - distribution) Unfortunately, consensus one use, systematic comparison ever done. present work, compare three point-estimates effect , well widely known beta, extracted comparable frequentist model. comparisons can help us draw bridges relationships two influential statistical frameworks.","code":""},{"path":[]},{"path":"https://easystats.github.io/bayestestR/articles/web_only/indicesEstimationComparison.html","id":"methods","dir":"Articles > Web_only","previous_headings":"Effect Point-Estimates in the Bayesian Framework > Experiment 1: Relationship with Error (Noise) and Sample Size","what":"Methods","title":"In-Depth 1: Comparison of Point-Estimates","text":"carrying simulation aimed modulating following characteristics: Model type: linear logistic. “True” effect (known parameters values data drawn): Can 1 0 (effect). Sample size: 20 100 steps 10. Error: Gaussian noise applied predictor SD uniformly spread 0.33 6.66 (1000 different values). generated dataset combination characteristics, resulting total 2 * 2 * 9 * 1000 = 36000 Bayesian frequentist models. code used generation available (please note takes usually several days/weeks complete).","code":"library(ggplot2) library(datawizard) library(see) library(parameters)  df <- read.csv(\"https://raw.github.com/easystats/circus/master/data/bayesSim_study1.csv\")"},{"path":[]},{"path":"https://easystats.github.io/bayestestR/articles/web_only/indicesEstimationComparison.html","id":"sensitivity-to-noise","dir":"Articles > Web_only","previous_headings":"Effect Point-Estimates in the Bayesian Framework > Experiment 1: Relationship with Error (Noise) and Sample Size > Results","what":"Sensitivity to Noise","title":"In-Depth 1: Comparison of Point-Estimates","text":"","code":"dat <- df dat <- data_select(dat, select = c(\"error\", \"true_effect\", \"outcome_type\", \"Coefficient\", \"Median\", \"Mean\", \"MAP\")) dat <- reshape_longer(   dat,   select = -c(\"error\", \"true_effect\", \"outcome_type\"),   names_to = \"estimate\",   values_to = \"value\" ) dat$temp <- as.factor(cut(dat$error, 10, labels = FALSE))  tmp <- lapply(split(dat, dat$temp), function(x) {   x$error_group <- rep(round(mean(x$error), 1), times = nrow(x))   return(x) })  dat <- do.call(rbind, tmp) dat <- data_filter(dat, value < 6)  ggplot(dat, aes(x = error_group, y = value, fill = estimate, group = interaction(estimate, error_group))) +   # geom_hline(yintercept = 0) +   # geom_point(alpha=0.05, size=2, stroke = 0, shape=16) +   # geom_smooth(method=\"loess\") +   geom_boxplot(outlier.shape = NA) +   theme_modern() +   scale_fill_manual(     values = c(\"Coefficient\" = \"#607D8B\", \"MAP\" = \"#795548\", \"Mean\" = \"#FF9800\", \"Median\" = \"#FFEB3B\"),     name = \"Index\"   ) +   ylab(\"Point-estimate\") +   xlab(\"Noise\") +   facet_wrap(~ outcome_type * true_effect, scales = \"free\")"},{"path":"https://easystats.github.io/bayestestR/articles/web_only/indicesEstimationComparison.html","id":"sensitivity-to-sample-size","dir":"Articles > Web_only","previous_headings":"Effect Point-Estimates in the Bayesian Framework > Experiment 1: Relationship with Error (Noise) and Sample Size > Results","what":"Sensitivity to Sample Size","title":"In-Depth 1: Comparison of Point-Estimates","text":"","code":"dat <- df dat <- data_select(dat, select = c(\"sample_size\", \"true_effect\", \"outcome_type\", \"Coefficient\", \"Median\", \"Mean\", \"MAP\")) dat <- reshape_longer(   dat,   select = -c(\"sample_size\", \"true_effect\", \"outcome_type\"),   names_to = \"estimate\",   values_to = \"value\" ) dat$temp <- as.factor(cut(dat$sample_size, 10, labels = FALSE))  tmp <- lapply(split(dat, dat$temp), function(x) {   x$size_group <- rep(round(mean(x$sample_size), 1), times = nrow(x))   return(x) })  dat <- do.call(rbind, tmp) dat <- data_filter(dat, value < 6)  ggplot(dat, aes(x = size_group, y = value, fill = estimate, group = interaction(estimate, size_group))) +   # geom_hline(yintercept = 0) +   # geom_point(alpha=0.05, size=2, stroke = 0, shape=16) +   # geom_smooth(method=\"loess\") +   geom_boxplot(outlier.shape = NA) +   theme_modern() +   scale_fill_manual(     values = c(\"Coefficient\" = \"#607D8B\", \"MAP\" = \"#795548\", \"Mean\" = \"#FF9800\", \"Median\" = \"#FFEB3B\"),     name = \"Index\"   ) +   ylab(\"Point-estimate\") +   xlab(\"Sample size\") +   facet_wrap(~ outcome_type * true_effect, scales = \"free\")"},{"path":"https://easystats.github.io/bayestestR/articles/web_only/indicesEstimationComparison.html","id":"statistical-modelling","dir":"Articles > Web_only","previous_headings":"Effect Point-Estimates in the Bayesian Framework > Experiment 1: Relationship with Error (Noise) and Sample Size > Results","what":"Statistical Modelling","title":"In-Depth 1: Comparison of Point-Estimates","text":"fitted (frequentist) multiple linear regression statistically test predict presence absence effect estimates well interaction noise sample size. suggests , order delineate presence absence effect, compared frequentist’s beta coefficient: linear models, Mean better predictor, closely followed Median, MAP frequentist Coefficient. logistic models, MAP better predictor, followed Median, Mean , behind, frequentist Coefficient. Overall, median appears safe choice, maintaining high performance across different types models.","code":"dat <- df dat <- data_select(dat, select = c(\"sample_size\", \"true_effect\", \"outcome_type\", \"Coefficient\", \"Median\", \"Mean\", \"MAP\")) dat <- reshape_longer(   dat,   select = -c(\"sample_size\", \"error\", \"true_effect\", \"outcome_type\"),   names_to = \"estimate\",   values_to = \"value\" )  out <- glm(true_effect ~ outcome_type / estimate / value, data = dat, family = \"binomial\") out <- parameters(out, ci_method = \"wald\") out <- data_select(out, c(\"Parameter\", \"Coefficient\", \"p\")) out <- data_filter(out, grep(\"^outcome_type(.*):value$\", x = out$Parameter)) out <- out[order(out$Coefficient, decreasing = TRUE), ] knitr::kable(out, digits = 2)"},{"path":[]},{"path":"https://easystats.github.io/bayestestR/articles/web_only/indicesEstimationComparison.html","id":"methods-1","dir":"Articles > Web_only","previous_headings":"Effect Point-Estimates in the Bayesian Framework > Experiment 2: Relationship with Sampling Characteristics","what":"Methods","title":"In-Depth 1: Comparison of Point-Estimates","text":"carrying another simulation aimed modulating following characteristics: Model type: linear logistic. “True” effect (original regression coefficient data drawn): Can 1 0 (effect). draws: 10 5000 step 5 (1000 iterations). warmup: Ratio warmup iterations. 1/10 9/10 step 0.1 (9 iterations). generated 3 datasets combination characteristics, resulting total 2 * 2 * 8 * 40 * 9 * 3 = 34560 Bayesian frequentist models. code used generation avaible (please note takes usually several days/weeks complete).","code":"df <- read.csv(\"https://raw.github.com/easystats/circus/master/data/bayesSim_study2.csv\")"},{"path":[]},{"path":"https://easystats.github.io/bayestestR/articles/web_only/indicesEstimationComparison.html","id":"sensitivity-to-number-of-iterations","dir":"Articles > Web_only","previous_headings":"Effect Point-Estimates in the Bayesian Framework > Experiment 2: Relationship with Sampling Characteristics > Results","what":"Sensitivity to number of iterations","title":"In-Depth 1: Comparison of Point-Estimates","text":"","code":"dat <- df dat <- data_select(dat, select = c(\"iterations\", \"true_effect\", \"outcome_type\", \"beta\", \"Median\", \"Mean\", \"MAP\")) dat <- reshape_longer(   dat,   select = -c(\"iterations\", \"true_effect\", \"outcome_type\"),   names_to = \"estimate\",   values_to = \"value\" ) dat$temp <- as.factor(cut(dat$iterations, 5, labels = FALSE))  tmp <- lapply(split(dat, dat$temp), function(x) {   x$iterations_group <- rep(round(mean(x$iterations), 1), times = nrow(x))   return(x) })  dat <- do.call(rbind, tmp) dat <- data_filter(dat, value < 6)  ggplot(dat, aes(x = iterations_group, y = value, fill = estimate, group = interaction(estimate, iterations_group))) +   geom_boxplot(outlier.shape = NA) +   theme_classic() +   scale_fill_manual(     values = c(\"beta\" = \"#607D8B\", \"MAP\" = \"#795548\", \"Mean\" = \"#FF9800\", \"Median\" = \"#FFEB3B\"),     name = \"Index\"   ) +   ylab(\"Point-estimate of the true value 0\\n\") +   xlab(\"\\nNumber of Iterations\") +   facet_wrap(~ outcome_type * true_effect, scales = \"free\")"},{"path":"https://easystats.github.io/bayestestR/articles/web_only/indicesEstimationComparison.html","id":"sensitivity-to-warmup-ratio","dir":"Articles > Web_only","previous_headings":"Effect Point-Estimates in the Bayesian Framework > Experiment 2: Relationship with Sampling Characteristics > Results","what":"Sensitivity to warmup ratio","title":"In-Depth 1: Comparison of Point-Estimates","text":"","code":"dat <- df dat$warmup <- dat$warmup / dat$iterations dat <- data_select(dat, select = c(\"warmup\", \"true_effect\", \"outcome_type\", \"beta\", \"Median\", \"Mean\", \"MAP\")) dat <- reshape_longer(   dat,   select = -c(\"warmup\", \"true_effect\", \"outcome_type\"),   names_to = \"estimate\",   values_to = \"value\" ) dat$temp <- as.factor(cut(dat$warmup, 3, labels = FALSE))  tmp <- lapply(split(dat, dat$temp), function(x) {   x$warmup_group <- rep(round(mean(x$warmup), 1), times = nrow(x))   return(x) })  dat <- do.call(rbind, tmp) dat <- data_filter(dat, value < 6)  ggplot(dat, aes(x = warmup_group, y = value, fill = estimate, group = interaction(estimate, warmup_group))) +   geom_boxplot(outlier.shape = NA) +   theme_classic() +   scale_fill_manual(     values = c(\"beta\" = \"#607D8B\", \"MAP\" = \"#795548\", \"Mean\" = \"#FF9800\", \"Median\" = \"#FFEB3B\"),     name = \"Index\"   ) +   ylab(\"Point-estimate of the true value 0\\n\") +   xlab(\"\\nNumber of Iterations\") +   facet_wrap(~ outcome_type * true_effect, scales = \"free\")"},{"path":"https://easystats.github.io/bayestestR/articles/web_only/indicesEstimationComparison.html","id":"discussion","dir":"Articles > Web_only","previous_headings":"Effect Point-Estimates in the Bayesian Framework","what":"Discussion","title":"In-Depth 1: Comparison of Point-Estimates","text":"Conclusions can found guidelines section article.","code":""},{"path":"https://easystats.github.io/bayestestR/articles/web_only/indicesEstimationComparison.html","id":"suggestions","dir":"Articles > Web_only","previous_headings":"","what":"Suggestions","title":"In-Depth 1: Comparison of Point-Estimates","text":"advice, opinion , encourage let us know opening discussion thread making pull request.","code":""},{"path":"https://easystats.github.io/bayestestR/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Dominique Makowski. Author, maintainer.            @Dom_Makowski Daniel Lüdecke. Author.            @strengejacke Mattan S. Ben-Shachar. Author.            @mattansb Indrajeet Patil. Author.            @patilindrajeets Michael D. Wilson. Author. Brenton M. Wiernik. Author.            @bmwiernik Paul-Christian Bürkner. Reviewer. Tristan Mahr. Reviewer. Henrik Singmann. Contributor. Quentin F. Gronau. Contributor. Sam Crawley. Contributor.","code":""},{"path":"https://easystats.github.io/bayestestR/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Makowski, D., Ben-Shachar, M., & Lüdecke, D. (2019). bayestestR: Describing Effects Uncertainty, Existence Significance within Bayesian Framework. Journal Open Source Software, 4(40), 1541. doi:10.21105/joss.01541","code":"@Article{,   title = {bayestestR: Describing Effects and their Uncertainty, Existence and Significance within the Bayesian Framework.},   author = {Dominique Makowski and Mattan S. Ben-Shachar and Daniel Lüdecke},   journal = {Journal of Open Source Software},   doi = {10.21105/joss.01541},   year = {2019},   number = {40},   volume = {4},   pages = {1541},   url = {https://joss.theoj.org/papers/10.21105/joss.01541}, }"},{"path":"https://easystats.github.io/bayestestR/index.html","id":"bayestestr-","dir":"","previous_headings":"","what":"Understand and Describe Bayesian Models and Posterior Distributions","title":"Understand and Describe Bayesian Models and Posterior Distributions","text":"Become Bayesian master ⚠️ changed default CI width! Please make informed decision set explicitly (ci = 0.89, ci = 0.95 anything else decide) ⚠️ Existing R packages allow users easily fit large variety models extract visualize posterior draws. However, packages return limited set indices (e.g., point-estimates CIs). bayestestR provides comprehensive consistent set functions analyze describe posterior distributions generated variety models objects, including popular modeling packages rstanarm, brms BayesFactor. can reference package documentation follows: Makowski, D., Ben-Shachar, M. S., & Lüdecke, D. (2019). bayestestR: Describing Effects Uncertainty, Existence Significance within Bayesian Framework. Journal Open Source Software, 4(40), 1541. 10.21105/joss.01541 Makowski, D., Ben-Shachar, M. S., Chen, S. H. ., & Lüdecke, D. (2019). Indices Effect Existence Significance Bayesian Framework. Frontiers Psychology 2019;10:2767. 10.3389/fpsyg.2019.02767","code":""},{"path":"https://easystats.github.io/bayestestR/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Understand and Describe Bayesian Models and Posterior Distributions","text":"bayestestR package available CRAN, latest development version available R-universe (rOpenSci). downloaded package, can load using: Tip Instead library(datawizard), use library(easystats). make features easystats-ecosystem available. stay updated, use easystats::install_latest().","code":"library(\"bayestestR\")"},{"path":"https://easystats.github.io/bayestestR/index.html","id":"documentation","dir":"","previous_headings":"","what":"Documentation","title":"Understand and Describe Bayesian Models and Posterior Distributions","text":"Access package documentation check-vignettes:","code":""},{"path":"https://easystats.github.io/bayestestR/index.html","id":"tutorials","dir":"","previous_headings":"Documentation","what":"Tutorials","title":"Understand and Describe Bayesian Models and Posterior Distributions","text":"Get Started Bayesian Analysis Example 1: Initiation Bayesian models Example 2: Confirmation Bayesian skills Example 3: Become Bayesian master","code":""},{"path":"https://easystats.github.io/bayestestR/index.html","id":"articles","dir":"","previous_headings":"Documentation","what":"Articles","title":"Understand and Describe Bayesian Models and Posterior Distributions","text":"Credible Intervals (CI) Probability Direction (pd) Region Practical Equivalence (ROPE) Bayes Factors (BF) Comparison Point-Estimates Comparison Indices Effect Existence Reporting Guidelines","code":""},{"path":"https://easystats.github.io/bayestestR/index.html","id":"features","dir":"","previous_headings":"","what":"Features","title":"Understand and Describe Bayesian Models and Posterior Distributions","text":"Bayesian framework, parameters estimated probabilistic fashion distributions. distributions can summarised described reporting four types indices: mean(), median() map_estimate() estimation mode. point_estimate() can used get can run directly models. hdi() Highest Density Intervals (HDI), spi() Shortest Probability Intervals (SPI) eti() Equal-Tailed Intervals (ETI). ci() can used general method Confidence Credible Intervals (CI). p_direction() Bayesian equivalent frequentist p-value (see Makowski et al., 2019) p_pointnull() represents odds null hypothesis (h0 = 0) compared likely hypothesis (MAP). bf_pointnull() classic Bayes Factor (BF) assessing likelihood effect presence absence (h0 = 0). p_rope() probability effect falling inside Region Practical Equivalence (ROPE). bf_rope() computes Bayes factor null defined region (ROPE). p_significance() combines region equivalence probability direction. describe_posterior() master function can compute indices cited . describe_posterior() works many objects, including complex brmsfit-models. better readability, output separated model components: bayestestR also includes many features useful Bayesian analsyes. examples:","code":"describe_posterior(   rnorm(10000),   centrality = \"median\",   test = c(\"p_direction\", \"p_significance\") ) ## Summary of Posterior Distribution ##  ## Parameter |   Median |        95% CI |     pd |   ps ## ---------------------------------------------------- ## Posterior | 5.93e-03 | [-1.93, 1.96] | 50.29% | 0.46 zinb <- read.csv(\"http://stats.idre.ucla.edu/stat/data/fish.csv\") set.seed(123) model <- brm(   bf(     count ~ child + camper + (1 | persons),     zi ~ child + camper + (1 | persons)   ),   data = zinb,   family = zero_inflated_poisson(),   chains = 1,   iter = 500 )  describe_posterior(   model,   effects = \"all\",   component = \"all\",   test = c(\"p_direction\", \"p_significance\"),   centrality = \"all\" ) ## Summary of Posterior Distribution ##  ## Parameter   | Median |  Mean |   MAP |         95% CI |     pd |   ps |  Rhat |    ESS ## -------------------------------------------------------------------------------------- ## (Intercept) |   0.96 |  0.96 |  0.96 | [-0.81,  2.51] | 90.00% | 0.88 | 1.011 | 110.00 ## child       |  -1.16 | -1.16 | -1.16 | [-1.36, -0.94] |   100% | 1.00 | 0.996 | 278.00 ## camper      |   0.73 |  0.72 |  0.73 | [ 0.54,  0.91] |   100% | 1.00 | 0.996 | 271.00 ##  ## # Fixed effects (zero-inflated) ##  ## Parameter   | Median |  Mean |   MAP |         95% CI |     pd |   ps |  Rhat |    ESS ## -------------------------------------------------------------------------------------- ## (Intercept) |  -0.48 | -0.51 | -0.22 | [-2.03,  0.89] | 78.00% | 0.73 | 0.997 | 138.00 ## child       |   1.85 |  1.86 |  1.81 | [ 1.19,  2.54] |   100% | 1.00 | 0.996 | 303.00 ## camper      |  -0.88 | -0.86 | -0.99 | [-1.61, -0.07] | 98.40% | 0.96 | 0.996 | 292.00 ##  ## # Random effects (conditional) Intercept: persons ##  ## Parameter |    Median |  Mean |   MAP |         95% CI |     pd |   ps |  Rhat |    ESS ## --------------------------------------------------------------------------------------- ## persons.1 |     -0.99 | -1.01 | -0.84 | [-2.68,  0.80] | 92.00% | 0.90 | 1.007 | 106.00 ## persons.2 | -4.65e-03 | -0.04 |  0.03 | [-1.63,  1.66] | 50.00% | 0.45 | 1.013 | 109.00 ## persons.3 |      0.69 |  0.66 |  0.69 | [-0.95,  2.34] | 79.60% | 0.78 | 1.010 | 114.00 ## persons.4 |      1.57 |  1.56 |  1.56 | [-0.05,  3.29] | 96.80% | 0.96 | 1.009 | 114.00 ##  ## # Random effects (zero-inflated) Intercept: persons ##  ## Parameter | Median |  Mean |   MAP |         95% CI |     pd |   ps |  Rhat |    ESS ## ------------------------------------------------------------------------------------ ## persons.1 |   1.10 |  1.11 |  1.08 | [-0.23,  2.72] | 94.80% | 0.93 | 0.997 | 166.00 ## persons.2 |   0.18 |  0.18 |  0.22 | [-0.94,  1.58] | 63.20% | 0.54 | 0.996 | 154.00 ## persons.3 |  -0.30 | -0.31 | -0.54 | [-1.79,  1.02] | 64.00% | 0.59 | 0.997 | 154.00 ## persons.4 |  -1.45 | -1.46 | -1.44 | [-2.90, -0.10] | 98.00% | 0.97 | 1.000 | 189.00 ##  ## # Random effects (conditional) SD/Cor: persons ##  ## Parameter   | Median | Mean |  MAP |         95% CI |   pd |   ps |  Rhat |    ESS ## ---------------------------------------------------------------------------------- ## (Intercept) |   1.42 | 1.58 | 1.07 | [ 0.71,  3.58] | 100% | 1.00 | 1.010 | 126.00 ##  ## # Random effects (zero-inflated) SD/Cor: persons ##  ## Parameter   | Median | Mean |  MAP |         95% CI |   pd |   ps |  Rhat |    ESS ## ---------------------------------------------------------------------------------- ## (Intercept) |   1.30 | 1.49 | 0.99 | [ 0.63,  3.41] | 100% | 1.00 | 0.996 | 129.00"},{"path":"https://easystats.github.io/bayestestR/index.html","id":"point-estimates","dir":"","previous_headings":"","what":"Point-estimates","title":"Understand and Describe Bayesian Models and Posterior Distributions","text":"easystats packages, plot() methods available see package many functions:  median mean available base R functions, map_estimate() bayestestR can used directly find Highest Maximum Posteriori (MAP) estimate posterior, .e., value associated highest probability density (“peak” posterior distribution). words, estimation mode continuous parameters.","code":"library(bayestestR)  posterior <- distribution_gamma(10000, 1.5) # Generate a skewed distribution centrality <- point_estimate(posterior) # Get indices of centrality centrality ## Point Estimate ##  ## Median | Mean |  MAP ## -------------------- ## 1.18   | 1.50 | 0.51"},{"path":"https://easystats.github.io/bayestestR/index.html","id":"uncertainty-ci","dir":"","previous_headings":"","what":"Uncertainty (CI)","title":"Understand and Describe Bayesian Models and Posterior Distributions","text":"hdi() computes Highest Density Interval (HDI) posterior distribution, .e., interval contains points within interval higher probability density points outside interval. HDI can used context Bayesian posterior characterization Credible Interval (CI). Unlike equal-tailed intervals (see eti()) typically exclude 2.5% tail distribution, HDI equal-tailed therefore always includes mode(s) posterior distributions.","code":"posterior <- distribution_chisquared(10000, 4)  hdi(posterior, ci = 0.89) ## 89% HDI: [0.18, 7.63]  eti(posterior, ci = 0.89) ## 89% ETI: [0.75, 9.25]"},{"path":[]},{"path":"https://easystats.github.io/bayestestR/index.html","id":"probability-of-direction-pd","dir":"","previous_headings":"Existence and Significance Testing","what":"Probability of Direction (pd)","title":"Understand and Describe Bayesian Models and Posterior Distributions","text":"p_direction() computes Probability Direction (pd, also known Maximum Probability Effect - MPE). varies 50% 100% (.e., 0.5 1) can interpreted probability (expressed percentage) parameter (described posterior distribution) strictly positive negative (whichever probable). mathematically defined proportion posterior distribution median’s sign. Although differently expressed, index fairly similar (.e., strongly correlated) frequentist p-value. Relationship p-value: cases, seems pd corresponds frequentist one-sided p-value formula p-value = (1-pd/100) two-sided p-value (commonly reported) formula p-value = 2*(1-pd/100). Thus, pd 95%, 97.5% 99.5% 99.95% corresponds approximately two-sided p-value respectively .1, .05, .01 .001. See reporting guidelines.","code":"posterior <- distribution_normal(10000, 0.4, 0.2) p_direction(posterior) ## Probability of Direction: 0.98"},{"path":"https://easystats.github.io/bayestestR/index.html","id":"rope","dir":"","previous_headings":"Existence and Significance Testing","what":"ROPE","title":"Understand and Describe Bayesian Models and Posterior Distributions","text":"rope() computes proportion (percentage) HDI (default 89% HDI) posterior distribution lies within region practical equivalence. Statistically, probability posterior distribution different 0 make much sense (probability different single point infinite). Therefore, idea underlining ROPE let user define area around null value enclosing values equivalent null value practical purposes Kruschke (2018). Kruschke suggests null value set, default, -0.1 0.1 range standardized parameter (negligible effect size according Cohen, 1988). generalized: instance, linear models, ROPE set 0 +/- .1 * sd(y). ROPE range can automatically computed models using rope_range function. Kruschke suggests using proportion 95% (90%, considered stable) HDI falls within ROPE index “null-hypothesis” testing (understood Bayesian framework, see equivalence_test).","code":"posterior <- distribution_normal(10000, 0.4, 0.2) rope(posterior, range = c(-0.1, 0.1)) ## # Proportion of samples inside the ROPE [-0.10, 0.10]: ##  ## inside ROPE ## ----------- ## 4.40 %"},{"path":"https://easystats.github.io/bayestestR/index.html","id":"bayes-factor","dir":"","previous_headings":"Existence and Significance Testing","what":"Bayes Factor","title":"Understand and Describe Bayesian Models and Posterior Distributions","text":"bayesfactor_parameters() computes Bayes factors null (either point interval), bases prior posterior samples single parameter. Bayes factor indicates degree mass posterior distribution shifted away closer null value(s) (relative prior distribution), thus indicating null value become less likely given observed data. null interval, Bayes factor computed comparing prior posterior odds parameter falling within outside null; null point, Savage-Dickey density ratio computed, also approximation Bayes factor comparing marginal likelihoods model model tested parameter restricted point null (Wagenmakers, Lodewyckx, Kuriyal, & Grasman, 2010).  lollipops represent density point-null prior distribution (blue lollipop dotted distribution) posterior distribution (red lollipop yellow distribution). ratio two - Savage-Dickey ratio - indicates degree mass parameter distribution shifted away closer null. info, see Bayes factors vignette.","code":"prior <- distribution_normal(10000, mean = 0, sd = 1) posterior <- distribution_normal(10000, mean = 1, sd = 0.7)  bayesfactor_parameters(posterior, prior, direction = \"two-sided\", null = 0) ## Bayes Factor (Savage-Dickey density ratio) ##  ## BF   ## ---- ## 1.94 ##  ## * Evidence Against The Null: 0"},{"path":[]},{"path":"https://easystats.github.io/bayestestR/index.html","id":"find-ropes-appropriate-range","dir":"","previous_headings":"Utilities","what":"Find ROPE’s appropriate range","title":"Understand and Describe Bayesian Models and Posterior Distributions","text":"rope_range(): function attempts automatically finding suitable “default” values Region Practical Equivalence (ROPE). Kruschke (2018) suggests null value set, default, range -0.1 0.1 standardized parameter (negligible effect size according Cohen, 1988), can generalised linear models -0.1 * sd(y), 0.1 * sd(y). logistic models, parameters expressed log odds ratio can converted standardized difference formula sqrt(3)/pi, resulting range -0.05 0.05.","code":"rope_range(model)"},{"path":"https://easystats.github.io/bayestestR/index.html","id":"density-estimation","dir":"","previous_headings":"Utilities","what":"Density Estimation","title":"Understand and Describe Bayesian Models and Posterior Distributions","text":"estimate_density(): function wrapper different methods density estimation. default, uses base R density default uses different smoothing bandwidth (\"SJ\") legacy default implemented base R density function (\"nrd0\"). However, Deng & Wickham suggest method = \"KernSmooth\" fastest accurate.","code":""},{"path":"https://easystats.github.io/bayestestR/index.html","id":"perfect-distributions","dir":"","previous_headings":"Utilities","what":"Perfect Distributions","title":"Understand and Describe Bayesian Models and Posterior Distributions","text":"distribution(): Generate sample size n near-perfect distributions.","code":"distribution(n = 10) ##  [1] -1.55 -1.00 -0.66 -0.38 -0.12  0.12  0.38  0.66  1.00  1.55"},{"path":"https://easystats.github.io/bayestestR/index.html","id":"probability-of-a-value","dir":"","previous_headings":"Utilities","what":"Probability of a Value","title":"Understand and Describe Bayesian Models and Posterior Distributions","text":"density_at(): Compute density given point distribution.","code":"density_at(rnorm(1000, 1, 1), 1) ## [1] 0.37"},{"path":"https://easystats.github.io/bayestestR/index.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"Understand and Describe Bayesian Models and Posterior Distributions","text":"Please note bayestestR project released Contributor Code Conduct. contributing project, agree abide terms.","code":""},{"path":[]},{"path":"https://easystats.github.io/bayestestR/reference/area_under_curve.html","id":null,"dir":"Reference","previous_headings":"","what":"Area under the Curve (AUC) — area_under_curve","title":"Area under the Curve (AUC) — area_under_curve","text":"Based DescTools AUC function. can calculate area curve naive algorithm elaborated spline approach. curve must given vectors xy-coordinates. function can handle unsorted x values (sorting x) ties x values (ignoring duplicates).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/area_under_curve.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Area under the Curve (AUC) — area_under_curve","text":"","code":"area_under_curve(x, y, method = c(\"trapezoid\", \"step\", \"spline\"), ...)  auc(x, y, method = c(\"trapezoid\", \"step\", \"spline\"), ...)"},{"path":"https://easystats.github.io/bayestestR/reference/area_under_curve.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Area under the Curve (AUC) — area_under_curve","text":"x Vector x values. y Vector y values. method Method compute Area Curve (AUC). Can \"trapezoid\" (default), \"step\" \"spline\". \"trapezoid\", curve formed connecting points direct line (composite trapezoid rule). \"step\" chosen stepwise connection two points used. calculating area spline interpolation splinefun function used combination integrate. ... Arguments passed methods.","code":""},{"path":[]},{"path":"https://easystats.github.io/bayestestR/reference/area_under_curve.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Area under the Curve (AUC) — area_under_curve","text":"","code":"library(bayestestR) posterior <- distribution_normal(1000)  dens <- estimate_density(posterior) dens <- dens[dens$x > 0, ] x <- dens$x y <- dens$y  area_under_curve(x, y, method = \"trapezoid\") #> [1] 0.498307 area_under_curve(x, y, method = \"step\") #> [1] 0.499534 area_under_curve(x, y, method = \"spline\") #> [1] 0.498307"},{"path":"https://easystats.github.io/bayestestR/reference/as.data.frame.density.html","id":null,"dir":"Reference","previous_headings":"","what":"Coerce to a Data Frame — as.data.frame.density","title":"Coerce to a Data Frame — as.data.frame.density","text":"Coerce Data Frame","code":""},{"path":"https://easystats.github.io/bayestestR/reference/as.data.frame.density.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coerce to a Data Frame — as.data.frame.density","text":"","code":"# S3 method for density as.data.frame(x, ...)"},{"path":"https://easystats.github.io/bayestestR/reference/as.data.frame.density.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Coerce to a Data Frame — as.data.frame.density","text":"x R object. ... additional arguments passed methods.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/as.numeric.p_direction.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert to Numeric — as.numeric.map_estimate","title":"Convert to Numeric — as.numeric.map_estimate","text":"Convert Numeric","code":""},{"path":"https://easystats.github.io/bayestestR/reference/as.numeric.p_direction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert to Numeric — as.numeric.map_estimate","text":"","code":"# S3 method for map_estimate as.numeric(x, ...)  # S3 method for p_direction as.numeric(x, ...)  # S3 method for p_map as.numeric(x, ...)  # S3 method for p_significance as.numeric(x, ...)"},{"path":"https://easystats.github.io/bayestestR/reference/as.numeric.p_direction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert to Numeric — as.numeric.map_estimate","text":"x object coerced tested. ... arguments passed methods.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayes Factors (BF) — bayesfactor","title":"Bayes Factors (BF) — bayesfactor","text":"function compte Bayes factors (BFs) appropriate input. vectors single models, compute BFs single parameters(), hypothesis specified, BFs restricted models(). multiple models, return BF corresponding comparison models() model comparison passed, compute inclusion BF().  complete overview functions, read Bayes factor vignette.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayes Factors (BF) — bayesfactor","text":"","code":"bayesfactor(   ...,   prior = NULL,   direction = \"two-sided\",   null = 0,   hypothesis = NULL,   effects = c(\"fixed\", \"random\", \"all\"),   verbose = TRUE,   denominator = 1,   match_models = FALSE,   prior_odds = NULL )"},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayes Factors (BF) — bayesfactor","text":"... numeric vector, model object(s), output bayesfactor_models. prior object representing prior distribution (see 'Details'). direction Test type (see 'Details'). One 0, \"two-sided\" (default, two tailed), -1, \"left\" (left tailed) 1, \"right\" (right tailed). null Value null, either scalar (point-null) range (interval-null). hypothesis character vector specifying restrictions logical conditions (see examples ). effects results fixed effects, random effects returned? applies mixed models. May abbreviated. verbose Toggle warnings. denominator Either integer indicating models use denominator, model used denominator. Ignored BFBayesFactor. match_models See details. prior_odds Optional vector prior odds models. See BayesFactor::priorOdds<-.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bayes Factors (BF) — bayesfactor","text":"type Bayes factor, depending input. See bayesfactor_parameters(), bayesfactor_models() bayesfactor_inclusion()","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Bayes Factors (BF) — bayesfactor","text":"also plot()-method implemented see-package.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bayes Factors (BF) — bayesfactor","text":"","code":"library(bayestestR)  if (require(\"logspline\")) {   prior <- distribution_normal(1000, mean = 0, sd = 1)   posterior <- distribution_normal(1000, mean = .5, sd = .3)    bayesfactor(posterior, prior = prior, verbose = FALSE) } #> Loading required package: logspline #> Bayes Factor (Savage-Dickey density ratio) #>  #> BF   #> ---- #> 1.21 #>  #> * Evidence Against The Null: 0 # \\dontrun{ # rstanarm models # --------------- if (require(\"rstanarm\")) {   model <- stan_lmer(extra ~ group + (1 | ID), data = sleep)   bayesfactor(model, verbose = FALSE) } #> Loading required package: rstanarm #> Loading required package: Rcpp #> This is rstanarm version 2.21.3 #> - See https://mc-stan.org/rstanarm/articles/priors for changes to default priors! #> - Default priors may change, so it's safest to specify priors, even if equivalent to the defaults. #> - For execution on a local, multicore CPU with excess RAM we recommend calling #>   options(mc.cores = parallel::detectCores()) #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 4.8e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.48 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.290484 seconds (Warm-up) #> Chain 1:                0.289436 seconds (Sampling) #> Chain 1:                0.57992 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 2.1e-05 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.280454 seconds (Warm-up) #> Chain 2:                0.242229 seconds (Sampling) #> Chain 2:                0.522683 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 2e-05 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.26993 seconds (Warm-up) #> Chain 3:                0.19661 seconds (Sampling) #> Chain 3:                0.46654 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 2.1e-05 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.30059 seconds (Warm-up) #> Chain 4:                0.251298 seconds (Sampling) #> Chain 4:                0.551888 seconds (Total) #> Chain 4:  #> Bayes Factor (Savage-Dickey density ratio)  #>  #> Parameter   |    BF #> ------------------- #> (Intercept) | 0.186 #> group2      |  3.46 #>  #> * Evidence Against The Null: 0 # }  if (require(\"logspline\")) {   # Frequentist models   # ---------------   m0 <- lm(extra ~ 1, data = sleep)   m1 <- lm(extra ~ group, data = sleep)   m2 <- lm(extra ~ group + ID, data = sleep)    comparison <- bayesfactor(m0, m1, m2)   comparison    bayesfactor(comparison) } #> Inclusion Bayes Factors (Model Averaged) #>  #>       P(prior) P(posterior) Inclusion BF #> group     0.67         1.00     5.61e+03 #> ID        0.33         1.00     9.77e+03 #>  #> * Compared among: all models #> *    Priors odds: uniform-equal"},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_inclusion.html","id":null,"dir":"Reference","previous_headings":"","what":"Inclusion Bayes Factors for testing predictors across Bayesian models — bayesfactor_inclusion","title":"Inclusion Bayes Factors for testing predictors across Bayesian models — bayesfactor_inclusion","text":"bf_* function alias main function.  info, see Bayes factors vignette.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_inclusion.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Inclusion Bayes Factors for testing predictors across Bayesian models — bayesfactor_inclusion","text":"","code":"bayesfactor_inclusion(models, match_models = FALSE, prior_odds = NULL, ...)  bf_inclusion(models, match_models = FALSE, prior_odds = NULL, ...)"},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_inclusion.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Inclusion Bayes Factors for testing predictors across Bayesian models — bayesfactor_inclusion","text":"models object class bayesfactor_models() BFBayesFactor. match_models See details. prior_odds Optional vector prior odds models. See BayesFactor::priorOdds<-. ... Arguments passed methods.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_inclusion.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Inclusion Bayes Factors for testing predictors across Bayesian models — bayesfactor_inclusion","text":"data frame containing prior posterior probabilities, log(BF) effect (Use .numeric() extract non-log Bayes factors; see examples).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_inclusion.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Inclusion Bayes Factors for testing predictors across Bayesian models — bayesfactor_inclusion","text":"Inclusion Bayes factors answer question: observed data probable models particular effect, models without particular effect? words, average - models effect \\(X\\) likely produced observed data models without effect \\(X\\)?","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_inclusion.html","id":"match-models","dir":"Reference","previous_headings":"","what":"Match Models","title":"Inclusion Bayes Factors for testing predictors across Bayesian models — bayesfactor_inclusion","text":"match_models=FALSE (default), Inclusion BFs computed comparing models term models without term. TRUE, comparison restricted models (1) include interactions term interest; (2) interaction terms, averaging done across models containe main effect terms interaction term comprised.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_inclusion.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Inclusion Bayes Factors for testing predictors across Bayesian models — bayesfactor_inclusion","text":"Random effects lmer style converted interaction terms: .e., (X|G) become terms 1:G X:G.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_inclusion.html","id":"interpreting-bayes-factors","dir":"Reference","previous_headings":"","what":"Interpreting Bayes Factors","title":"Inclusion Bayes Factors for testing predictors across Bayesian models — bayesfactor_inclusion","text":"Bayes factor greater 1 can interpreted evidence null, one convention Bayes factor greater 3 can considered \"substantial\" evidence null (vice versa, Bayes factor smaller 1/3 indicates substantial evidence favor null-model) (Wetzels et al. 2011).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_inclusion.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Inclusion Bayes Factors for testing predictors across Bayesian models — bayesfactor_inclusion","text":"Hinne, M., Gronau, Q. F., van den Bergh, D., Wagenmakers, E. (2019, March 25). conceptual introduction Bayesian Model Averaging. doi:10.31234/osf.io/wgb64 Clyde, M. ., Ghosh, J., & Littman, M. L. (2011). Bayesian adaptive sampling variable selection model averaging. Journal Computational Graphical Statistics, 20(1), 80-101. Mathot, S. (2017). Bayes like Baws: Interpreting Bayesian Repeated Measures JASP Blog post.","code":""},{"path":[]},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_inclusion.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Inclusion Bayes Factors for testing predictors across Bayesian models — bayesfactor_inclusion","text":"Mattan S. Ben-Shachar","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_inclusion.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Inclusion Bayes Factors for testing predictors across Bayesian models — bayesfactor_inclusion","text":"","code":"library(bayestestR)  # Using bayesfactor_models: # ------------------------------ mo0 <- lm(Sepal.Length ~ 1, data = iris) mo1 <- lm(Sepal.Length ~ Species, data = iris) mo2 <- lm(Sepal.Length ~ Species + Petal.Length, data = iris) mo3 <- lm(Sepal.Length ~ Species * Petal.Length, data = iris)  BFmodels <- bayesfactor_models(mo1, mo2, mo3, denominator = mo0) (bf_inc <- bayesfactor_inclusion(BFmodels)) #> Inclusion Bayes Factors (Model Averaged) #>  #>                      P(prior) P(posterior) Inclusion BF #> Species                  0.75         1.00     2.02e+55 #> Petal.Length             0.50         1.00     3.58e+26 #> Petal.Length:Species     0.25         0.04        0.113 #>  #> * Compared among: all models #> *    Priors odds: uniform-equal  as.numeric(bf_inc) #> [1] 2.021143e+55 3.575448e+26 1.131202e-01  # \\dontrun{ # BayesFactor # ------------------------------- library(BayesFactor) #> Loading required package: coda #> Loading required package: Matrix #> ************ #> Welcome to BayesFactor 0.9.12-4.4. If you have questions, please contact Richard Morey (richarddmorey@gmail.com). #>  #> Type BFManual() to open the manual. #> ************  BF <- generalTestBF(len ~ supp * dose, ToothGrowth, progress = FALSE)  bayesfactor_inclusion(BF) #> Inclusion Bayes Factors (Model Averaged) #>  #>           P(prior) P(posterior) Inclusion BF #> supp          0.60         0.98        36.50 #> dose          0.60         1.00     5.98e+12 #> dose:supp     0.20         0.54         4.68 #>  #> * Compared among: all models #> *    Priors odds: uniform-equal  # compare only matched models: bayesfactor_inclusion(BF, match_models = TRUE) #> Inclusion Bayes Factors (Model Averaged) #>  #>           P(prior) P(posterior) Inclusion BF #> supp          0.40         0.44        24.68 #> dose          0.40         0.46     4.13e+12 #> dose:supp     0.20         0.54         1.22 #>  #> * Compared among: matched models only #> *    Priors odds: uniform-equal # }"},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_models.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayes Factors (BF) for model comparison — bayesfactor_models","title":"Bayes Factors (BF) for model comparison — bayesfactor_models","text":"function computes extracts Bayes factors fitted models.  bf_* function alias main function.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayes Factors (BF) for model comparison — bayesfactor_models","text":"","code":"bayesfactor_models(..., denominator = 1, verbose = TRUE)  bf_models(..., denominator = 1, verbose = TRUE)  # S3 method for default bayesfactor_models(..., denominator = 1, verbose = TRUE)  # S3 method for bayesfactor_models update(object, subset = NULL, reference = NULL, ...)  # S3 method for bayesfactor_models as.matrix(x, ...)"},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayes Factors (BF) for model comparison — bayesfactor_models","text":"... Fitted models (see details), fit data, single BFBayesFactor object (see 'Details'). Ignored .matrix(), update(). following named arguments present, passed insight::get_loglikelihood (see details): estimator (defaults \"ML\") check_response  (defaults FALSE) denominator Either integer indicating models use denominator, model used denominator. Ignored BFBayesFactor. verbose Toggle warnings. object, x bayesfactor_models() object. subset Vector model indices keep remove. reference Index model reference , \"top\" reference best model, \"bottom\" reference worst model.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bayes Factors (BF) for model comparison — bayesfactor_models","text":"data frame containing models' formulas (reconstructed fixed random effects) log(BF)s  (Use .numeric() extract non-log Bayes factors; see examples), prints nicely.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_models.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bayes Factors (BF) for model comparison — bayesfactor_models","text":"passed models supported insight DV models tested equality (else assumed true), models' terms extracted (allowing follow-analysis bayesfactor_inclusion). brmsfit stanreg models, Bayes factors computed using bridgesampling package. brmsfit models must fitted save_pars = save_pars(= TRUE). stanreg models must fitted defined diagnostic_file. BFBayesFactor, bayesfactor_models() mostly wraparound BayesFactor::extractBF(). model types, Bayes factors computed using BIC approximation. Note BICs extracted using insight::get_loglikelihood, see documentation options dealing transformed responses REML estimation. order correctly precisely estimate Bayes factors, rule thumb 4 P's: Proper Priors Plentiful Posteriors. many? number posterior samples needed testing substantially larger estimation (default 4000 samples may enough many cases). conservative rule thumb obtain 10 times samples required estimation (Gronau, Singmann, & Wagenmakers, 2017). less 40,000 samples detected, bayesfactor_models() gives warning.  See also Bayes factors vignette.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_models.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Bayes Factors (BF) for model comparison — bayesfactor_models","text":"also plot()-method implemented see-package.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_models.html","id":"interpreting-bayes-factors","dir":"Reference","previous_headings":"","what":"Interpreting Bayes Factors","title":"Bayes Factors (BF) for model comparison — bayesfactor_models","text":"Bayes factor greater 1 can interpreted evidence null, one convention Bayes factor greater 3 can considered \"substantial\" evidence null (vice versa, Bayes factor smaller 1/3 indicates substantial evidence favor null-model) (Wetzels et al. 2011).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_models.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Bayes Factors (BF) for model comparison — bayesfactor_models","text":"Gronau, Q. F., Singmann, H., & Wagenmakers, E. J. (2017). Bridgesampling: R package estimating normalizing constants. arXiv preprint arXiv:1710.08162. Kass, R. E., Raftery, . E. (1995). Bayes Factors. Journal American Statistical Association, 90(430), 773-795. Robert, C. P. (2016). expected demise Bayes factor. Journal Mathematical Psychology, 72, 33–37. Wagenmakers, E. J. (2007). practical solution pervasive problems p values. Psychonomic bulletin & review, 14(5), 779-804. Wetzels, R., Matzke, D., Lee, M. D., Rouder, J. N., Iverson, G. J., Wagenmakers, E.-J. (2011). Statistical Evidence Experimental Psychology: Empirical Comparison Using 855 t Tests. Perspectives Psychological Science, 6(3), 291–298. doi:10.1177/1745691611406923","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_models.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Bayes Factors (BF) for model comparison — bayesfactor_models","text":"Mattan S. Ben-Shachar","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_models.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bayes Factors (BF) for model comparison — bayesfactor_models","text":"","code":"# With lm objects: # ---------------- lm1 <- lm(mpg ~ 1, data = mtcars) lm2 <- lm(mpg ~ hp, data = mtcars) lm3 <- lm(mpg ~ hp + drat, data = mtcars) lm4 <- lm(mpg ~ hp * drat, data = mtcars) (BFM <- bayesfactor_models(lm1, lm2, lm3, lm4, denominator = 1)) #> Bayes Factors for Model Comparison #>  #>       Model           BF #> [lm2] hp        4.54e+05 #> [lm3] hp + drat 7.70e+07 #> [lm4] hp * drat 1.59e+07 #>  #> * Against Denominator: [lm1] (Intercept only) #> *   Bayes Factor Type: BIC approximation # bayesfactor_models(lm2, lm3, lm4, denominator = lm1) # same result # bayesfactor_models(lm1, lm2, lm3, lm4, denominator = lm1) # same result   update(BFM, reference = \"bottom\") #> Bayes Factors for Model Comparison #>  #>       Model           BF #> [lm2] hp        4.54e+05 #> [lm3] hp + drat 7.70e+07 #> [lm4] hp * drat 1.59e+07 #>  #> * Against Denominator: [lm1] (Intercept only) #> *   Bayes Factor Type: BIC approximation as.matrix(BFM) #> # Bayes Factors for Model Comparison  #>  #>            Numerator #> Denominator #>  #>           |      [1] |      [2] |      [3] |      [4] #> ---------------------------------------------------------------- #> [1] (Intercept only) |        1 | 4.54e+05 | 7.70e+07 | 1.59e+07 #> [2] hp               | 2.20e-06 |        1 |   169.72 |    35.09 #> [3] hp + drat        | 1.30e-08 |    0.006 |        1 |    0.207 #> [4] hp * drat        | 6.28e-08 |    0.028 |     4.84 |        1 as.numeric(BFM) #> [1]        1.0   453874.3 77029881.3 15925712.4   lm2b <- lm(sqrt(mpg) ~ hp, data = mtcars) # Set check_response = TRUE for transformed responses bayesfactor_models(lm2b, denominator = lm2, check_response = TRUE) #> Bayes Factors for Model Comparison #>  #>        Model   BF #> [lm2b] hp    6.94 #>  #> * Against Denominator: [lm2] hp #> *   Bayes Factor Type: BIC approximation  # \\dontrun{ # With lmerMod objects: # --------------------- if (require(\"lme4\")) {   lmer1 <- lmer(Sepal.Length ~ Petal.Length + (1 | Species), data = iris)   lmer2 <- lmer(Sepal.Length ~ Petal.Length + (Petal.Length | Species), data = iris)   lmer3 <- lmer(Sepal.Length ~ Petal.Length + (Petal.Length | Species) + (1 | Petal.Width),     data = iris   )   bayesfactor_models(lmer1, lmer2, lmer3,     denominator = 1,     estimator = \"REML\"   ) } #> Loading required package: lme4 #> boundary (singular) fit: see help('isSingular') #> boundary (singular) fit: see help('isSingular') #> Bayes Factors for Model Comparison #>  #>         Model                                                          BF #> [lmer2] Petal.Length + (Petal.Length | Species)                     0.058 #> [lmer3] Petal.Length + (Petal.Length | Species) + (1 | Petal.Width) 0.005 #>  #> * Against Denominator: [lmer1] Petal.Length + (1 | Species) #> *   Bayes Factor Type: BIC approximation  # rstanarm models # --------------------- # (note that a unique diagnostic_file MUST be specified in order to work) if (require(\"rstanarm\")) {   stan_m0 <- stan_glm(Sepal.Length ~ 1,     data = iris,     family = gaussian(),     diagnostic_file = file.path(tempdir(), \"df0.csv\")   )   stan_m1 <- stan_glm(Sepal.Length ~ Species,     data = iris,     family = gaussian(),     diagnostic_file = file.path(tempdir(), \"df1.csv\")   )   stan_m2 <- stan_glm(Sepal.Length ~ Species + Petal.Length,     data = iris,     family = gaussian(),     diagnostic_file = file.path(tempdir(), \"df2.csv\")   )   bayesfactor_models(stan_m1, stan_m2, denominator = stan_m0, verbose = FALSE) } #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 2.6e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.26 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.024594 seconds (Warm-up) #> Chain 1:                0.062693 seconds (Sampling) #> Chain 1:                0.087287 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 1.3e-05 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.02727 seconds (Warm-up) #> Chain 2:                0.066368 seconds (Sampling) #> Chain 2:                0.093638 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 1.3e-05 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.029296 seconds (Warm-up) #> Chain 3:                0.061406 seconds (Sampling) #> Chain 3:                0.090702 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 1.2e-05 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.024849 seconds (Warm-up) #> Chain 4:                0.062389 seconds (Sampling) #> Chain 4:                0.087238 seconds (Total) #> Chain 4:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 2.3e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.039558 seconds (Warm-up) #> Chain 1:                0.083006 seconds (Sampling) #> Chain 1:                0.122564 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 1.3e-05 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.040432 seconds (Warm-up) #> Chain 2:                0.074738 seconds (Sampling) #> Chain 2:                0.11517 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 1.3e-05 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.048415 seconds (Warm-up) #> Chain 3:                0.080515 seconds (Sampling) #> Chain 3:                0.12893 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 1.4e-05 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.042385 seconds (Warm-up) #> Chain 4:                0.07257 seconds (Sampling) #> Chain 4:                0.114955 seconds (Total) #> Chain 4:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 2.3e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.121281 seconds (Warm-up) #> Chain 1:                0.172307 seconds (Sampling) #> Chain 1:                0.293588 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 1.3e-05 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.120651 seconds (Warm-up) #> Chain 2:                0.169512 seconds (Sampling) #> Chain 2:                0.290163 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 1.3e-05 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.127164 seconds (Warm-up) #> Chain 3:                0.17724 seconds (Sampling) #> Chain 3:                0.304404 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 1.4e-05 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.120213 seconds (Warm-up) #> Chain 4:                0.15605 seconds (Sampling) #> Chain 4:                0.276263 seconds (Total) #> Chain 4:  #> Bayes Factors for Model Comparison #>  #>     Model                        BF #> [1] Species                6.27e+27 #> [2] Species + Petal.Length 2.26e+53 #>  #> * Against Denominator: [3] (Intercept only) #> *   Bayes Factor Type: marginal likelihoods (bridgesampling)   # brms models # -------------------- # (note the save_pars MUST be set to save_pars(all = TRUE) in order to work) if (require(\"brms\")) {   brm1 <- brm(Sepal.Length ~ 1, data = iris, save_all_pars = TRUE)   brm2 <- brm(Sepal.Length ~ Species, data = iris, save_all_pars = TRUE)   brm3 <- brm(     Sepal.Length ~ Species + Petal.Length,     data = iris,     save_pars = save_pars(all = TRUE)   )    bayesfactor_models(brm1, brm2, brm3, denominator = 1) } #> Loading required package: brms #> Loading 'brms' package (version 2.18.0). Useful instructions #> can be found by typing help('brms'). A more detailed introduction #> to the package is available through vignette('brms_overview'). #>  #> Attaching package: ‘brms’ #> The following object is masked from ‘package:lme4’: #>  #>     ngrps #> The following objects are masked from ‘package:rstanarm’: #>  #>     dirichlet, exponential, get_y, lasso, ngrps #> The following object is masked from ‘package:stats’: #>  #>     ar #> Warning: Argument 'save_all_pars' is deprecated. Please use argument 'all' in function 'save_pars()' instead. #> Compiling Stan program... #> Start sampling #>  #> SAMPLING FOR MODEL 'b734c673164a6f43fed9f2aa2f7aa6c1' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 3.8e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.38 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.066297 seconds (Warm-up) #> Chain 1:                0.069237 seconds (Sampling) #> Chain 1:                0.135534 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL 'b734c673164a6f43fed9f2aa2f7aa6c1' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 1.8e-05 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.058863 seconds (Warm-up) #> Chain 2:                0.054527 seconds (Sampling) #> Chain 2:                0.11339 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL 'b734c673164a6f43fed9f2aa2f7aa6c1' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 1.8e-05 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.062258 seconds (Warm-up) #> Chain 3:                0.062058 seconds (Sampling) #> Chain 3:                0.124316 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL 'b734c673164a6f43fed9f2aa2f7aa6c1' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 1.8e-05 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.058801 seconds (Warm-up) #> Chain 4:                0.064214 seconds (Sampling) #> Chain 4:                0.123015 seconds (Total) #> Chain 4:  #> Warning: Argument 'save_all_pars' is deprecated. Please use argument 'all' in function 'save_pars()' instead. #> Compiling Stan program... #> Start sampling #>  #> SAMPLING FOR MODEL '5febdc357d0437dbd375707228b461a5' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 1.3e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.02831 seconds (Warm-up) #> Chain 1:                0.026033 seconds (Sampling) #> Chain 1:                0.054343 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL '5febdc357d0437dbd375707228b461a5' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 7e-06 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.028456 seconds (Warm-up) #> Chain 2:                0.029018 seconds (Sampling) #> Chain 2:                0.057474 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL '5febdc357d0437dbd375707228b461a5' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 1e-05 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.029502 seconds (Warm-up) #> Chain 3:                0.029181 seconds (Sampling) #> Chain 3:                0.058683 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL '5febdc357d0437dbd375707228b461a5' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 8e-06 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.031437 seconds (Warm-up) #> Chain 4:                0.034798 seconds (Sampling) #> Chain 4:                0.066235 seconds (Total) #> Chain 4:  #> Compiling Stan program... #> recompiling to avoid crashing R session #> Start sampling #>  #> SAMPLING FOR MODEL '5febdc357d0437dbd375707228b461a5' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 1.6e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.084183 seconds (Warm-up) #> Chain 1:                0.093514 seconds (Sampling) #> Chain 1:                0.177697 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL '5febdc357d0437dbd375707228b461a5' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 9e-06 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.091131 seconds (Warm-up) #> Chain 2:                0.094111 seconds (Sampling) #> Chain 2:                0.185242 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL '5febdc357d0437dbd375707228b461a5' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 8e-06 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.08757 seconds (Warm-up) #> Chain 3:                0.084679 seconds (Sampling) #> Chain 3:                0.172249 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL '5febdc357d0437dbd375707228b461a5' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 8e-06 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.083859 seconds (Warm-up) #> Chain 4:                0.090761 seconds (Sampling) #> Chain 4:                0.17462 seconds (Total) #> Chain 4:  #> Warning: Bayes factors might not be precise. #>   For precise Bayes factors, sampling at least 40,000 posterior samples is #>   recommended. #> Computation of Marginal Likelihood: estimating marginal likelihood, please wait... #> Computation of Marginal Likelihood: estimating marginal likelihood, please wait... #> Computation of Marginal Likelihood: estimating marginal likelihood, please wait... #> Bayes Factors for Model Comparison #>  #>     Model                        BF #> [2] Species                5.90e+29 #> [3] Species + Petal.Length 7.60e+55 #>  #> * Against Denominator: [1] (Intercept only) #> *   Bayes Factor Type: marginal likelihoods (bridgesampling)   # BayesFactor # --------------------------- if (require(\"BayesFactor\")) {   data(puzzles)   BF <- anovaBF(RT ~ shape * color + ID,     data = puzzles,     whichRandom = \"ID\", progress = FALSE   )   BF   bayesfactor_models(BF) # basically the same } #> Bayes Factors for Model Comparison #>  #>     Model                               BF #> [2] shape + ID                        2.82 #> [3] color + ID                        2.83 #> [4] shape + color + ID               11.62 #> [5] shape + color + shape:color + ID  4.27 #>  #> * Against Denominator: [1] ID #> *   Bayes Factor Type: JZS (BayesFactor) # }"},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_parameters.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayes Factors (BF) for a Single Parameter — bayesfactor_parameters","title":"Bayes Factors (BF) for a Single Parameter — bayesfactor_parameters","text":"method computes Bayes factors null (either point interval), based prior posterior samples single parameter. Bayes factor indicates degree mass posterior distribution shifted away closer null value(s) (relative prior distribution), thus indicating null value become less likely given observed data.  null interval, Bayes factor computed comparing prior posterior odds parameter falling within outside null interval (Morey & Rouder, 2011; Liao et al., 2020); null point, Savage-Dickey density ratio computed, also approximation Bayes factor comparing marginal likelihoods model model tested parameter restricted point null (Wagenmakers et al., 2010; Heck, 2019).  Note logspline package used estimating densities probabilities, must installed function work. bayesfactor_pointnull() bayesfactor_rope() wrappers around bayesfactor_parameters different defaults null tested (point range, respectively). Aliases main functions prefixed bf_*, like bf_parameters() bf_pointnull(). info, particular specifying correct priors factors 2 levels, see Bayes factors vignette.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_parameters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayes Factors (BF) for a Single Parameter — bayesfactor_parameters","text":"","code":"bayesfactor_parameters(   posterior,   prior = NULL,   direction = \"two-sided\",   null = 0,   verbose = TRUE,   ... )  bayesfactor_pointnull(   posterior,   prior = NULL,   direction = \"two-sided\",   null = 0,   verbose = TRUE,   ... )  bayesfactor_rope(   posterior,   prior = NULL,   direction = \"two-sided\",   null = rope_range(posterior),   verbose = TRUE,   ... )  bf_parameters(   posterior,   prior = NULL,   direction = \"two-sided\",   null = 0,   verbose = TRUE,   ... )  bf_pointnull(   posterior,   prior = NULL,   direction = \"two-sided\",   null = 0,   verbose = TRUE,   ... )  bf_rope(   posterior,   prior = NULL,   direction = \"two-sided\",   null = rope_range(posterior),   verbose = TRUE,   ... )  # S3 method for numeric bayesfactor_parameters(   posterior,   prior = NULL,   direction = \"two-sided\",   null = 0,   verbose = TRUE,   ... )  # S3 method for stanreg bayesfactor_parameters(   posterior,   prior = NULL,   direction = \"two-sided\",   null = 0,   verbose = TRUE,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"conditional\", \"location\", \"smooth_terms\", \"sigma\", \"zi\",     \"zero_inflated\", \"all\"),   parameters = NULL,   ... )  # S3 method for brmsfit bayesfactor_parameters(   posterior,   prior = NULL,   direction = \"two-sided\",   null = 0,   verbose = TRUE,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"conditional\", \"location\", \"smooth_terms\", \"sigma\", \"zi\",     \"zero_inflated\", \"all\"),   parameters = NULL,   ... )  # S3 method for blavaan bayesfactor_parameters(   posterior,   prior = NULL,   direction = \"two-sided\",   null = 0,   verbose = TRUE,   ... )  # S3 method for data.frame bayesfactor_parameters(   posterior,   prior = NULL,   direction = \"two-sided\",   null = 0,   verbose = TRUE,   ... )"},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_parameters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayes Factors (BF) for a Single Parameter — bayesfactor_parameters","text":"posterior numerical vector, stanreg / brmsfit object, emmGrid data frame - representing posterior distribution(s) (see 'Details'). prior object representing prior distribution (see 'Details'). direction Test type (see 'Details'). One 0, \"two-sided\" (default, two tailed), -1, \"left\" (left tailed) 1, \"right\" (right tailed). null Value null, either scalar (point-null) range (interval-null). verbose Toggle warnings. ... Arguments passed methods. (Can used pass arguments internal logspline::logspline().) effects results fixed effects, random effects returned? applies mixed models. May abbreviated. component results parameters, parameters conditional model zero-inflated part model returned? May abbreviated. applies brms-models. parameters Regular expression pattern describes parameters returned. Meta-parameters (like lp__ prior_) filtered default, parameters typically appear summary() returned. Use parameters select specific parameters output.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_parameters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bayes Factors (BF) for a Single Parameter — bayesfactor_parameters","text":"data frame containing (log) Bayes factor representing evidence null  (Use .numeric() extract non-log Bayes factors; see examples).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_parameters.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bayes Factors (BF) for a Single Parameter — bayesfactor_parameters","text":"method used compute Bayes factors based prior posterior distributions.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_parameters.html","id":"one-sided-amp-dividing-tests-setting-an-order-restriction-","dir":"Reference","previous_headings":"","what":"One-sided & Dividing Tests (setting an order restriction)","title":"Bayes Factors (BF) for a Single Parameter — bayesfactor_parameters","text":"One sided tests (controlled direction) conducted restricting prior posterior non-null values (\"alternative\") one side null (Morey & Wagenmakers, 2014). example, prior hypothesis parameter positive, alternative restricted region right null (point interval). example, Bayes factor comparing \"null\" 0-0.1 alternative >0.1, set bayesfactor_parameters(null = c(0, 0.1), direction = \">\").  also possible compute Bayes factor dividing hypotheses - , null alternative complementary, opposing one-sided hypotheses (Morey & Wagenmakers, 2014). example, Bayes factor comparing \"null\" <0 alternative >0, set bayesfactor_parameters(null = c(-Inf, 0)).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_parameters.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Bayes Factors (BF) for a Single Parameter — bayesfactor_parameters","text":"also plot()-method implemented see-package.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_parameters.html","id":"setting-the-correct-prior","dir":"Reference","previous_headings":"","what":"Setting the correct prior","title":"Bayes Factors (BF) for a Single Parameter — bayesfactor_parameters","text":"computation Bayes factors, model priors must proper priors (least flat, preferable informative); priors alternative get wider, likelihood null value(s) increases, extreme completely flat priors null infinitely favorable alternative (called Jeffreys-Lindley-Bartlett paradox). Thus, ever try (want) compute Bayes factor informed prior.  (Note default, brms::brm() uses flat priors fixed-effects; See example .)  important provide correct prior meaningful results. posterior numerical vector, prior also numerical vector. posterior data.frame, prior also data.frame, matching column order. posterior stanreg, brmsfit supported Bayesian model: prior can set NULL, case prior samples drawn internally. prior can also model equivalent posterior samples priors . See unupdate(). Note: posterior brmsfit_multiple model, prior must provided. posterior emmGrid / emm_list object: prior also emmGrid / emm_list object equivalent posterior created model priors samples . See unupdate(). prior can also original (posterior) model. , function try update emmGrid / emm_list use unupdate()d prior-model. (done brmsfit models.) Note: emmGrid undergone transformations (\"log\", \"response\", etc.), regriding, prior must emmGrid object, stated .","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_parameters.html","id":"interpreting-bayes-factors","dir":"Reference","previous_headings":"","what":"Interpreting Bayes Factors","title":"Bayes Factors (BF) for a Single Parameter — bayesfactor_parameters","text":"Bayes factor greater 1 can interpreted evidence null, one convention Bayes factor greater 3 can considered \"substantial\" evidence null (vice versa, Bayes factor smaller 1/3 indicates substantial evidence favor null-model) (Wetzels et al. 2011).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_parameters.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Bayes Factors (BF) for a Single Parameter — bayesfactor_parameters","text":"Wagenmakers, E. J., Lodewyckx, T., Kuriyal, H., Grasman, R. (2010). Bayesian hypothesis testing psychologists: tutorial Savage-Dickey method. Cognitive psychology, 60(3), 158-189. Heck, D. W. (2019). caveat Savage–Dickey density ratio: case computing Bayes factors regression parameters. British Journal Mathematical Statistical Psychology, 72(2), 316-333. Morey, R. D., & Wagenmakers, E. J. (2014). Simple relation Bayesian order-restricted point-null hypothesis tests. Statistics & Probability Letters, 92, 121-124. Morey, R. D., & Rouder, J. N. (2011). Bayes factor approaches testing interval null hypotheses. Psychological methods, 16(4), 406. Liao, J. G., Midya, V., & Berg, . (2020). Connecting contrasting Bayes factor modified ROPE procedure testing interval null hypotheses. American Statistician, 1-19. Wetzels, R., Matzke, D., Lee, M. D., Rouder, J. N., Iverson, G. J., Wagenmakers, E.-J. (2011). Statistical Evidence Experimental Psychology: Empirical Comparison Using 855 t Tests. Perspectives Psychological Science, 6(3), 291–298. doi:10.1177/1745691611406923","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_parameters.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Bayes Factors (BF) for a Single Parameter — bayesfactor_parameters","text":"Mattan S. Ben-Shachar","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_parameters.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bayes Factors (BF) for a Single Parameter — bayesfactor_parameters","text":"","code":"library(bayestestR) if (require(\"logspline\")) {   prior <- distribution_normal(1000, mean = 0, sd = 1)   posterior <- distribution_normal(1000, mean = .5, sd = .3)   (BF_pars <- bayesfactor_parameters(posterior, prior, verbose = FALSE))    as.numeric(BF_pars) } #> [1] 1.212843 # \\dontrun{ # rstanarm models # --------------- if (require(\"rstanarm\") && require(\"emmeans\") && require(\"logspline\")) {   contrasts(sleep$group) <- contr.equalprior_pairs # see vingette   stan_model <- stan_lmer(extra ~ group + (1 | ID), data = sleep)   bayesfactor_parameters(stan_model)   bayesfactor_parameters(stan_model, null = rope_range(stan_model))    # emmGrid objects   # ---------------   group_diff <- pairs(emmeans(stan_model, ~group))   bayesfactor_parameters(group_diff, prior = stan_model)    # Or   group_diff_prior <- pairs(emmeans(unupdate(stan_model), ~group))   bayesfactor_parameters(group_diff, prior = group_diff_prior) } #> Loading required package: emmeans #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 3.6e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.36 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.266569 seconds (Warm-up) #> Chain 1:                0.209679 seconds (Sampling) #> Chain 1:                0.476248 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 2.1e-05 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.288156 seconds (Warm-up) #> Chain 2:                0.230141 seconds (Sampling) #> Chain 2:                0.518297 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 1.9e-05 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.265227 seconds (Warm-up) #> Chain 3:                0.225118 seconds (Sampling) #> Chain 3:                0.490345 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 2e-05 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.279461 seconds (Warm-up) #> Chain 4:                0.314048 seconds (Sampling) #> Chain 4:                0.593509 seconds (Total) #> Chain 4:  #> Sampling priors, please wait... #> Warning: Bayes factors might not be precise. #>   For precise Bayes factors, sampling at least 40,000 posterior samples is #>   recommended. #> Sampling priors, please wait... #> Sampling priors, please wait... #> Warning: Bayes factors might not be precise. #>   For precise Bayes factors, sampling at least 40,000 posterior samples is #>   recommended. #> Sampling priors, please wait... #> Warning: Bayes factors might not be precise. #>   For precise Bayes factors, sampling at least 40,000 posterior samples is #>   recommended. #> Bayes Factor (Savage-Dickey density ratio) #>  #> Parameter       |   BF #> ---------------------- #> group1 - group2 | 3.05 #>  #> * Evidence Against The Null: 0  # brms models # ----------- if (require(\"brms\")) {   contrasts(sleep$group) <- contr.equalprior_pairs # see vingette   my_custom_priors <-     set_prior(\"student_t(3, 0, 1)\", class = \"b\") +     set_prior(\"student_t(3, 0, 1)\", class = \"sd\", group = \"ID\")    brms_model <- brm(extra ~ group + (1 | ID),     data = sleep,     prior = my_custom_priors   )   bayesfactor_parameters(brms_model) } #> Compiling Stan program... #> Start sampling #>  #> SAMPLING FOR MODEL 'adf329cdde529e84ce328f199529dd87' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 3.7e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.37 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.122208 seconds (Warm-up) #> Chain 1:                0.153848 seconds (Sampling) #> Chain 1:                0.276056 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL 'adf329cdde529e84ce328f199529dd87' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 2.6e-05 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.26 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.137426 seconds (Warm-up) #> Chain 2:                0.145637 seconds (Sampling) #> Chain 2:                0.283063 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL 'adf329cdde529e84ce328f199529dd87' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 1.5e-05 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.142173 seconds (Warm-up) #> Chain 3:                0.130589 seconds (Sampling) #> Chain 3:                0.272762 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL 'adf329cdde529e84ce328f199529dd87' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 1.4e-05 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.129502 seconds (Warm-up) #> Chain 4:                0.134787 seconds (Sampling) #> Chain 4:                0.264289 seconds (Total) #> Chain 4:  #> Sampling priors, please wait... #> Warning: Bayes factors might not be precise. #>   For precise Bayes factors, sampling at least 40,000 posterior samples is #>   recommended. #> Bayes Factor (Savage-Dickey density ratio)  #>  #> Parameter   |    BF #> ------------------- #> (Intercept) |  5.83 #> group1      | 10.90 #>  #> * Evidence Against The Null: 0 # }"},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_restricted.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayes Factors (BF) for Order Restricted Models — bayesfactor_restricted","title":"Bayes Factors (BF) for Order Restricted Models — bayesfactor_restricted","text":"method computes Bayes factors comparing model order restrictions parameters fully unrestricted model. Note method used confirmatory analyses.  bf_* function alias main function. info, particular specifying correct priors factors 2 levels, see Bayes factors vignette.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_restricted.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayes Factors (BF) for Order Restricted Models — bayesfactor_restricted","text":"","code":"bayesfactor_restricted(   posterior,   hypothesis,   prior = NULL,   verbose = TRUE,   ... )  bf_restricted(posterior, hypothesis, prior = NULL, verbose = TRUE, ...)  # S3 method for stanreg bayesfactor_restricted(   posterior,   hypothesis,   prior = NULL,   verbose = TRUE,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"conditional\", \"zi\", \"zero_inflated\", \"all\"),   ... )  # S3 method for brmsfit bayesfactor_restricted(   posterior,   hypothesis,   prior = NULL,   verbose = TRUE,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"conditional\", \"zi\", \"zero_inflated\", \"all\"),   ... )  # S3 method for blavaan bayesfactor_restricted(   posterior,   hypothesis,   prior = NULL,   verbose = TRUE,   ... )  # S3 method for emmGrid bayesfactor_restricted(   posterior,   hypothesis,   prior = NULL,   verbose = TRUE,   ... )  # S3 method for bayesfactor_restricted as.logical(x, which = c(\"posterior\", \"prior\"), ...)"},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_restricted.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayes Factors (BF) for Order Restricted Models — bayesfactor_restricted","text":"posterior stanreg / brmsfit object, emmGrid data frame - representing posterior distribution(s) (see Details). hypothesis character vector specifying restrictions logical conditions (see examples ). prior object representing prior distribution (see Details). verbose Toggle warnings. ... Currently used. effects results fixed effects, random effects returned? applies mixed models. May abbreviated. component results parameters, parameters conditional model zero-inflated part model returned? May abbreviated. applies brms-models. x object class bayesfactor_restricted logical matrix posterior prior distribution(s)?","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_restricted.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bayes Factors (BF) for Order Restricted Models — bayesfactor_restricted","text":"data frame containing (log) Bayes factor representing evidence un-restricted model (Use .numeric() extract non-log Bayes factors; see examples). (bool_results attribute contains results sample, indicating included hypothesized restriction.)","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_restricted.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bayes Factors (BF) for Order Restricted Models — bayesfactor_restricted","text":"method used compute Bayes factors order-restricted models vs un-restricted models setting order restriction prior posterior distributions (Morey & Wagenmakers, 2013).  (Though possible use bayesfactor_restricted() test interval restrictions, suitable testing order restrictions; see examples).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_restricted.html","id":"setting-the-correct-prior","dir":"Reference","previous_headings":"","what":"Setting the correct prior","title":"Bayes Factors (BF) for Order Restricted Models — bayesfactor_restricted","text":"computation Bayes factors, model priors must proper priors (least flat, preferable informative); priors alternative get wider, likelihood null value(s) increases, extreme completely flat priors null infinitely favorable alternative (called Jeffreys-Lindley-Bartlett paradox). Thus, ever try (want) compute Bayes factor informed prior.  (Note default, brms::brm() uses flat priors fixed-effects; See example .)  important provide correct prior meaningful results. posterior numerical vector, prior also numerical vector. posterior data.frame, prior also data.frame, matching column order. posterior stanreg, brmsfit supported Bayesian model: prior can set NULL, case prior samples drawn internally. prior can also model equivalent posterior samples priors . See unupdate(). Note: posterior brmsfit_multiple model, prior must provided. posterior emmGrid / emm_list object: prior also emmGrid / emm_list object equivalent posterior created model priors samples . See unupdate(). prior can also original (posterior) model. , function try update emmGrid / emm_list use unupdate()d prior-model. (done brmsfit models.) Note: emmGrid undergone transformations (\"log\", \"response\", etc.), regriding, prior must emmGrid object, stated .","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_restricted.html","id":"interpreting-bayes-factors","dir":"Reference","previous_headings":"","what":"Interpreting Bayes Factors","title":"Bayes Factors (BF) for Order Restricted Models — bayesfactor_restricted","text":"Bayes factor greater 1 can interpreted evidence null, one convention Bayes factor greater 3 can considered \"substantial\" evidence null (vice versa, Bayes factor smaller 1/3 indicates substantial evidence favor null-model) (Wetzels et al. 2011).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_restricted.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Bayes Factors (BF) for Order Restricted Models — bayesfactor_restricted","text":"Morey, R. D., & Wagenmakers, E. J. (2014). Simple relation Bayesian order-restricted point-null hypothesis tests. Statistics & Probability Letters, 92, 121-124. Morey, R. D., & Rouder, J. N. (2011). Bayes factor approaches testing interval null hypotheses. Psychological methods, 16(4), 406. Morey, R. D. (Jan, 2015). Multiple Comparisons BayesFactor, Part 2 – order restrictions. Retrieved https://richarddmorey.org/category/order-restrictions/.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayesfactor_restricted.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bayes Factors (BF) for Order Restricted Models — bayesfactor_restricted","text":"","code":"set.seed(444) library(bayestestR) prior <- data.frame(   A = rnorm(1000),   B = rnorm(1000),   C = rnorm(1000) )  posterior <- data.frame(   A = rnorm(1000, .4, 0.7),   B = rnorm(1000, -.2, 0.4),   C = rnorm(1000, 0, 0.5) )  hyps <- c(   \"A > B & B > C\",   \"A > B & A > C\",   \"C > A\" )   (b <- bayesfactor_restricted(posterior, hypothesis = hyps, prior = prior)) #> Bayes Factor (Order-Restriction) #>  #> Hypothesis    P(Prior) P(Posterior)    BF #> A > B & B > C     0.17         0.25  1.52 #> A > B & A > C     0.33         0.62  1.91 #> C > A             0.52         0.31 0.600 #>  #> * Bayes factors for the restricted model vs. the un-restricted model.  bool <- as.logical(b, which = \"posterior\") head(bool) #>      A > B & B > C A > B & A > C C > A #> [1,]         FALSE          TRUE FALSE #> [2,]         FALSE          TRUE FALSE #> [3,]          TRUE          TRUE FALSE #> [4,]         FALSE          TRUE FALSE #> [5,]         FALSE          TRUE FALSE #> [6,]         FALSE          TRUE FALSE   see::plots(   plot(estimate_density(posterior)),   # distribution **conditional** on the restrictions   plot(estimate_density(posterior[bool[, hyps[1]], ])) + ggplot2::ggtitle(hyps[1]),   plot(estimate_density(posterior[bool[, hyps[2]], ])) + ggplot2::ggtitle(hyps[2]),   plot(estimate_density(posterior[bool[, hyps[3]], ])) + ggplot2::ggtitle(hyps[3]),   guides = \"collect\" )  # \\dontrun{ # rstanarm models # --------------- data(\"mtcars\")  fit_stan <- rstanarm::stan_glm(mpg ~ wt + cyl + am,   data = mtcars, refresh = 0 ) hyps <- c(   \"am > 0 & cyl < 0\",   \"cyl < 0\",   \"wt - cyl > 0\" )  bayesfactor_restricted(fit_stan, hypothesis = hyps) #> Sampling priors, please wait... #> Bayes Factor (Order-Restriction) #>  #> Hypothesis       P(Prior) P(Posterior)    BF #> am > 0 & cyl < 0     0.25         0.54  2.15 #> cyl < 0              0.49         1.00  2.04 #> wt - cyl > 0         0.48         0.10 0.199 #>  #> * Bayes factors for the restricted model vs. the un-restricted model. # } # \\dontrun{ # emmGrid objects # --------------- # replicating http://bayesfactor.blogspot.com/2015/01/multiple-comparisons-with-bayesfactor-2.html data(\"disgust\") contrasts(disgust$condition) <- contr.equalprior_pairs # see vignette fit_model <- rstanarm::stan_glm(score ~ condition, data = disgust, family = gaussian()) #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 2.3e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.23 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.039882 seconds (Warm-up) #> Chain 1:                0.057582 seconds (Sampling) #> Chain 1:                0.097464 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 1.2e-05 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.040465 seconds (Warm-up) #> Chain 2:                0.055512 seconds (Sampling) #> Chain 2:                0.095977 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 1.4e-05 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.040542 seconds (Warm-up) #> Chain 3:                0.064357 seconds (Sampling) #> Chain 3:                0.104899 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 2.7e-05 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.039054 seconds (Warm-up) #> Chain 4:                0.058759 seconds (Sampling) #> Chain 4:                0.097813 seconds (Total) #> Chain 4:   em_condition <- emmeans::emmeans(fit_model, ~condition) hyps <- c(\"lemon < control & control < sulfur\")  bayesfactor_restricted(em_condition, prior = fit_model, hypothesis = hyps) #> Sampling priors, please wait... #> Bayes Factor (Order-Restriction) #>  #> Hypothesis                         P(Prior) P(Posterior)   BF #> lemon < control & control < sulfur     0.17         0.76 4.56 #>  #> * Bayes factors for the restricted model vs. the un-restricted model. # > # Bayes Factor (Order-Restriction) # > # >                          Hypothesis P(Prior) P(Posterior)   BF # >  lemon < control & control < sulfur     0.17         0.75 4.49 # > --- # > Bayes factors for the restricted model vs. the un-restricted model. # }"},{"path":"https://easystats.github.io/bayestestR/reference/bayestestR-package.html","id":null,"dir":"Reference","previous_headings":"","what":"bayestestR: Describing Effects and their Uncertainty, Existence and\nSignificance within the Bayesian Framework — bayestestR-package","title":"bayestestR: Describing Effects and their Uncertainty, Existence and\nSignificance within the Bayesian Framework — bayestestR-package","text":"Existing R packages allow users easily fit large variety models extract visualize posterior draws. However, packages return limited set indices (e.g., point-estimates CIs). bayestestR provides comprehensive consistent set functions analyze describe posterior distributions generated variety models objects, including popular modeling packages rstanarm, brms BayesFactor. References: Makowski et al. (2019) doi:10.21105/joss.01541 Makowski et al. (2019) doi:10.3389/fpsyg.2019.02767","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bayestestR-package.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"bayestestR: Describing Effects and their Uncertainty, Existence and\nSignificance within the Bayesian Framework — bayestestR-package","text":"bayestestR","code":""},{"path":[]},{"path":"https://easystats.github.io/bayestestR/reference/bayestestR-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"bayestestR: Describing Effects and their Uncertainty, Existence and\nSignificance within the Bayesian Framework — bayestestR-package","text":"Maintainer: Dominique Makowski dom.makowski@gmail.com (ORCID) (@Dom_Makowski) Authors: Daniel Lüdecke d.luedecke@uke.de (ORCID) (@strengejacke) Mattan S. Ben-Shachar matanshm@post.bgu.ac.il (ORCID) (@mattansb) Indrajeet Patil patilindrajeet.science@gmail.com (ORCID) (@patilindrajeets) Michael D. Wilson michael.d.wilson@curtin.edu.au (ORCID) Brenton M. Wiernik brenton@wiernik.org (ORCID) (@bmwiernik) contributors: Paul-Christian Bürkner paul.buerkner@gmail.com [reviewer] Tristan Mahr tristan.mahr@wisc.edu (ORCID) [reviewer] Henrik Singmann singmann@gmail.com (ORCID) [contributor] Quentin F. Gronau (ORCID) [contributor] Sam Crawley sam@crawley.nz (ORCID) [contributor]","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bci.html","id":null,"dir":"Reference","previous_headings":"","what":"Bias Corrected and Accelerated Interval (BCa) — bci","title":"Bias Corrected and Accelerated Interval (BCa) — bci","text":"Compute Bias Corrected Accelerated Interval (BCa) posterior distributions.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bias Corrected and Accelerated Interval (BCa) — bci","text":"","code":"bci(x, ...)  bcai(x, ...)  # S3 method for numeric bci(x, ci = 0.95, verbose = TRUE, ...)  # S3 method for data.frame bci(x, ci = 0.95, verbose = TRUE, ...)  # S3 method for MCMCglmm bci(x, ci = 0.95, verbose = TRUE, ...)  # S3 method for sim.merMod bci(   x,   ci = 0.95,   effects = c(\"fixed\", \"random\", \"all\"),   parameters = NULL,   verbose = TRUE,   ... )  # S3 method for sim bci(x, ci = 0.95, parameters = NULL, verbose = TRUE, ...)  # S3 method for emmGrid bci(x, ci = 0.95, verbose = TRUE, ...)  # S3 method for stanreg bci(   x,   ci = 0.95,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"location\", \"all\", \"conditional\", \"smooth_terms\", \"sigma\",     \"distributional\", \"auxiliary\"),   parameters = NULL,   verbose = TRUE,   ... )  # S3 method for brmsfit bci(   x,   ci = 0.95,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"conditional\", \"zi\", \"zero_inflated\", \"all\"),   parameters = NULL,   verbose = TRUE,   ... )  # S3 method for BFBayesFactor bci(x, ci = 0.95, verbose = TRUE, ...)"},{"path":"https://easystats.github.io/bayestestR/reference/bci.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bias Corrected and Accelerated Interval (BCa) — bci","text":"x Vector representing posterior distribution, data frame vectors. Can also Bayesian model. bayestestR supports wide range models (see, example, methods(\"hdi\")) documented 'Usage' section, methods classes mostly resemble arguments .numeric .data.framemethods. ... Currently used. ci Value vector probability (credible) interval - CI (0 1) estimated. Default .95 (95%). verbose Toggle warnings. effects results fixed effects, random effects returned? applies mixed models. May abbreviated. parameters Regular expression pattern describes parameters returned. Meta-parameters (like lp__ prior_) filtered default, parameters typically appear summary() returned. Use parameters select specific parameters output. component results parameters, parameters conditional model zero-inflated part model returned? May abbreviated. applies brms-models.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bci.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bias Corrected and Accelerated Interval (BCa) — bci","text":"data frame following columns: Parameter model parameter(s), x model-object. x vector, column missing. CI probability credible interval. CI_low, CI_high lower upper credible interval limits parameters.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bci.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bias Corrected and Accelerated Interval (BCa) — bci","text":"Unlike equal-tailed intervals (see eti()) typically exclude 2.5% tail distribution always include median, HDI equal-tailed therefore always includes mode(s) posterior distributions. can useful better represent credibility mass distribution, HDI also limitations. See spi() details.  95% 89% Credible Intervals (CI) two reasonable ranges characterize uncertainty related estimation (see discussion differences two values).  89% intervals (ci = 0.89) deemed stable , instance, 95% intervals (Kruschke, 2014). effective sample size least 10.000 recommended one wants estimate 95% intervals high precision (Kruschke, 2014, p. 183ff). Unfortunately, default number posterior samples Bayes packages (e.g., rstanarm brms) 4.000 (thus, might want increase fitting model). Moreover, 89 indicates arbitrariness interval limits - remarkable property highest prime number exceed already unstable 95% threshold (McElreath, 2015).  However, 95% advantages . instance, shares (case normal posterior distribution) intuitive relationship standard deviation conveys accurate image (artificial) bounds distribution. Also, wider, makes analyses conservative (.e., probability covering 0 larger 95% CI lower ranges 89%), good thing context reproducibility crisis.  95% equal-tailed interval (ETI) 2.5% distribution either side limits. indicates 2.5th percentile 97.5h percentile. symmetric distributions, two methods computing credible intervals, ETI HDI, return similar results.  case skewed distributions. Indeed, possible parameter values ETI lower credibility (less probable) parameter values outside ETI. property seems undesirable summary credible values distribution.  hand, ETI range change transformations applied distribution (instance, log odds scale probabilities): lower higher bounds transformed distribution correspond transformed lower higher bounds original distribution. contrary, applying transformations distribution change resulting HDI.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bci.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Bias Corrected and Accelerated Interval (BCa) — bci","text":"DiCiccio, T. J. B. Efron. (1996). Bootstrap Confidence Intervals. Statistical Science. 11(3): 189–212. 10.1214/ss/1032280214","code":""},{"path":[]},{"path":"https://easystats.github.io/bayestestR/reference/bci.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bias Corrected and Accelerated Interval (BCa) — bci","text":"","code":"posterior <- rnorm(1000) bci(posterior) #> 95% ETI: [-2.04, 1.91] bci(posterior, ci = c(0.80, 0.89, 0.95)) #> Equal-Tailed Interval #>  #> 80% ETI       |       89% ETI |       95% ETI #> --------------------------------------------- #> [-1.34, 1.34] | [-1.64, 1.61] | [-2.04, 1.91]"},{"path":"https://easystats.github.io/bayestestR/reference/bic_to_bf.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert BIC indices to Bayes Factors via the BIC-approximation method. — bic_to_bf","title":"Convert BIC indices to Bayes Factors via the BIC-approximation method. — bic_to_bf","text":"difference two Bayesian information criterion (BIC) indices two models can used approximate Bayes factors via:  $$BF_{10} = e^{(BIC_0 - BIC_1)/2}$$","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bic_to_bf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert BIC indices to Bayes Factors via the BIC-approximation method. — bic_to_bf","text":"","code":"bic_to_bf(bic, denominator, log = FALSE)"},{"path":"https://easystats.github.io/bayestestR/reference/bic_to_bf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert BIC indices to Bayes Factors via the BIC-approximation method. — bic_to_bf","text":"bic vector BIC values. denominator BIC value use denominator (test ). log TRUE, return log(BF).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bic_to_bf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert BIC indices to Bayes Factors via the BIC-approximation method. — bic_to_bf","text":"Bayes Factors corresponding BIC values denominator.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bic_to_bf.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Convert BIC indices to Bayes Factors via the BIC-approximation method. — bic_to_bf","text":"Wagenmakers, E. J. (2007). practical solution pervasive problems p values. Psychonomic bulletin & review, 14(5), 779-804","code":""},{"path":"https://easystats.github.io/bayestestR/reference/bic_to_bf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert BIC indices to Bayes Factors via the BIC-approximation method. — bic_to_bf","text":"","code":"bic1 <- BIC(lm(Sepal.Length ~ 1, data = iris)) bic2 <- BIC(lm(Sepal.Length ~ Species, data = iris)) bic3 <- BIC(lm(Sepal.Length ~ Species + Petal.Length, data = iris)) bic4 <- BIC(lm(Sepal.Length ~ Species * Petal.Length, data = iris))  bic_to_bf(c(bic1, bic2, bic3, bic4), denominator = bic1) #> [1] 1.000000e+00 1.695852e+29 5.843105e+55 2.203243e+54"},{"path":"https://easystats.github.io/bayestestR/reference/check_prior.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if Prior is Informative — check_prior","title":"Check if Prior is Informative — check_prior","text":"Performs simple test check whether prior informative posterior. idea, accompanying heuristics, discussed blogpost.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/check_prior.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if Prior is Informative — check_prior","text":"","code":"check_prior(model, method = \"gelman\", simulate_priors = TRUE, ...)"},{"path":"https://easystats.github.io/bayestestR/reference/check_prior.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if Prior is Informative — check_prior","text":"model stanreg, stanfit, brmsfit, blavaan, MCMCglmm object. method Can \"gelman\" \"lakeland\". \"gelman\" method, SD posterior 0.1 times SD prior, prior considered informative. \"lakeland\" method, prior considered informative posterior falls within 95% HDI prior. simulate_priors prior distributions simulated using simulate_prior() (default; faster) sampled via unupdate() (slower, accurate). ... Currently used.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/check_prior.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if Prior is Informative — check_prior","text":"data frame two columns: parameter names quality prior (might \"informative\", \"uninformative\") \"determinable\" prior distribution determined).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/check_prior.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Check if Prior is Informative — check_prior","text":"https://statmodeling.stat.columbia.edu/2019/08/10/","code":""},{"path":"https://easystats.github.io/bayestestR/reference/check_prior.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check if Prior is Informative — check_prior","text":"","code":"# \\dontrun{ library(bayestestR) if (require(\"rstanarm\")) {   model <- stan_glm(mpg ~ wt + am, data = mtcars, chains = 1, refresh = 0)   check_prior(model, method = \"gelman\")   check_prior(model, method = \"lakeland\")    # An extreme example where both methods diverge:   model <- stan_glm(mpg ~ wt,     data = mtcars[1:3, ],     prior = normal(-3.3, 1, FALSE),     prior_intercept = normal(0, 1000, FALSE),     refresh = 0   )   check_prior(model, method = \"gelman\")   check_prior(model, method = \"lakeland\")   plot(si(model)) # can provide visual confirmation to the Lakeland method } #> Sampling priors, please wait... #> Warning: Support intervals might not be precise. #>   For precise support intervals, sampling at least 40,000 posterior #>   samples is recommended. #> Warning: More than 1 SI detected. Plot the result to investigate.  # }"},{"path":"https://easystats.github.io/bayestestR/reference/ci.html","id":null,"dir":"Reference","previous_headings":"","what":"Confidence/Credible/Compatibility Interval (CI) — ci","title":"Confidence/Credible/Compatibility Interval (CI) — ci","text":"Compute Confidence/Credible/Compatibility Intervals (CI) Support Intervals (SI) Bayesian frequentist models. Documentation accessible :","code":""},{"path":"https://easystats.github.io/bayestestR/reference/ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Confidence/Credible/Compatibility Interval (CI) — ci","text":"","code":"ci(x, ...)  # S3 method for numeric ci(x, ci = 0.95, method = \"ETI\", verbose = TRUE, BF = 1, ...)  # S3 method for data.frame ci(x, ci = 0.95, method = \"ETI\", verbose = TRUE, BF = 1, ...)  # S3 method for sim.merMod ci(   x,   ci = 0.95,   method = \"ETI\",   effects = c(\"fixed\", \"random\", \"all\"),   parameters = NULL,   verbose = TRUE,   ... )  # S3 method for sim ci(x, ci = 0.95, method = \"ETI\", parameters = NULL, verbose = TRUE, ...)  # S3 method for stanreg ci(   x,   ci = 0.95,   method = \"ETI\",   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"location\", \"all\", \"conditional\", \"smooth_terms\", \"sigma\",     \"distributional\", \"auxiliary\"),   parameters = NULL,   verbose = TRUE,   BF = 1,   ... )  # S3 method for brmsfit ci(   x,   ci = 0.95,   method = \"ETI\",   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"conditional\", \"zi\", \"zero_inflated\", \"all\"),   parameters = NULL,   verbose = TRUE,   BF = 1,   ... )  # S3 method for BFBayesFactor ci(x, ci = 0.95, method = \"ETI\", verbose = TRUE, BF = 1, ...)  # S3 method for MCMCglmm ci(x, ci = 0.95, method = \"ETI\", verbose = TRUE, ...)"},{"path":"https://easystats.github.io/bayestestR/reference/ci.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Confidence/Credible/Compatibility Interval (CI) — ci","text":"x stanreg brmsfit model, vector representing posterior distribution. ... Currently used. ci Value vector probability CI (0 1) estimated. Default .95 (95%). method Can 'ETI' (default), 'HDI', 'BCI', 'SPI' 'SI'. verbose Toggle warnings. BF amount support required included support interval. effects results fixed effects, random effects returned? applies mixed models. May abbreviated. parameters Regular expression pattern describes parameters returned. Meta-parameters (like lp__ prior_) filtered default, parameters typically appear summary() returned. Use parameters select specific parameters output. component results parameters, parameters conditional model zero-inflated part model returned? May abbreviated. applies brms-models.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/ci.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Confidence/Credible/Compatibility Interval (CI) — ci","text":"data frame following columns: Parameter model parameter(s), x model-object. x vector, column missing. CI probability credible interval. CI_low, CI_high lower upper credible interval limits parameters.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/ci.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Confidence/Credible/Compatibility Interval (CI) — ci","text":"Bayesian models Frequentist models","code":""},{"path":"https://easystats.github.io/bayestestR/reference/ci.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Confidence/Credible/Compatibility Interval (CI) — ci","text":"comes interpretation, recommend thinking CI terms \"uncertainty\" \"compatibility\" interval, latter defined “Given value interval background assumptions, data seem surprising” (Gelman & Greenland 2019).  also plot()-method implemented see-package.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/ci.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Confidence/Credible/Compatibility Interval (CI) — ci","text":"Gelman , Greenland S. confidence intervals better termed \"uncertainty intervals\"? BMJ 2019;l5381. 10.1136/bmj.l5381","code":""},{"path":[]},{"path":"https://easystats.github.io/bayestestR/reference/ci.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Confidence/Credible/Compatibility Interval (CI) — ci","text":"","code":"library(bayestestR)  posterior <- rnorm(1000) ci(posterior, method = \"ETI\") #> 95% ETI: [-1.91, 1.95] ci(posterior, method = \"HDI\") #> 95% HDI: [-1.98, 1.82]  df <- data.frame(replicate(4, rnorm(100))) ci(df, method = \"ETI\", ci = c(0.80, 0.89, 0.95)) #> Equal-Tailed Interval #>  #> Parameter |       80% ETI |       89% ETI |       95% ETI #> --------------------------------------------------------- #> X1        | [-1.09, 1.23] | [-1.38, 1.65] | [-1.58, 1.95] #> X2        | [-1.56, 1.02] | [-1.82, 1.15] | [-2.11, 1.35] #> X3        | [-1.21, 0.97] | [-1.28, 1.12] | [-1.52, 1.71] #> X4        | [-1.06, 1.54] | [-1.50, 1.74] | [-1.93, 2.01] ci(df, method = \"HDI\", ci = c(0.80, 0.89, 0.95)) #> Highest Density Interval #>  #> Parameter |       80% HDI |       89% HDI |       95% HDI #> --------------------------------------------------------- #> X1        | [-1.04, 1.29] | [-1.60, 1.26] | [-1.60, 1.91] #> X2        | [-1.18, 1.18] | [-1.67, 1.22] | [-2.39, 1.22] #> X3        | [-1.00, 1.06] | [-1.28, 1.16] | [-1.89, 1.30] #> X4        | [-0.70, 1.74] | [-1.21, 1.92] | [-2.02, 2.08]  model <- stan_glm(mpg ~ wt, data = mtcars, chains = 2, iter = 200, refresh = 0) #> Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#bulk-ess #> Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#tail-ess ci(model, method = \"ETI\", ci = c(0.80, 0.89)) #> Equal-Tailed Interval #>  #> Parameter   |        80% ETI |        89% ETI | Effects |   Component #> --------------------------------------------------------------------- #> (Intercept) | [35.16, 39.35] | [34.66, 40.09] |   fixed | conditional #> wt          | [-6.01, -4.66] | [-6.23, -4.49] |   fixed | conditional ci(model, method = \"HDI\", ci = c(0.80, 0.89)) #> Highest Density Interval  #>  #> Parameter   |        80% HDI |        89% HDI #> --------------------------------------------- #> (Intercept) | [35.08, 39.34] | [34.61, 39.97] #> wt          | [-5.92, -4.60] | [-6.27, -4.60] model <- brms::brm(mpg ~ wt + cyl, data = mtcars) #> Compiling Stan program... #> Start sampling #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 1.6e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.028077 seconds (Warm-up) #> Chain 1:                0.025228 seconds (Sampling) #> Chain 1:                0.053305 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 7e-06 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.033921 seconds (Warm-up) #> Chain 2:                0.031564 seconds (Sampling) #> Chain 2:                0.065485 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 7e-06 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.030286 seconds (Warm-up) #> Chain 3:                0.032974 seconds (Sampling) #> Chain 3:                0.06326 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 7e-06 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.027123 seconds (Warm-up) #> Chain 4:                0.026767 seconds (Sampling) #> Chain 4:                0.05389 seconds (Total) #> Chain 4:  ci(model, method = \"ETI\") #> Equal-Tailed Interval #>  #> Parameter   |        95% ETI | Effects |   Component #> ---------------------------------------------------- #> b_Intercept | [36.13, 43.22] |   fixed | conditional #> b_wt        | [-4.83, -1.62] |   fixed | conditional #> b_cyl       | [-2.39, -0.63] |   fixed | conditional ci(model, method = \"HDI\") #> Highest Density Interval  #>  #> Parameter   |        95% HDI #> ---------------------------- #> (Intercept) | [36.25, 43.29] #> wt          | [-4.64, -1.47] #> cyl         | [-2.42, -0.67] bf <- ttestBF(x = rnorm(100, 1, 1)) ci(bf, method = \"ETI\") #> Equal-Tailed Interval #>  #> Parameter  |      95% ETI #> ------------------------- #> Difference | [0.70, 1.12] ci(bf, method = \"HDI\") #> Highest Density Interval #>  #> Parameter  |      95% HDI #> ------------------------- #> Difference | [0.73, 1.13] model <- emtrends(model, ~1, \"wt\") ci(model, method = \"ETI\") #> Equal-Tailed Interval #>  #> Parameter |        95% ETI #> -------------------------- #> overall   | [-4.83, -1.62] ci(model, method = \"HDI\") #> Highest Density Interval #>  #> Parameter |        95% HDI #> -------------------------- #> overall   | [-4.64, -1.47]"},{"path":"https://easystats.github.io/bayestestR/reference/contr.equalprior.html","id":null,"dir":"Reference","previous_headings":"","what":"Contrast Matrices for Equal Marginal Priors in Bayesian Estimation — contr.equalprior","title":"Contrast Matrices for Equal Marginal Priors in Bayesian Estimation — contr.equalprior","text":"Build contrasts factors equal marginal priors levels. 3 functions give orthogonal contrasts, scaled differently allow different prior specifications (see 'Details'). Implementation Singmann & Gronau's bfrms, following description Rouder, Morey, Speckman, & Province (2012, p. 363).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/contr.equalprior.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Contrast Matrices for Equal Marginal Priors in Bayesian Estimation — contr.equalprior","text":"","code":"contr.equalprior(n, contrasts = TRUE, sparse = FALSE)  contr.equalprior_pairs(n, contrasts = TRUE, sparse = FALSE)  contr.equalprior_deviations(n, contrasts = TRUE, sparse = FALSE)"},{"path":"https://easystats.github.io/bayestestR/reference/contr.equalprior.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Contrast Matrices for Equal Marginal Priors in Bayesian Estimation — contr.equalprior","text":"n vector levels factor, number levels. contrasts logical indicating whether contrasts     computed. sparse logical indicating result sparse     (class dgCMatrix), using     package Matrix.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/contr.equalprior.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Contrast Matrices for Equal Marginal Priors in Bayesian Estimation — contr.equalprior","text":"matrix n rows k columns, k=n-1 contrasts TRUE k=n contrasts FALSE.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/contr.equalprior.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Contrast Matrices for Equal Marginal Priors in Bayesian Estimation — contr.equalprior","text":"using stats::contr.treatment, dummy variable difference level reference level. useful setting different priors coefficient, used one trying set general prior differences means, (well stats::contr.sum others) results unequal marginal priors means difference .   can see priors means (narrow prior), likewise pairwise differences (priors differences narrow). solution use one methods provided , result marginally equal priors means differences . Though obscure interpretation parameters, setting equal priors means differences important useful specifying equal priors means factor differences correct estimation Bayes factors contrasts order restrictions multi-level factors (k>2). See info specifying correct priors factors 2 levels Bayes factors vignette. NOTE: setting priors dummy variables, always: Use priors centered 0! location/centered priors meaningless! Use identically-scaled priors dummy variables single factor! contr.equalprior returns original orthogonal-normal contrasts described Rouder, Morey, Speckman, & Province (2012, p. 363). Setting contrasts = FALSE returns \\(I_{n} - \\frac{1}{n}\\) matrix.","code":"library(brms)  data <- data.frame(   group = factor(rep(LETTERS[1:4], each = 3)),   y = rnorm(12) )  contrasts(data$group) # R's default contr.treatment #>   B C D #> A 0 0 0 #> B 1 0 0 #> C 0 1 0 #> D 0 0 1  model_prior <- brm(   y ~ group, data = data,   sample_prior = \"only\",   # Set the same priors on the 3 dummy variable   # (Using an arbitrary scale)   prior = set_prior(\"normal(0, 10)\", coef = c(\"groupB\", \"groupC\", \"groupD\")) )  est <- emmeans::emmeans(model_prior, pairwise ~ group)  point_estimate(est, centr = \"mean\", disp = TRUE) #> Point Estimate #> #> Parameter |  Mean |    SD #> ------------------------- #> A         | -0.01 |  6.35 #> B         | -0.10 |  9.59 #> C         |  0.11 |  9.55 #> D         | -0.16 |  9.52 #> A - B     |  0.10 |  9.94 #> A - C     | -0.12 |  9.96 #> A - D     |  0.15 |  9.87 #> B - C     | -0.22 | 14.38 #> B - D     |  0.05 | 14.14 #> C - D     |  0.27 | 14.00"},{"path":"https://easystats.github.io/bayestestR/reference/contr.equalprior.html","id":"contr-equalprior-pairs","dir":"Reference","previous_headings":"","what":"contr.equalprior_pairs","title":"Contrast Matrices for Equal Marginal Priors in Bayesian Estimation — contr.equalprior","text":"Useful setting priors terms pairwise differences means - scales priors defines prior distribution pair-wise differences pairwise differences (e.g., - B, B - C, etc.).   means prior distribution, distribution differences matches prior set \"normal(0, 10)\". Success!","code":"contrasts(data$group) <- contr.equalprior_pairs contrasts(data$group) #>         [,1]       [,2]       [,3] #> A  0.0000000  0.6123724  0.0000000 #> B -0.1893048 -0.2041241  0.5454329 #> C -0.3777063 -0.2041241 -0.4366592 #> D  0.5670111 -0.2041241 -0.1087736  model_prior <- brm(   y ~ group, data = data,   sample_prior = \"only\",   # Set the same priors on the 3 dummy variable   # (Using an arbitrary scale)   prior = set_prior(\"normal(0, 10)\", coef = c(\"group1\", \"group2\", \"group3\")) )  est <- emmeans(model_prior, pairwise ~ group)  point_estimate(est, centr = \"mean\", disp = TRUE) #> Point Estimate #> #> Parameter |  Mean |    SD #> ------------------------- #> A         | -0.31 |  7.46 #> B         | -0.24 |  7.47 #> C         | -0.34 |  7.50 #> D         | -0.30 |  7.25 #> A - B     | -0.08 | 10.00 #> A - C     |  0.03 | 10.03 #> A - D     | -0.01 |  9.85 #> B - C     |  0.10 | 10.28 #> B - D     |  0.06 |  9.94 #> C - D     | -0.04 | 10.18"},{"path":"https://easystats.github.io/bayestestR/reference/contr.equalprior.html","id":"contr-equalprior-deviations","dir":"Reference","previous_headings":"","what":"contr.equalprior_deviations","title":"Contrast Matrices for Equal Marginal Priors in Bayesian Estimation — contr.equalprior","text":"Useful setting priors terms deviations mean grand mean - scales priors defines prior distribution distance (, ) mean one levels might overall mean. (See examples.)","code":""},{"path":"https://easystats.github.io/bayestestR/reference/contr.equalprior.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Contrast Matrices for Equal Marginal Priors in Bayesian Estimation — contr.equalprior","text":"Rouder, J. N., Morey, R. D., Speckman, P. L., & Province, J. M. (2012). Default Bayes factors ANOVA designs. Journal Mathematical Psychology, 56(5), 356-374. https://doi.org/10.1016/j.jmp.2012.08.001","code":""},{"path":"https://easystats.github.io/bayestestR/reference/contr.equalprior.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Contrast Matrices for Equal Marginal Priors in Bayesian Estimation — contr.equalprior","text":"","code":"contr.equalprior(2) # Q_2 in Rouder et al. (2012, p. 363) #>            [,1] #> [1,] -0.7071068 #> [2,]  0.7071068  contr.equalprior(5) # equivalent to Q_5 in Rouder et al. (2012, p. 363) #>               [,1]       [,2]       [,3]       [,4] #> [1,]  0.000000e+00  0.0000000  0.0000000  0.8944272 #> [2,] -4.163336e-17  0.0000000  0.8660254 -0.2236068 #> [3,] -5.773503e-01 -0.5773503 -0.2886751 -0.2236068 #> [4,] -2.113249e-01  0.7886751 -0.2886751 -0.2236068 #> [5,]  7.886751e-01 -0.2113249 -0.2886751 -0.2236068  ## check decomposition Q3 <- contr.equalprior(3) Q3 %*% t(Q3) ## 2/3 on diagonal and -1/3 on off-diagonal elements #>            [,1]       [,2]       [,3] #> [1,]  0.6666667 -0.3333333 -0.3333333 #> [2,] -0.3333333  0.6666667 -0.3333333 #> [3,] -0.3333333 -0.3333333  0.6666667"},{"path":"https://easystats.github.io/bayestestR/reference/convert_bayesian_as_frequentist.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert (refit) a Bayesian model to frequentist — convert_bayesian_as_frequentist","title":"Convert (refit) a Bayesian model to frequentist — convert_bayesian_as_frequentist","text":"Refit Bayesian model frequentist. Can useful comparisons.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/convert_bayesian_as_frequentist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert (refit) a Bayesian model to frequentist — convert_bayesian_as_frequentist","text":"","code":"convert_bayesian_as_frequentist(model, data = NULL, REML = TRUE)  bayesian_as_frequentist(model, data = NULL, REML = TRUE)"},{"path":"https://easystats.github.io/bayestestR/reference/convert_bayesian_as_frequentist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert (refit) a Bayesian model to frequentist — convert_bayesian_as_frequentist","text":"model Bayesian model. data Data used model. NULL, try extract model. REML mixed effects, models estimated using restricted maximum likelihood (REML) (TRUE, default) maximum likelihood (FALSE)?","code":""},{"path":"https://easystats.github.io/bayestestR/reference/convert_bayesian_as_frequentist.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert (refit) a Bayesian model to frequentist — convert_bayesian_as_frequentist","text":"","code":"# \\donttest{ # Rstanarm ---------------------- if (require(\"rstanarm\")) {   # Simple regressions   model <- stan_glm(Sepal.Length ~ Species,     data = iris, chains = 2, refresh = 0   )   bayesian_as_frequentist(model) } #>  #> Call: #> stats::lm(formula = formula$conditional, data = data) #>  #> Coefficients: #>       (Intercept)  Speciesversicolor   Speciesvirginica   #>             5.006              0.930              1.582   #>  # }  # \\dontrun{ if (require(\"rstanarm\")) {   model <- stan_glm(vs ~ mpg,     family = \"binomial\",     data = mtcars, chains = 2, refresh = 0   )   bayesian_as_frequentist(model)    # Mixed models   model <- stan_glmer(Sepal.Length ~ Petal.Length + (1 | Species),     data = iris, chains = 2, refresh = 0   )   bayesian_as_frequentist(model)    model <- stan_glmer(vs ~ mpg + (1 | cyl),     family = \"binomial\",     data = mtcars, chains = 2, refresh = 0   )   bayesian_as_frequentist(model) } #> Generalized linear mixed model fit by maximum likelihood (Laplace #>   Approximation) [glmerMod] #>  Family: binomial  ( logit ) #> Formula: vs ~ mpg + (1 | cyl) #>    Data: data #>      AIC      BIC   logLik deviance df.resid  #>  31.1738  35.5710 -12.5869  25.1738       29  #> Random effects: #>  Groups Name        Std.Dev. #>  cyl    (Intercept) 1.925    #> Number of obs: 32, groups:  cyl, 3 #> Fixed Effects: #> (Intercept)          mpg   #>     -3.9227       0.1723   # }"},{"path":"https://easystats.github.io/bayestestR/reference/cwi.html","id":null,"dir":"Reference","previous_headings":"","what":"Curvewise Intervals (CWI) — cwi","title":"Curvewise Intervals (CWI) — cwi","text":"Compute Curvewise interval (CWI) (also called \"simultaneous interval\" \"joint interval\") posterior distributions using ggdist::curve_interval(). Whereas typical \"pointwise intervals\" contain xx% posterior single parameter, joint/curvewise intervals contain xx% posterior distribution parameters.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/cwi.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Curvewise Intervals (CWI) — cwi","text":"","code":"cwi(x, ...)  # S3 method for data.frame cwi(x, ci = 0.95, ...)"},{"path":"https://easystats.github.io/bayestestR/reference/cwi.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Curvewise Intervals (CWI) — cwi","text":"x Vector representing posterior distribution, data frame vectors. Can also Bayesian model. bayestestR supports wide range models (see, example, methods(\"hdi\")) documented 'Usage' section, methods classes mostly resemble arguments .numeric .data.framemethods. ... Currently used. ci Value vector probability (credible) interval - CI (0 1) estimated. Default .95 (95%).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/cwi.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Curvewise Intervals (CWI) — cwi","text":"data frame following columns: Parameter model parameter(s), x model-object. x vector, column missing. CI probability credible interval. CI_low, CI_high lower upper credible interval limits parameters.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/cwi.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Curvewise Intervals (CWI) — cwi","text":"Applied model predictions, pointwise intervals contain xx% predicted response values conditional specific predictor values. contrast, curvewise intervals contain xx% predicted response values across predictor values. Put another way, curvewise intervals contain xx% full prediction lines model. details, see ggdist documentation curvewise intervals.","code":""},{"path":[]},{"path":"https://easystats.github.io/bayestestR/reference/cwi.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Curvewise Intervals (CWI) — cwi","text":"","code":"# \\donttest{ library(bayestestR)  if (require(\"ggplot2\") && require(\"rstanarm\") && require(\"ggdist\")) {   # Generate data =============================================   k <- 11 # number of curves (iterations)   n <- 201 # number of rows   data <- data.frame(x = seq(-15, 15, length.out = n))    # Simulate iterations as new columns   for (i in 1:k) {     data[paste0(\"iter_\", i)] <- dnorm(data$x, seq(-5, 5, length.out = k)[i], 3)   }    # Note: first, we need to transpose the data to have iters as rows   iters <- datawizard::data_transpose(data[paste0(\"iter_\", 1:k)])    # Compute Median   data$Median <- point_estimate(iters)[[\"Median\"]]    # Compute Credible Intervals ================================    # Compute ETI (default type of CI)   data[c(\"ETI_low\", \"ETI_high\")] <- eti(iters, ci = 0.5)[c(\"CI_low\", \"CI_high\")]    # Compute CWI   # ggdist::curve_interval(reshape_iterations(data), iter_value .width = c(.5))    # Visualization =============================================   ggplot(data, aes(x = x, y = Median)) +     geom_ribbon(aes(ymin = ETI_low, ymax = ETI_high), fill = \"red\", alpha = 0.3) +     geom_line(size = 1) +     geom_line(       data = reshape_iterations(data),       aes(y = iter_value, group = iter_group),       alpha = 0.3     ) } #> Loading required package: ggplot2 #> Loading required package: ggdist #>  #> Attaching package: ‘ggdist’ #> The following objects are masked from ‘package:brms’: #>  #>     dstudent_t, pstudent_t, qstudent_t, rstudent_t #> The following object is masked from ‘package:bayestestR’: #>  #>     hdi #> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. #> ℹ Please use `linewidth` instead.  # }"},{"path":"https://easystats.github.io/bayestestR/reference/density_at.html","id":null,"dir":"Reference","previous_headings":"","what":"Density Probability at a Given Value — density_at","title":"Density Probability at a Given Value — density_at","text":"Compute density value given point distribution (.e., value y axis value x distribution).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/density_at.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Density Probability at a Given Value — density_at","text":"","code":"density_at(posterior, x, precision = 2^10, method = \"kernel\", ...)"},{"path":"https://easystats.github.io/bayestestR/reference/density_at.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Density Probability at a Given Value — density_at","text":"posterior Vector representing posterior distribution. x value get approximate probability. precision Number points density data. See n parameter density. method Density estimation method. Can \"kernel\" (default), \"logspline\" \"KernSmooth\". ... Currently used.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/density_at.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Density Probability at a Given Value — density_at","text":"","code":"library(bayestestR) posterior <- distribution_normal(n = 10) density_at(posterior, 0) #> [1] 0.3207106 density_at(posterior, c(0, 1)) #> [1] 0.3207106 0.2375008"},{"path":"https://easystats.github.io/bayestestR/reference/describe_posterior.html","id":null,"dir":"Reference","previous_headings":"","what":"Describe Posterior Distributions — describe_posterior","title":"Describe Posterior Distributions — describe_posterior","text":"Compute indices relevant describe characterize posterior distributions.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/describe_posterior.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Describe Posterior Distributions — describe_posterior","text":"","code":"describe_posterior(posteriors, ...)  # S3 method for numeric describe_posterior(   posteriors,   centrality = \"median\",   dispersion = FALSE,   ci = 0.95,   ci_method = \"eti\",   test = c(\"p_direction\", \"rope\"),   rope_range = \"default\",   rope_ci = 0.95,   keep_iterations = FALSE,   bf_prior = NULL,   BF = 1,   ... )  # S3 method for stanreg describe_posterior(   posteriors,   centrality = \"median\",   dispersion = FALSE,   ci = 0.95,   ci_method = \"eti\",   test = c(\"p_direction\", \"rope\"),   rope_range = \"default\",   rope_ci = 0.95,   keep_iterations = FALSE,   bf_prior = NULL,   diagnostic = c(\"ESS\", \"Rhat\"),   priors = FALSE,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"location\", \"all\", \"conditional\", \"smooth_terms\", \"sigma\",     \"distributional\", \"auxiliary\"),   parameters = NULL,   BF = 1,   ... )  # S3 method for brmsfit describe_posterior(   posteriors,   centrality = \"median\",   dispersion = FALSE,   ci = 0.95,   ci_method = \"eti\",   test = c(\"p_direction\", \"rope\"),   rope_range = \"default\",   rope_ci = 0.95,   keep_iterations = FALSE,   bf_prior = NULL,   diagnostic = c(\"ESS\", \"Rhat\"),   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"conditional\", \"zi\", \"zero_inflated\", \"all\", \"location\",     \"distributional\", \"auxiliary\"),   parameters = NULL,   BF = 1,   priors = FALSE,   ... )"},{"path":"https://easystats.github.io/bayestestR/reference/describe_posterior.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Describe Posterior Distributions — describe_posterior","text":"posteriors vector, data frame model posterior draws. bayestestR supports wide range models (see methods(\"describe_posterior\")) documented 'Usage' section, methods classes mostly resemble arguments .numeric method. ... Additional arguments passed methods. centrality point-estimates (centrality indices) compute.  Character (vector) list one options: \"median\", \"mean\", \"MAP\" \"\". dispersion Logical, TRUE, computes indices dispersion related estimate(s) (SD MAD mean median, respectively). ci Value vector probability CI (0 1) estimated. Default .95 (95%). ci_method type index used Credible Interval. Can \"ETI\" (default, see eti()), \"HDI\" (see hdi()), \"BCI\" (see bci()), \"SPI\" (see spi()), \"SI\" (see si()). test indices effect existence compute. Character (vector) list one options: \"p_direction\" (\"pd\"), \"rope\", \"p_map\", \"equivalence_test\" (\"equitest\"), \"bayesfactor\" (\"bf\") \"\" compute tests. \"test\", corresponding bayestestR function called (e.g. rope() p_direction()) results included summary output. rope_range ROPE's lower higher bounds. list two values (e.g., c(-0.1, 0.1)) \"default\". \"default\", bounds set x +- 0.1*SD(response). rope_ci Credible Interval (CI) probability, corresponding proportion HDI, use percentage ROPE. keep_iterations TRUE, keep iterations (draws) bootstrapped Bayesian models. added additional columns named iter_1, iter_2, .... can reshape long format running reshape_iterations(). bf_prior Distribution representing prior computation Bayes factors / SI. Used input posterior, otherwise (case models) ignored. BF amount support required included support interval. diagnostic Diagnostic metrics compute.  Character (vector) list one options: \"ESS\", \"Rhat\", \"MCSE\" \"\". priors Add prior used parameter. effects results fixed effects, random effects returned? applies mixed models. May abbreviated. component results parameters, parameters conditional model zero-inflated part model returned? May abbreviated. applies brms-models. parameters Regular expression pattern describes parameters returned. Meta-parameters (like lp__ prior_) filtered default, parameters typically appear summary() returned. Use parameters select specific parameters output.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/describe_posterior.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Describe Posterior Distributions — describe_posterior","text":"One components point estimates (like posterior mean median), intervals tests can omitted summary output setting related argument NULL. example, test = NULL centrality = NULL return HDI (CI).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/describe_posterior.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Describe Posterior Distributions — describe_posterior","text":"Makowski, D., Ben-Shachar, M. S., Chen, S. H. ., Lüdecke, D. (2019). Indices Effect Existence Significance Bayesian Framework. Frontiers Psychology 2019;10:2767. doi:10.3389/fpsyg.2019.02767 Region Practical Equivalence (ROPE) Bayes factors","code":""},{"path":"https://easystats.github.io/bayestestR/reference/describe_posterior.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Describe Posterior Distributions — describe_posterior","text":"","code":"library(bayestestR)  if (require(\"logspline\")) {   x <- rnorm(1000)   describe_posterior(x)   describe_posterior(x, centrality = \"all\", dispersion = TRUE, test = \"all\")   describe_posterior(x, ci = c(0.80, 0.90))    df <- data.frame(replicate(4, rnorm(100)))   describe_posterior(df)   describe_posterior(df, centrality = \"all\", dispersion = TRUE, test = \"all\")   describe_posterior(df, ci = c(0.80, 0.90))    df <- data.frame(replicate(4, rnorm(20)))   head(reshape_iterations(describe_posterior(df, keep_iterations = TRUE))) } #> Warning: Could not estimate a good default ROPE range. Using 'c(-0.1, 0.1)'. #> Warning: Could not estimate a good default ROPE range. Using 'c(-0.1, 0.1)'. #> Warning: Prior not specified! Please specify a prior (in the form 'prior = #>   distribution_normal(1000, 0, 1)') to get meaningful results. #> Warning: Bayes factors might not be precise. #>   For precise Bayes factors, sampling at least 40,000 posterior samples is #>   recommended. #> Warning: Could not estimate a good default ROPE range. Using 'c(-0.1, 0.1)'. #> Warning: Prior not specified! Please specify priors (with column order matching #>   'posterior') to get meaningful results. #> Warning: Bayes factors might not be precise. #>   For precise Bayes factors, sampling at least 40,000 posterior samples is #>   recommended. #>   Parameter     Median   CI    CI_low   CI_high   pd ROPE_CI ROPE_low ROPE_high #> 1        X1 -0.6515244 0.95 -1.630517 0.2687579 0.70    0.95     -0.1       0.1 #> 2        X2 -0.1329377 0.95 -1.176432 1.0924115 0.60    0.95     -0.1       0.1 #> 3        X3 -0.3181801 0.95 -1.612736 1.2170517 0.70    0.95     -0.1       0.1 #> 4        X4  0.1420190 0.95 -1.120196 1.3832350 0.55    0.95     -0.1       0.1 #> 5        X1 -0.6515244 0.95 -1.630517 0.2687579 0.70    0.95     -0.1       0.1 #> 6        X2 -0.1329377 0.95 -1.176432 1.0924115 0.60    0.95     -0.1       0.1 #>   ROPE_Percentage iter_index iter_group iter_value #> 1      0.11111111          1          1  0.1401798 #> 2      0.11111111          2          1 -0.5956548 #> 3      0.05555556          3          1  0.9770296 #> 4      0.16666667          4          1  0.3966213 #> 5      0.11111111          1          2 -0.8382385 #> 6      0.11111111          2          2 -0.8799540 # \\dontrun{ # rstanarm models # ----------------------------------------------- if (require(\"rstanarm\") && require(\"emmeans\")) {   model <- stan_glm(mpg ~ wt + gear, data = mtcars, chains = 2, iter = 200, refresh = 0)   describe_posterior(model)   describe_posterior(model, centrality = \"all\", dispersion = TRUE, test = \"all\")   describe_posterior(model, ci = c(0.80, 0.90))    # emmeans estimates   # -----------------------------------------------   describe_posterior(emtrends(model, ~1, \"wt\")) } #> Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#bulk-ess #> Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#tail-ess #> Warning: Bayes factors might not be precise. #>   For precise Bayes factors, sampling at least 40,000 posterior samples is #>   recommended. #> Summary of Posterior Distribution #>  #> Parameter | Median |         95% CI |   pd |          ROPE | % in ROPE #> ---------------------------------------------------------------------- #> overall   |  -5.43 | [-7.02, -4.02] | 100% | [-0.10, 0.10] |        0%  # brms models # ----------------------------------------------- if (require(\"brms\")) {   model <- brms::brm(mpg ~ wt + cyl, data = mtcars)   describe_posterior(model)   describe_posterior(model, ci = c(0.80, 0.90)) } #> Compiling Stan program... #> recompiling to avoid crashing R session #> Start sampling #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 1.2e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.027703 seconds (Warm-up) #> Chain 1:                0.023969 seconds (Sampling) #> Chain 1:                0.051672 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 7e-06 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.028427 seconds (Warm-up) #> Chain 2:                0.030368 seconds (Sampling) #> Chain 2:                0.058795 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 6e-06 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.044158 seconds (Warm-up) #> Chain 3:                0.036725 seconds (Sampling) #> Chain 3:                0.080883 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 8e-06 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.032024 seconds (Warm-up) #> Chain 4:                0.03075 seconds (Sampling) #> Chain 4:                0.062774 seconds (Total) #> Chain 4:  #> Summary of Posterior Distribution  #>  #> Parameter   | Median |         80% CI |         90% CI |     pd |          ROPE | % in ROPE |  Rhat |     ESS #> ------------------------------------------------------------------------------------------------------------- #> (Intercept) |  39.64 | [37.40, 41.95] | [36.73, 42.55] |   100% | [-0.60, 0.60] |        0% | 1.000 | 5744.00 #> wt          |  -3.17 | [-4.20, -2.13] | [-4.49, -1.80] |   100% | [-0.60, 0.60] |        0% | 1.001 | 2180.00 #> cyl         |  -1.52 | [-2.08, -0.96] | [-2.23, -0.79] | 99.92% | [-0.60, 0.60] |        0% | 1.000 | 2202.00  # BayesFactor objects # ----------------------------------------------- if (require(\"BayesFactor\")) {   bf <- ttestBF(x = rnorm(100, 1, 1))   describe_posterior(bf)   describe_posterior(bf, centrality = \"all\", dispersion = TRUE, test = \"all\")   describe_posterior(bf, ci = c(0.80, 0.90)) } #> Summary of Posterior Distribution #>  #> Parameter  | Median |       80% CI |       90% CI |   pd |          ROPE | % in ROPE |       BF |              Prior #> -------------------------------------------------------------------------------------------------------------------- #> Difference |   0.85 | [0.71, 0.99] | [0.68, 1.03] | 100% | [-0.10, 0.10] |        0% | 1.36e+10 | Cauchy (0 +- 0.71) # }"},{"path":"https://easystats.github.io/bayestestR/reference/describe_prior.html","id":null,"dir":"Reference","previous_headings":"","what":"Describe Priors — describe_prior","title":"Describe Priors — describe_prior","text":"Returns summary priors used model.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/describe_prior.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Describe Priors — describe_prior","text":"","code":"describe_prior(model, ...)  # S3 method for brmsfit describe_prior(   model,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"conditional\", \"zi\", \"zero_inflated\", \"all\", \"location\",     \"distributional\", \"auxiliary\"),   parameters = NULL,   ... )"},{"path":"https://easystats.github.io/bayestestR/reference/describe_prior.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Describe Priors — describe_prior","text":"model Bayesian model. ... Currently used. effects results fixed effects, random effects returned? applies mixed models. May abbreviated. component results parameters, parameters conditional model zero-inflated part model returned? May abbreviated. applies brms-models. parameters Regular expression pattern describes parameters returned. Meta-parameters (like lp__ prior_) filtered default, parameters typically appear summary() returned. Use parameters select specific parameters output.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/describe_prior.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Describe Priors — describe_prior","text":"","code":"# \\dontrun{ library(bayestestR)  # rstanarm models # ----------------------------------------------- if (require(\"rstanarm\")) {   model <- rstanarm::stan_glm(mpg ~ wt + cyl, data = mtcars)   describe_prior(model) } #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 2.4e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.24 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.057606 seconds (Warm-up) #> Chain 1:                0.054853 seconds (Sampling) #> Chain 1:                0.112459 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 1.3e-05 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.065177 seconds (Warm-up) #> Chain 2:                0.063695 seconds (Sampling) #> Chain 2:                0.128872 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 1.4e-05 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.061258 seconds (Warm-up) #> Chain 3:                0.059621 seconds (Sampling) #> Chain 3:                0.120879 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 1.2e-05 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.056074 seconds (Warm-up) #> Chain 4:                0.057445 seconds (Sampling) #> Chain 4:                0.113519 seconds (Total) #> Chain 4:  #>     Parameter Prior_Distribution Prior_Location Prior_Scale #> 1 (Intercept)             normal       20.09062   15.067370 #> 2          wt             normal        0.00000   15.399106 #> 3         cyl             normal        0.00000    8.436748  # brms models # ----------------------------------------------- if (require(\"brms\")) {   model <- brms::brm(mpg ~ wt + cyl, data = mtcars)   describe_prior(model) } #> Compiling Stan program... #> Start sampling #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 1.1e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.030612 seconds (Warm-up) #> Chain 1:                0.028931 seconds (Sampling) #> Chain 1:                0.059543 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 7e-06 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.0326 seconds (Warm-up) #> Chain 2:                0.031513 seconds (Sampling) #> Chain 2:                0.064113 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 7e-06 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.034917 seconds (Warm-up) #> Chain 3:                0.027808 seconds (Sampling) #> Chain 3:                0.062725 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 7e-06 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.027427 seconds (Warm-up) #> Chain 4:                0.025535 seconds (Sampling) #> Chain 4:                0.052962 seconds (Total) #> Chain 4:  #>     Parameter Prior_Distribution Prior_Location Prior_Scale Prior_df #> 1 b_Intercept          student_t           19.2         5.4        3 #> 2        b_wt            uniform             NA          NA       NA #> 3       b_cyl            uniform             NA          NA       NA #> 4       sigma          student_t            0.0         5.4        3  # BayesFactor objects # ----------------------------------------------- if (require(\"BayesFactor\")) {   bf <- ttestBF(x = rnorm(100, 1, 1))   describe_prior(bf) } #>    Parameter Prior_Distribution Prior_Location Prior_Scale #> 1 Difference             cauchy              0   0.7071068 # }"},{"path":"https://easystats.github.io/bayestestR/reference/diagnostic_draws.html","id":null,"dir":"Reference","previous_headings":"","what":"Diagnostic values for each iteration — diagnostic_draws","title":"Diagnostic values for each iteration — diagnostic_draws","text":"Returns accumulated log-posterior, average Metropolis acceptance rate, divergent transitions, treedepth rather terminated evolution normally.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/diagnostic_draws.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Diagnostic values for each iteration — diagnostic_draws","text":"","code":"diagnostic_draws(posteriors, ...)"},{"path":"https://easystats.github.io/bayestestR/reference/diagnostic_draws.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Diagnostic values for each iteration — diagnostic_draws","text":"posteriors stanreg, stanfit, brmsfit, blavaan object. ... Currently used.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/diagnostic_draws.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Diagnostic values for each iteration — diagnostic_draws","text":"","code":"# \\dontrun{ set.seed(333)  if (require(\"brms\", quietly = TRUE)) {   model <- brm(mpg ~ wt * cyl * vs,     data = mtcars,     iter = 100, control = list(adapt_delta = 0.80),     refresh = 0   )   diagnostic_draws(model) } #> Compiling Stan program... #> Start sampling #> Warning: There were 62 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See #> https://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded #> Warning: Examine the pairs() plot to diagnose sampling problems #> Warning: The largest R-hat is 2.29, indicating chains have not mixed. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#r-hat #> Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#bulk-ess #> Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#tail-ess #>     Chain Iteration Acceptance_Rate  Step_Size Tree_Depth n_Leapfrog Divergent #> 1       1         1       1.0000000 0.03696581          3          7         0 #> 2       1         2       0.9894872 0.03696581          9        767         0 #> 3       1         3       0.9551779 0.03696581          8        447         0 #> 4       1         4       0.8838487 0.03696581          9        519         0 #> 5       1         5       0.8383379 0.03696581          6        107         0 #> 6       1         6       0.9910736 0.03696581          9        959         0 #> 7       1         7       0.9580047 0.03696581          6         63         0 #> 8       1         8       0.9813853 0.03696581          8        319         0 #> 9       1         9       0.9563311 0.03696581          9       1023         0 #> 10      1        10       0.8072390 0.03696581          6        127         0 #> 11      1        11       0.9907066 0.03696581          8        511         0 #> 12      1        12       0.7033913 0.03696581         10       1023         0 #> 13      1        13       0.9502721 0.03696581         10       1023         0 #> 14      1        14       0.9236364 0.03696581          8        511         0 #> 15      1        15       0.8545915 0.03696581          6        127         0 #> 16      1        16       0.4970200 0.03696581          6        127         0 #> 17      1        17       0.9688066 0.03696581          6         63         0 #> 18      1        18       0.9583665 0.03696581          6         95         0 #> 19      1        19       0.9712676 0.03696581          8        319         0 #> 20      1        20       0.9513006 0.03696581          6         79         0 #> 21      1        21       0.9492356 0.03696581          8        511         0 #> 22      1        22       0.9892077 0.03696581         10       1023         0 #> 23      1        23       0.9837775 0.03696581          5         31         0 #> 24      1        24       0.9678266 0.03696581         10       1023         0 #> 25      1        25       0.9298230 0.03696581          9        703         0 #> 26      1        26       0.9767824 0.03696581          9        735         0 #> 27      1        27       0.9501833 0.03696581         10       1023         0 #> 28      1        28       0.8225470 0.03696581         10       1023         0 #> 29      1        29       0.2139818 0.03696581          7        131         0 #> 30      1        30       0.8802679 0.03696581          6         63         0 #> 31      1        31       0.9993004 0.03696581          6         63         0 #> 32      1        32       0.9995453 0.03696581          5         31         0 #> 33      1        33       0.9844625 0.03696581          6         95         0 #> 34      1        34       0.5669878 0.03696581          7        191         0 #> 35      1        35       0.9962562 0.03696581         10       1023         0 #> 36      1        36       0.9905468 0.03696581         10       1023         0 #> 37      1        37       0.9688774 0.03696581          9        639         0 #> 38      1        38       0.9650367 0.03696581          9        575         0 #> 39      1        39       0.9859473 0.03696581         10       1023         0 #> 40      1        40       0.9801381 0.03696581         10       1023         0 #> 41      1        41       0.9232358 0.03696581          6         63         0 #> 42      1        42       0.8954614 0.03696581          4         23         0 #> 43      1        43       0.9140245 0.03696581          6         63         0 #> 44      1        44       0.9640199 0.03696581         10       1023         0 #> 45      1        45       0.9799776 0.03696581          6        127         0 #> 46      1        46       0.9646881 0.03696581          7        127         0 #> 47      1        47       0.9803538 0.03696581         10       1023         0 #> 48      1        48       0.9291462 0.03696581          7        191         0 #> 49      1        49       0.9310889 0.03696581          9        959         0 #> 50      1        50       0.9820208 0.03696581          9        575         0 #> 51      2         1       0.9840734 0.03913321          8        511         0 #> 52      2         2       0.9916898 0.03913321         10       1023         0 #> 53      2         3       0.4125404 0.03913321          3         15         0 #> 54      2         4       0.8820916 0.03913321          5         47         0 #> 55      2         5       0.9982727 0.03913321          7        167         0 #> 56      2         6       0.9954919 0.03913321          8        319         0 #> 57      2         7       0.9702520 0.03913321         10       1023         0 #> 58      2         8       0.9950590 0.03913321          8        271         0 #> 59      2         9       0.8939403 0.03913321          9        767         0 #> 60      2        10       0.9774728 0.03913321          6        127         0 #> 61      2        11       0.9736513 0.03913321         10       1023         0 #> 62      2        12       0.9941285 0.03913321          9        831         0 #> 63      2        13       0.9864682 0.03913321          7        255         0 #> 64      2        14       0.9220716 0.03913321          8        255         0 #> 65      2        15       0.9730662 0.03913321          7        255         0 #> 66      2        16       0.9637653 0.03913321          9        703         0 #> 67      2        17       0.9776346 0.03913321          9        959         0 #> 68      2        18       0.9812320 0.03913321          8        383         0 #> 69      2        19       0.9950280 0.03913321          8        255         0 #> 70      2        20       0.8555305 0.03913321         10       1023         0 #> 71      2        21       0.8114497 0.03913321         10       1023         0 #> 72      2        22       0.9936517 0.03913321         10       1023         0 #> 73      2        23       0.9511408 0.03913321          6         63         0 #> 74      2        24       0.8795138 0.03913321         10       1023         0 #> 75      2        25       0.7676204 0.03913321          6        127         0 #> 76      2        26       0.9076580 0.03913321          6         63         0 #> 77      2        27       0.9747519 0.03913321          7        127         0 #> 78      2        28       0.9973579 0.03913321          7        191         0 #> 79      2        29       0.9903424 0.03913321          6        127         0 #> 80      2        30       0.9400705 0.03913321          6         63         0 #> 81      2        31       0.9941171 0.03913321          7        255         0 #> 82      2        32       0.7860512 0.03913321         10       1023         0 #> 83      2        33       0.9358181 0.03913321          9        671         0 #> 84      2        34       0.9060514 0.03913321          8        319         0 #> 85      2        35       0.9973750 0.03913321          6         63         0 #> 86      2        36       0.9550003 0.03913321          6        127         0 #> 87      2        37       0.9994275 0.03913321          4         31         0 #> 88      2        38       0.9889282 0.03913321          6        127         0 #> 89      2        39       0.8585775 0.03913321         10       1023         0 #> 90      2        40       0.7945934 0.03913321         10       1023         0 #> 91      2        41       0.9736460 0.03913321          9        639         0 #> 92      2        42       0.9116174 0.03913321          8        319         0 #> 93      2        43       0.9520999 0.03913321          7        127         0 #> 94      2        44       0.9927156 0.03913321          6        127         0 #> 95      2        45       0.9586724 0.03913321         10       1023         0 #> 96      2        46       0.8395297 0.03913321         10       1023         0 #> 97      2        47       0.9915700 0.03913321          8        383         0 #> 98      2        48       0.8011094 0.03913321          7        223         0 #> 99      2        49       0.9983364 0.03913321          8        511         0 #> 100     2        50       0.9998636 0.03913321         10       1023         0 #> 101     3         1       0.9907123 0.03617950         10       1023         0 #> 102     3         2       0.9638903 0.03617950         10       1023         0 #> 103     3         3       0.9828862 0.03617950          6        127         0 #> 104     3         4       0.9856038 0.03617950         10       1023         0 #> 105     3         5       0.8317391 0.03617950         10       1023         0 #> 106     3         6       0.9813305 0.03617950          8        511         0 #> 107     3         7       0.9926690 0.03617950          7        255         0 #> 108     3         8       0.9313785 0.03617950         10       1023         0 #> 109     3         9       0.9119247 0.03617950          8        383         0 #> 110     3        10       0.9326973 0.03617950          9        895         0 #> 111     3        11       0.8659167 0.03617950          7        255         0 #> 112     3        12       0.9961598 0.03617950          8        383         0 #> 113     3        13       0.9873728 0.03617950          7        175         0 #> 114     3        14       0.9731802 0.03617950          9        639         0 #> 115     3        15       0.9805663 0.03617950         10       1023         0 #> 116     3        16       0.9238887 0.03617950         10       1023         0 #> 117     3        17       0.9328058 0.03617950          9        695         0 #> 118     3        18       0.6172113 0.03617950          6        127         0 #> 119     3        19       0.9999440 0.03617950         10       1023         0 #> 120     3        20       0.9971349 0.03617950         10       1023         0 #> 121     3        21       0.9998964 0.03617950         10       1023         0 #> 122     3        22       0.9890977 0.03617950          9        639         0 #> 123     3        23       0.9535630 0.03617950          8        511         0 #> 124     3        24       0.9984774 0.03617950          6        127         0 #> 125     3        25       0.8109202 0.03617950          8        391         0 #> 126     3        26       0.9940565 0.03617950          5         63         0 #> 127     3        27       0.8751525 0.03617950          9        767         0 #> 128     3        28       0.8873955 0.03617950          7        143         0 #> 129     3        29       0.5948483 0.03617950          9        767         0 #> 130     3        30       0.9822950 0.03617950          6         63         0 #> 131     3        31       0.9987211 0.03617950          9        639         0 #> 132     3        32       0.9980513 0.03617950         10       1023         0 #> 133     3        33       0.7628818 0.03617950          7        127         0 #> 134     3        34       0.9971371 0.03617950          8        319         0 #> 135     3        35       0.8924336 0.03617950         10       1023         0 #> 136     3        36       0.9768615 0.03617950          6         63         0 #> 137     3        37       0.7974863 0.03617950          8        383         0 #> 138     3        38       0.9599486 0.03617950         10       1023         0 #> 139     3        39       0.9673094 0.03617950          5         63         0 #> 140     3        40       0.9241732 0.03617950          9        895         0 #> 141     3        41       0.9628010 0.03617950          6         63         0 #> 142     3        42       0.6691461 0.03617950         10       1023         0 #> 143     3        43       0.9727235 0.03617950          9       1023         0 #> 144     3        44       0.7447363 0.03617950         10       1023         0 #> 145     3        45       0.9800381 0.03617950         10       1023         0 #> 146     3        46       0.7910821 0.03617950          6         83         0 #> 147     3        47       0.9962949 0.03617950         10       1023         0 #> 148     3        48       0.9693448 0.03617950         10       1023         0 #> 149     3        49       0.8559418 0.03617950          7        255         0 #> 150     3        50       0.9878421 0.03617950          6         63         0 #> 151     4         1       0.9987729 0.04011403         10       1023         0 #> 152     4         2       0.8463154 0.04011403          8        319         0 #> 153     4         3       0.9990641 0.04011403         10       1023         0 #> 154     4         4       0.6751057 0.04011403          8        495         0 #> 155     4         5       0.4755907 0.04011403          8        447         0 #> 156     4         6       0.9240176 0.04011403          8        367         0 #> 157     4         7       0.9194824 0.04011403          8        423         0 #> 158     4         8       0.9067960 0.04011403         10       1023         0 #> 159     4         9       0.9259001 0.04011403         10       1023         0 #> 160     4        10       0.8995777 0.04011403         10       1023         0 #> 161     4        11       0.7367353 0.04011403          8        383         0 #> 162     4        12       0.9184172 0.04011403          7        255         0 #> 163     4        13       0.9184769 0.04011403          6         63         0 #> 164     4        14       0.9803280 0.04011403          9        831         0 #> 165     4        15       0.9567343 0.04011403          8        351         0 #> 166     4        16       0.9703837 0.04011403          8        431         0 #> 167     4        17       0.6317957 0.04011403          9        543         0 #> 168     4        18       0.8618890 0.04011403          9        847         0 #> 169     4        19       0.9802403 0.04011403         10       1023         0 #> 170     4        20       0.9909320 0.04011403         10       1023         0 #> 171     4        21       0.9799004 0.04011403          8        447         0 #> 172     4        22       0.9163401 0.04011403         10       1023         0 #> 173     4        23       0.9901749 0.04011403          6        127         0 #> 174     4        24       0.9686919 0.04011403          5         47         0 #> 175     4        25       0.4697441 0.04011403          9        623         0 #> 176     4        26       0.9831706 0.04011403         10       1023         0 #> 177     4        27       0.8655124 0.04011403         10       1023         0 #> 178     4        28       0.9815127 0.04011403          9        671         0 #> 179     4        29       0.9197771 0.04011403          7        255         0 #> 180     4        30       0.8189271 0.04011403         10       1023         0 #> 181     4        31       0.8846078 0.04011403         10       1023         0 #> 182     4        32       0.9415000 0.04011403          6         63         0 #> 183     4        33       0.9960346 0.04011403         10       1023         0 #> 184     4        34       0.9450065 0.04011403         10       1023         0 #> 185     4        35       0.9701710 0.04011403         10       1023         0 #> 186     4        36       0.8950674 0.04011403         10       1023         0 #> 187     4        37       0.9908084 0.04011403         10       1023         0 #> 188     4        38       0.8776364 0.04011403          4         31         0 #> 189     4        39       0.9960288 0.04011403         10       1023         0 #> 190     4        40       0.9906567 0.04011403          8        255         0 #> 191     4        41       0.9924329 0.04011403         10       1023         0 #> 192     4        42       0.9855137 0.04011403          6        127         0 #> 193     4        43       0.9437785 0.04011403          8        319         0 #> 194     4        44       0.8350018 0.04011403          8        319         0 #> 195     4        45       0.9956688 0.04011403          9        575         0 #> 196     4        46       0.6731434 0.04011403          3          7         0 #> 197     4        47       0.9201033 0.04011403          9        639         0 #> 198     4        48       0.7055560 0.04011403          6         63         0 #> 199     4        49       0.9248196 0.04011403          6        127         0 #> 200     4        50       0.9853350 0.04011403          6         63         0 #>       Energy LogPosterior #> 1   83.38666    -77.87059 #> 2   85.88407    -77.65607 #> 3   82.91287    -79.35152 #> 4   85.09555    -79.85661 #> 5   83.86603    -79.26706 #> 6   85.29570    -76.56174 #> 7   80.59691    -78.69974 #> 8   81.78780    -79.72431 #> 9   82.40037    -78.02694 #> 10  83.17586    -78.77174 #> 11  83.44400    -77.97600 #> 12  82.40745    -76.22318 #> 13  79.35518    -76.27576 #> 14  81.79436    -78.59348 #> 15  84.42823    -80.17643 #> 16  91.32811    -85.08372 #> 17  86.49167    -80.97882 #> 18  86.70600    -80.50675 #> 19  84.32942    -78.26699 #> 20  83.67358    -78.24245 #> 21  82.70030    -76.43846 #> 22  78.78843    -77.32447 #> 23  78.10889    -77.37444 #> 24  84.32947    -80.27724 #> 25  85.05842    -81.11404 #> 26  83.17205    -78.65499 #> 27  85.66240    -78.77809 #> 28  84.45546    -79.56783 #> 29  88.91707    -77.38015 #> 30  82.10113    -80.48904 #> 31  81.58477    -80.74822 #> 32  81.71621    -78.26622 #> 33  80.68532    -78.41712 #> 34  86.52389    -78.64257 #> 35  82.94942    -78.64199 #> 36  84.20418    -80.38969 #> 37  84.39811    -77.75717 #> 38  82.30586    -79.63567 #> 39  83.15587    -79.88599 #> 40  84.20031    -77.39482 #> 41  80.05276    -79.11951 #> 42  84.94094    -83.52422 #> 43  86.57851    -81.55305 #> 44  85.67627    -81.22593 #> 45  83.70947    -79.18632 #> 46  81.42733    -79.21396 #> 47  86.68147    -79.29555 #> 48  83.42481    -80.48731 #> 49  88.51245    -82.04345 #> 50  86.22471    -79.69256 #> 51  82.42690    -76.45838 #> 52  81.86738    -78.69272 #> 53  84.67189    -81.61388 #> 54  92.52127    -88.21186 #> 55  90.23304    -83.72328 #> 56  90.01075    -83.95343 #> 57  86.26656    -79.89679 #> 58  82.06452    -79.94102 #> 59  86.73181    -78.62315 #> 60  86.08253    -80.87430 #> 61  91.26656    -82.14418 #> 62  84.42185    -78.84486 #> 63  81.95570    -78.13730 #> 64  82.72276    -81.30379 #> 65  85.87427    -80.39847 #> 66  84.03024    -78.45096 #> 67  85.15737    -83.01752 #> 68  85.49680    -79.60590 #> 69  82.87916    -78.43588 #> 70  84.53985    -78.29247 #> 71  84.59801    -80.16468 #> 72  82.17264    -79.06248 #> 73  80.14966    -77.48478 #> 74  81.50159    -78.52488 #> 75  83.69757    -80.62817 #> 76  89.38884    -84.63932 #> 77  90.67381    -85.17018 #> 78  98.85120    -88.74462 #> 79  93.04707    -83.08884 #> 80  86.42076    -79.76252 #> 81  84.29825    -79.80594 #> 82  86.70125    -79.23986 #> 83  86.73925    -80.79496 #> 84  86.23051    -79.81140 #> 85  86.50315    -82.93108 #> 86  87.67695    -82.70427 #> 87  83.88941    -80.40532 #> 88  83.04078    -79.61498 #> 89  82.33379    -75.28975 #> 90  80.50622    -75.47235 #> 91  78.58281    -76.97753 #> 92  81.92287    -79.80593 #> 93  82.97444    -80.65099 #> 94  84.81151    -78.67175 #> 95  82.60535    -78.25304 #> 96  82.27198    -76.76364 #> 97  78.31594    -75.73941 #> 98  79.46550    -77.79521 #> 99  83.64270    -78.00380 #> 100 83.94273    -80.77255 #> 101 83.03187    -78.00024 #> 102 85.92445    -76.68228 #> 103 78.69969    -76.29138 #> 104 79.13868    -76.61785 #> 105 79.51211    -75.51937 #> 106 81.19360    -78.17930 #> 107 81.55566    -78.25256 #> 108 81.69633    -77.35522 #> 109 81.46545    -77.96413 #> 110 83.45192    -79.01948 #> 111 83.10959    -80.06394 #> 112 82.45265    -78.38008 #> 113 82.62726    -81.32509 #> 114 86.93002    -83.40735 #> 115 91.00237    -85.69446 #> 116 90.83929    -83.25039 #> 117 88.27955    -78.65654 #> 118 83.50313    -78.45742 #> 119 80.12538    -77.12698 #> 120 79.34641    -77.32591 #> 121 84.34189    -79.32496 #> 122 82.38661    -78.38221 #> 123 82.91048    -77.68482 #> 124 81.32459    -77.20172 #> 125 80.27363    -76.83740 #> 126 80.22029    -75.67895 #> 127 82.76182    -77.67964 #> 128 80.29737    -76.86495 #> 129 82.68236    -78.99204 #> 130 84.93961    -79.92886 #> 131 85.44600    -81.26068 #> 132 85.73547    -80.93309 #> 133 86.06208    -78.72111 #> 134 85.15412    -80.23624 #> 135 85.95536    -79.96564 #> 136 84.45610    -79.47690 #> 137 85.60735    -80.47600 #> 138 88.66174    -81.71887 #> 139 86.17250    -80.50803 #> 140 84.46275    -79.70593 #> 141 80.96076    -77.73350 #> 142 85.89119    -78.66037 #> 143 82.00876    -80.59730 #> 144 91.23395    -83.36012 #> 145 85.73760    -80.41222 #> 146 87.93416    -84.74197 #> 147 89.65987    -83.52144 #> 148 86.57168    -80.67477 #> 149 85.85036    -80.28163 #> 150 82.94077    -77.37168 #> 151 85.33576    -78.93280 #> 152 84.20859    -79.05923 #> 153 80.72072    -78.00216 #> 154 84.65049    -79.73544 #> 155 84.54777    -83.56487 #> 156 88.58873    -82.96461 #> 157 87.87352    -80.13105 #> 158 83.54002    -78.79683 #> 159 84.05700    -78.49009 #> 160 82.01143    -77.57497 #> 161 84.66701    -81.51389 #> 162 87.63924    -82.05751 #> 163 86.27999    -81.63100 #> 164 87.35430    -82.23524 #> 165 87.96518    -82.49036 #> 166 86.38546    -82.42989 #> 167 88.73697    -83.27638 #> 168 87.91749    -81.52784 #> 169 83.10714    -77.89963 #> 170 82.24837    -78.46440 #> 171 80.84211    -78.14288 #> 172 87.19770    -81.65343 #> 173 87.11213    -79.79724 #> 174 88.71134    -82.88666 #> 175 88.59991    -82.80728 #> 176 88.73439    -79.93024 #> 177 85.59720    -81.11086 #> 178 84.22978    -78.60223 #> 179 81.82101    -79.21579 #> 180 82.41902    -77.74291 #> 181 85.75765    -80.63781 #> 182 82.74156    -80.52889 #> 183 84.50398    -77.29882 #> 184 80.58182    -78.82387 #> 185 86.70609    -78.77819 #> 186 82.44380    -79.54130 #> 187 83.68581    -80.73259 #> 188 86.89962    -81.23335 #> 189 86.11333    -81.84304 #> 190 82.61433    -79.60545 #> 191 85.42133    -80.91944 #> 192 82.73209    -80.55601 #> 193 85.16602    -81.60437 #> 194 86.75845    -82.25328 #> 195 84.55475    -78.89214 #> 196 88.19147    -77.52144 #> 197 82.07779    -77.79299 #> 198 89.36604    -82.96997 #> 199 87.56209    -80.08283 #> 200 82.84202    -79.94888 # }"},{"path":"https://easystats.github.io/bayestestR/reference/diagnostic_posterior.html","id":null,"dir":"Reference","previous_headings":"","what":"Posteriors Sampling Diagnostic — diagnostic_posterior","title":"Posteriors Sampling Diagnostic — diagnostic_posterior","text":"Extract diagnostic metrics (Effective Sample Size (ESS), Rhat Monte Carlo Standard Error MCSE).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/diagnostic_posterior.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Posteriors Sampling Diagnostic — diagnostic_posterior","text":"","code":"diagnostic_posterior(posteriors, diagnostic = c(\"ESS\", \"Rhat\"), ...)  # S3 method for stanreg diagnostic_posterior(   posteriors,   diagnostic = \"all\",   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"location\", \"all\", \"conditional\", \"smooth_terms\", \"sigma\",     \"distributional\", \"auxiliary\"),   parameters = NULL,   ... )  # S3 method for brmsfit diagnostic_posterior(   posteriors,   diagnostic = \"all\",   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"conditional\", \"zi\", \"zero_inflated\", \"all\"),   parameters = NULL,   ... )"},{"path":"https://easystats.github.io/bayestestR/reference/diagnostic_posterior.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Posteriors Sampling Diagnostic — diagnostic_posterior","text":"posteriors stanreg, stanfit, brmsfit, blavaan object. diagnostic Diagnostic metrics compute.  Character (vector) list one options: \"ESS\", \"Rhat\", \"MCSE\" \"\". ... Currently used. effects parameters fixed effects, random effects returned? applies mixed models. May abbreviated. component type parameters return, parameters conditional model, zero-inflated part model, dispersion term, instrumental variables marginal effects returned? Applies models zero-inflated /dispersion formula, models instrumental variables (called fixed-effects regressions), models marginal effects mfx. May abbreviated. Note conditional component also called count mean component, depending model. three convenient shortcuts: component = \"\" returns possible parameters. component = \"location\", location parameters conditional, zero_inflated, smooth_terms, instruments returned (everything fixed random effects - depending effects argument - auxiliary parameters). component = \"distributional\" (\"auxiliary\"), components like sigma, dispersion, beta precision (auxiliary parameters) returned. parameters Regular expression pattern describes parameters returned.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/diagnostic_posterior.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Posteriors Sampling Diagnostic — diagnostic_posterior","text":"Effective Sample (ESS) large possible, although applications, effective sample size greater 1000 sufficient stable estimates (Bürkner, 2017). ESS corresponds number independent samples estimation power N autocorrelated samples. measure “much independent information autocorrelated chains” (Kruschke 2015, p182-3). Rhat closest 1. larger 1.1 (Gelman Rubin, 1992) 1.01 (Vehtari et al., 2019). split Rhat statistic quantifies consistency ensemble Markov chains. Monte Carlo Standard Error (MCSE) another measure accuracy chains. defined standard deviation chains divided effective sample size (formula mcse() Kruschke 2015, p. 187). MCSE “provides quantitative suggestion big estimation noise ”.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/diagnostic_posterior.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Posteriors Sampling Diagnostic — diagnostic_posterior","text":"Gelman, ., & Rubin, D. B. (1992). Inference iterative simulation using multiple sequences. Statistical science, 7(4), 457-472. Vehtari, ., Gelman, ., Simpson, D., Carpenter, B., Bürkner, P. C. (2019). Rank-normalization, folding, localization: improved Rhat assessing convergence MCMC. arXiv preprint arXiv:1903.08008. Kruschke, J. (2014). Bayesian data analysis: tutorial R, JAGS, Stan. Academic Press.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/diagnostic_posterior.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Posteriors Sampling Diagnostic — diagnostic_posterior","text":"","code":"# \\dontrun{ # rstanarm models # ----------------------------------------------- if (require(\"rstanarm\", quietly = TRUE)) {   model <- stan_glm(mpg ~ wt + gear, data = mtcars, chains = 2, iter = 200, refresh = 0)   diagnostic_posterior(model) } #> Warning: The largest R-hat is 1.13, indicating chains have not mixed. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#r-hat #> Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#bulk-ess #> Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#tail-ess #> Warning: Markov chains did not converge! Do not analyze results! #>     Parameter      Rhat      ESS       MCSE #> 1 (Intercept) 0.9980336 182.6025 0.36283152 #> 2        gear 0.9917174 206.3058 0.06519599 #> 3          wt 0.9978902 186.7773 0.04770867  # brms models # ----------------------------------------------- if (require(\"brms\", quietly = TRUE)) {   model <- brms::brm(mpg ~ wt + cyl, data = mtcars)   diagnostic_posterior(model) } #> Compiling Stan program... #> Start sampling #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 1.2e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.029588 seconds (Warm-up) #> Chain 1:                0.02671 seconds (Sampling) #> Chain 1:                0.056298 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 7e-06 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.032602 seconds (Warm-up) #> Chain 2:                0.028569 seconds (Sampling) #> Chain 2:                0.061171 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 7e-06 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.030808 seconds (Warm-up) #> Chain 3:                0.031525 seconds (Sampling) #> Chain 3:                0.062333 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 8e-06 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.032544 seconds (Warm-up) #> Chain 4:                0.029762 seconds (Sampling) #> Chain 4:                0.062306 seconds (Total) #> Chain 4:  #>     Parameter      Rhat      ESS        MCSE #> 1 b_Intercept 0.9997601 4553.140 0.025783250 #> 2       b_cyl 1.0007901 2017.036 0.009788731 #> 3        b_wt 1.0005248 1974.860 0.018154517 # }"},{"path":"https://easystats.github.io/bayestestR/reference/disgust.html","id":null,"dir":"Reference","previous_headings":"","what":"Moral Disgust Judgment — disgust","title":"Moral Disgust Judgment — disgust","text":"sample (simulated) dataset, used tests examples.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/disgust.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Moral Disgust Judgment — disgust","text":"data frame 500 rows 5 variables: score Score questionnaire, ranges 0 50 higher scores representing harsher moral judgment condition one three conditions, differing odor present room: pleasant scent associated cleanliness (lemon), disgusting scent (sulfur), control condition unusual odor present","code":"data(\"disgust\") head(disgust, n = 5) #>   score condition #> 1    13   control #> 2    26   control #> 3    30   control #> 4    23   control #> 5    34   control"},{"path":"https://easystats.github.io/bayestestR/reference/disgust.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Moral Disgust Judgment — disgust","text":"Richard D. Morey","code":""},{"path":"https://easystats.github.io/bayestestR/reference/distribution.html","id":null,"dir":"Reference","previous_headings":"","what":"Empirical Distributions — distribution","title":"Empirical Distributions — distribution","text":"Generate sequence n-quantiles, .e., sample size n near-perfect distribution.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/distribution.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Empirical Distributions — distribution","text":"","code":"distribution(type = \"normal\", ...)  distribution_custom(n, type = \"norm\", ..., random = FALSE)  distribution_beta(n, shape1, shape2, ncp = 0, random = FALSE, ...)  distribution_binomial(n, size = 1, prob = 0.5, random = FALSE, ...)  distribution_binom(n, size = 1, prob = 0.5, random = FALSE, ...)  distribution_cauchy(n, location = 0, scale = 1, random = FALSE, ...)  distribution_chisquared(n, df, ncp = 0, random = FALSE, ...)  distribution_chisq(n, df, ncp = 0, random = FALSE, ...)  distribution_gamma(n, shape, scale = 1, random = FALSE, ...)  distribution_mixture_normal(n, mean = c(-3, 3), sd = 1, random = FALSE, ...)  distribution_normal(n, mean = 0, sd = 1, random = FALSE, ...)  distribution_gaussian(n, mean = 0, sd = 1, random = FALSE, ...)  distribution_nbinom(n, size, prob, mu, phi, random = FALSE, ...)  distribution_poisson(n, lambda = 1, random = FALSE, ...)  distribution_student(n, df, ncp, random = FALSE, ...)  distribution_t(n, df, ncp, random = FALSE, ...)  distribution_student_t(n, df, ncp, random = FALSE, ...)  distribution_tweedie(n, xi = NULL, mu, phi, power = NULL, random = FALSE, ...)  distribution_uniform(n, min = 0, max = 1, random = FALSE, ...)  rnorm_perfect(n, mean = 0, sd = 1)"},{"path":"https://easystats.github.io/bayestestR/reference/distribution.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Empirical Distributions — distribution","text":"type Can names base R's Distributions, like \"cauchy\", \"pois\" \"beta\". ... Arguments passed methods. n number observations random Generate near-perfect random (simple wrappers base R r* functions) distributions. shape1, shape2 non-negative parameters Beta distribution. ncp non-centrality parameter. size number trials (zero ). prob probability success trial. location, scale location scale parameters. df degrees freedom (non-negative, can non-integer). shape Shape parameter. mean vector means. sd vector standard deviations. mu mean phi Corresponding glmmTMB's implementation nbinom distribution, size=mu/phi. lambda vector (non-negative) means. xi tweedie distributions, value xi variance var(Y) = phi * mu^xi. power Alias xi. min, max lower upper limits distribution.  Must finite.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/distribution.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Empirical Distributions — distribution","text":"random = FALSE, function return q*(ppoints(n), ...).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/distribution.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Empirical Distributions — distribution","text":"","code":"library(bayestestR) x <- distribution(n = 10) plot(density(x))   x <- distribution(type = \"gamma\", n = 100, shape = 2) plot(density(x))"},{"path":"https://easystats.github.io/bayestestR/reference/dot-extract_priors_rstanarm.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract and Returns the priors formatted for rstanarm — .extract_priors_rstanarm","title":"Extract and Returns the priors formatted for rstanarm — .extract_priors_rstanarm","text":"Extract Returns priors formatted rstanarm","code":""},{"path":"https://easystats.github.io/bayestestR/reference/dot-extract_priors_rstanarm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract and Returns the priors formatted for rstanarm — .extract_priors_rstanarm","text":"","code":".extract_priors_rstanarm(model, ...)"},{"path":"https://easystats.github.io/bayestestR/reference/dot-prior_new_location.html","id":null,"dir":"Reference","previous_headings":"","what":"Set a new location for a prior — .prior_new_location","title":"Set a new location for a prior — .prior_new_location","text":"Set new location prior","code":""},{"path":"https://easystats.github.io/bayestestR/reference/dot-prior_new_location.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set a new location for a prior — .prior_new_location","text":"","code":".prior_new_location(prior, sign, magnitude = 10)"},{"path":"https://easystats.github.io/bayestestR/reference/dot-select_nums.html","id":null,"dir":"Reference","previous_headings":"","what":"select numerics columns — .select_nums","title":"select numerics columns — .select_nums","text":"select numerics columns","code":""},{"path":"https://easystats.github.io/bayestestR/reference/dot-select_nums.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"select numerics columns — .select_nums","text":"","code":".select_nums(x)"},{"path":"https://easystats.github.io/bayestestR/reference/effective_sample.html","id":null,"dir":"Reference","previous_headings":"","what":"Effective Sample Size (ESS) — effective_sample","title":"Effective Sample Size (ESS) — effective_sample","text":"function returns effective sample size (ESS).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/effective_sample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Effective Sample Size (ESS) — effective_sample","text":"","code":"effective_sample(model, ...)  # S3 method for brmsfit effective_sample(   model,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"conditional\", \"zi\", \"zero_inflated\", \"all\"),   parameters = NULL,   ... )  # S3 method for stanreg effective_sample(   model,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"location\", \"all\", \"conditional\", \"smooth_terms\", \"sigma\",     \"distributional\", \"auxiliary\"),   parameters = NULL,   ... )"},{"path":"https://easystats.github.io/bayestestR/reference/effective_sample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Effective Sample Size (ESS) — effective_sample","text":"model stanreg, stanfit, brmsfit, blavaan, MCMCglmm object. ... Currently used. effects results fixed effects, random effects returned? applies mixed models. May abbreviated. component results parameters, parameters conditional model zero-inflated part model returned? May abbreviated. applies brms-models. parameters Regular expression pattern describes parameters returned. Meta-parameters (like lp__ prior_) filtered default, parameters typically appear summary() returned. Use parameters select specific parameters output.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/effective_sample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Effective Sample Size (ESS) — effective_sample","text":"data frame two columns: Parameter name effective sample size (ESS).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/effective_sample.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Effective Sample Size (ESS) — effective_sample","text":"Effective Sample (ESS) large possible, altough applications, effective sample size greater 1,000 sufficient stable estimates (Bürkner, 2017). ESS corresponds number independent samples estimation power N autocorrelated samples. measure “much independent information autocorrelated chains” (Kruschke 2015, p182-3).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/effective_sample.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Effective Sample Size (ESS) — effective_sample","text":"Kruschke, J. (2014). Bayesian data analysis: tutorial R, JAGS, Stan. Academic Press. Bürkner, P. C. (2017). brms: R package Bayesian multilevel models using Stan. Journal Statistical Software, 80(1), 1-28","code":""},{"path":"https://easystats.github.io/bayestestR/reference/effective_sample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Effective Sample Size (ESS) — effective_sample","text":"","code":"# \\dontrun{ library(rstanarm) model <- stan_glm(mpg ~ wt + gear, data = mtcars, chains = 2, iter = 200, refresh = 0) #> Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#bulk-ess #> Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#tail-ess effective_sample(model) #>     Parameter ESS #> 1 (Intercept) 172 #> 2          wt 181 #> 3        gear 175 # }"},{"path":"https://easystats.github.io/bayestestR/reference/equivalence_test.html","id":null,"dir":"Reference","previous_headings":"","what":"Test for Practical Equivalence — equivalence_test","title":"Test for Practical Equivalence — equivalence_test","text":"Perform Test Practical Equivalence Bayesian frequentist models.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/equivalence_test.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test for Practical Equivalence — equivalence_test","text":"","code":"equivalence_test(x, ...)  # S3 method for default equivalence_test(x, ...)  # S3 method for data.frame equivalence_test(x, range = \"default\", ci = 0.95, verbose = TRUE, ...)  # S3 method for stanreg equivalence_test(   x,   range = \"default\",   ci = 0.95,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"location\", \"all\", \"conditional\", \"smooth_terms\", \"sigma\",     \"distributional\", \"auxiliary\"),   parameters = NULL,   verbose = TRUE,   ... )  # S3 method for brmsfit equivalence_test(   x,   range = \"default\",   ci = 0.95,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"conditional\", \"zi\", \"zero_inflated\", \"all\"),   parameters = NULL,   verbose = TRUE,   ... )"},{"path":"https://easystats.github.io/bayestestR/reference/equivalence_test.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test for Practical Equivalence — equivalence_test","text":"x Vector representing posterior distribution. Can also stanreg brmsfit model. ... Currently used. range ROPE's lower higher bounds. \"default\" depending number outcome variables vector list. models one response, range vector length two (e.g., c(-0.1, 0.1)). multivariate models, range list numeric vectors response variable. Vector names correspond name response variables. \"default\" input vector, range set c(-0.1, 0.1). \"default\" input Bayesian model, rope_range() used. ci Credible Interval (CI) probability, corresponding proportion HDI, use percentage ROPE. verbose Toggle warnings. effects results fixed effects, random effects returned? applies mixed models. May abbreviated. component results parameters, parameters conditional model zero-inflated part model returned? May abbreviated. applies brms-models. parameters Regular expression pattern describes parameters returned. Meta-parameters (like lp__ prior_) filtered default, parameters typically appear summary() returned. Use parameters select specific parameters output.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/equivalence_test.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test for Practical Equivalence — equivalence_test","text":"data frame following columns: Parameter model parameter(s), x model-object. x vector, column missing. CI probability HDI. ROPE_low, ROPE_high limits ROPE. values identical parameters. ROPE_Percentage proportion HDI lies inside ROPE. ROPE_Equivalence \"test result\", character. Either \"rejected\", \"accepted\" \"undecided\". HDI_low , HDI_high lower upper HDI limits parameters.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/equivalence_test.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Test for Practical Equivalence — equivalence_test","text":"Documentation accessible : Bayesian models Frequentist models Bayesian models, Test Practical Equivalence based \"HDI+ROPE decision rule\" (Kruschke, 2014, 2018) check whether parameter values accepted rejected explicitly formulated \"null hypothesis\" (.e., ROPE). words, checks percentage 89% HDI null region (ROPE). percentage sufficiently low, null hypothesis rejected. percentage sufficiently high, null hypothesis accepted. Using ROPE HDI, Kruschke (2018) suggests using percentage 95% (89%, considered stable) HDI falls within ROPE decision rule. HDI completely outside ROPE, \"null hypothesis\" parameter \"rejected\". ROPE completely covers HDI, .e., credible values parameter inside region practical equivalence, null hypothesis accepted. Else, ’s undecided whether accept reject null hypothesis. full ROPE used (.e., 100% HDI), null hypothesis rejected accepted percentage posterior within ROPE smaller 2.5% greater 97.5%. Desirable results low proportions inside ROPE  (closer zero better).  attention required finding suitable values ROPE limits (argument range). See 'Details' rope_range() information. Multicollinearity: Non-independent covariates  parameters show strong correlations, .e. covariates independent, joint parameter distributions may shift towards away ROPE. cases, test practical equivalence may inappropriate results. Collinearity invalidates ROPE hypothesis testing based univariate marginals, probabilities conditional independence. problematic results \"undecided\" parameters, may either move towards \"rejection\" away (Kruschke 2014, 340f). equivalence_test() performs simple check pairwise correlations parameters, can collinearity two variables, first step check assumptions hypothesis testing look different pair plots. even sophisticated check projection predictive variable selection (Piironen Vehtari 2017).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/equivalence_test.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Test for Practical Equivalence — equivalence_test","text":"print()-method digits-argument control amount digits output, plot()-method visualize results equivalence-test (models ).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/equivalence_test.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Test for Practical Equivalence — equivalence_test","text":"Kruschke, J. K. (2018). Rejecting accepting parameter values Bayesian estimation. Advances Methods Practices Psychological Science, 1(2), 270-280. doi:10.1177/2515245918771304 Kruschke, J. K. (2014). Bayesian data analysis: tutorial R, JAGS, Stan. Academic Press Piironen, J., & Vehtari, . (2017). Comparison Bayesian predictive methods model selection. Statistics Computing, 27(3), 711–735. doi:10.1007/s11222-016-9649-y","code":""},{"path":"https://easystats.github.io/bayestestR/reference/equivalence_test.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Test for Practical Equivalence — equivalence_test","text":"","code":"library(bayestestR)  equivalence_test(x = rnorm(1000, 0, 0.01), range = c(-0.1, 0.1)) #> # Test for Practical Equivalence #>  #>   ROPE: [-0.10 0.10] #>  #> H0       | inside ROPE |      95% HDI #> ------------------------------------- #> Accepted |    100.00 % | [-0.02 0.02] #>  #>  equivalence_test(x = rnorm(1000, 0, 1), range = c(-0.1, 0.1)) #> # Test for Practical Equivalence #>  #>   ROPE: [-0.10 0.10] #>  #> H0        | inside ROPE |      95% HDI #> -------------------------------------- #> Undecided |      8.11 % | [-2.00 1.97] #>  #>  equivalence_test(x = rnorm(1000, 1, 0.01), range = c(-0.1, 0.1)) #> # Test for Practical Equivalence #>  #>   ROPE: [-0.10 0.10] #>  #> H0       | inside ROPE |     95% HDI #> ------------------------------------ #> Rejected |      0.00 % | [0.98 1.02] #>  #>  equivalence_test(x = rnorm(1000, 1, 1), ci = c(.50, .99)) #> # Test for Practical Equivalence #>  #>   ROPE: [-0.10 0.10] #>  #> H0       | inside ROPE |      50% HDI #> ------------------------------------- #> Rejected |      0.00 % | [ 0.31 1.64] #>  #>  #> H0        | inside ROPE |      99% HDI #> -------------------------------------- #> Undecided |      5.05 % | [-1.58 3.65] #>  #>   # print more digits test <- equivalence_test(x = rnorm(1000, 1, 1), ci = c(.50, .99)) print(test, digits = 4) #> # Test for Practical Equivalence #>  #>   ROPE: [-0.1000 0.1000] #>  #> H0       | inside ROPE |          50% HDI #> ----------------------------------------- #> Rejected |    0.0000 % | [ 0.3115 1.7148] #>  #>  #> H0        | inside ROPE |          99% HDI #> ------------------------------------------ #> Undecided |    4.9495 % | [-1.7070 3.7015] #>  #>  # \\dontrun{ library(rstanarm) model <- rstanarm::stan_glm(mpg ~ wt + cyl, data = mtcars) #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 2.9e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.29 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.061177 seconds (Warm-up) #> Chain 1:                0.058571 seconds (Sampling) #> Chain 1:                0.119748 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 1.2e-05 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.062332 seconds (Warm-up) #> Chain 2:                0.053697 seconds (Sampling) #> Chain 2:                0.116029 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 1.5e-05 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.057669 seconds (Warm-up) #> Chain 3:                0.058256 seconds (Sampling) #> Chain 3:                0.115925 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 1.3e-05 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.059109 seconds (Warm-up) #> Chain 4:                0.055293 seconds (Sampling) #> Chain 4:                0.114402 seconds (Total) #> Chain 4:  equivalence_test(model) #> Possible multicollinearity between cyl and wt (r = 0.78). This might lead to inappropriate results. See 'Details' in '?equivalence_test'. #> # Test for Practical Equivalence #>  #>   ROPE: [-0.60 0.60] #>  #> Parameter   |       H0 | inside ROPE |       95% HDI #> ---------------------------------------------------- #> (Intercept) | Rejected |      0.00 % | [36.21 43.06] #> wt          | Rejected |      0.00 % | [-4.74 -1.62] #> cyl         | Rejected |      0.00 % | [-2.36 -0.70] #>  #>   # plot result test <- equivalence_test(model) #> Possible multicollinearity between cyl and wt (r = 0.78). This might lead to inappropriate results. See 'Details' in '?equivalence_test'. plot(test) #> Picking joint bandwidth of 0.0895 #> Warning: Removed 400 rows containing non-finite values (`stat_density_ridges()`).   library(emmeans) equivalence_test(emtrends(model, ~1, \"wt\")) #> # Test for Practical Equivalence #>  #>   ROPE: [-0.10 0.10] #>  #> Parameter |       H0 | inside ROPE |       95% HDI #> -------------------------------------------------- #> overall   | Rejected |      0.00 % | [-4.74 -1.62] #>  #>   library(brms) model <- brms::brm(mpg ~ wt + cyl, data = mtcars) #> Compiling Stan program... #> Start sampling #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 1.1e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.0332 seconds (Warm-up) #> Chain 1:                0.031747 seconds (Sampling) #> Chain 1:                0.064947 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 6e-06 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.029429 seconds (Warm-up) #> Chain 2:                0.030898 seconds (Sampling) #> Chain 2:                0.060327 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 6e-06 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.033648 seconds (Warm-up) #> Chain 3:                0.02817 seconds (Sampling) #> Chain 3:                0.061818 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 7e-06 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.032864 seconds (Warm-up) #> Chain 4:                0.035408 seconds (Sampling) #> Chain 4:                0.068272 seconds (Total) #> Chain 4:  equivalence_test(model) #> Possible multicollinearity between b_cyl and b_wt (r = 0.78). This might lead to inappropriate results. See 'Details' in '?equivalence_test'. #> # Test for Practical Equivalence #>  #>   ROPE: [-0.60 0.60] #>  #> Parameter |       H0 | inside ROPE |       95% HDI #> -------------------------------------------------- #> Intercept | Rejected |      0.00 % | [36.28 43.22] #> wt        | Rejected |      0.00 % | [-4.66 -1.62] #> cyl       | Rejected |      0.00 % | [-2.36 -0.69] #>  #>   library(BayesFactor) bf <- ttestBF(x = rnorm(100, 1, 1)) # equivalence_test(bf) # }"},{"path":"https://easystats.github.io/bayestestR/reference/estimate_density.html","id":null,"dir":"Reference","previous_headings":"","what":"Density Estimation — estimate_density","title":"Density Estimation — estimate_density","text":"function wrapper different methods density estimation. default, uses base R density default uses different smoothing bandwidth (\"SJ\") legacy default implemented base R density function (\"nrd0\"). However, Deng Wickham suggest method = \"KernSmooth\" fastest accurate.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/estimate_density.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Density Estimation — estimate_density","text":"","code":"estimate_density(x, ...)  # S3 method for data.frame estimate_density(   x,   method = \"kernel\",   precision = 2^10,   extend = FALSE,   extend_scale = 0.1,   bw = \"SJ\",   ci = NULL,   select = NULL,   at = NULL,   group_by = NULL,   ... )"},{"path":"https://easystats.github.io/bayestestR/reference/estimate_density.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Density Estimation — estimate_density","text":"x Vector representing posterior distribution, data frame vectors. Can also Bayesian model. bayestestR supports wide range models (see, example, methods(\"hdi\")) documented 'Usage' section, methods classes mostly resemble arguments .numeric .data.framemethods. ... Currently used. method Density estimation method. Can \"kernel\" (default), \"logspline\" \"KernSmooth\". precision Number points density data. See n parameter density. extend Extend range x axis factor extend_scale. extend_scale Ratio range extend x axis. value 0.1 means x axis extended 1/10 range data. bw See eponymous argument density. , default changed \"SJ\", recommended. ci confidence interval threshold. used method = \"kernel\". feature experimental, use caution. select Character vector column names. NULL (default), numeric variables selected. arguments datawizard::find_columns() (exclude) can also used. Optional character vector. NULL input data frame, density estimation performed group (subsets) indicated . See examples. group_by Deprecated favour .","code":""},{"path":"https://easystats.github.io/bayestestR/reference/estimate_density.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Density Estimation — estimate_density","text":"also plot()-method implemented see-package.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/estimate_density.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Density Estimation — estimate_density","text":"Deng, H., & Wickham, H. (2011). Density estimation R. Electronic publication.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/estimate_density.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Density Estimation — estimate_density","text":"","code":"library(bayestestR)  set.seed(1) x <- rnorm(250, mean = 1)  # Basic usage density_kernel <- estimate_density(x) # default method is \"kernel\"  hist(x, prob = TRUE) lines(density_kernel$x, density_kernel$y, col = \"black\", lwd = 2) lines(density_kernel$x, density_kernel$CI_low, col = \"gray\", lty = 2) lines(density_kernel$x, density_kernel$CI_high, col = \"gray\", lty = 2) legend(\"topright\",   legend = c(\"Estimate\", \"95% CI\"),   col = c(\"black\", \"gray\"), lwd = 2, lty = c(1, 2) )   # Other Methods density_logspline <- estimate_density(x, method = \"logspline\") density_KernSmooth <- estimate_density(x, method = \"KernSmooth\") density_mixture <- estimate_density(x, method = \"mixture\")   hist(x, prob = TRUE) lines(density_kernel$x, density_kernel$y, col = \"black\", lwd = 2) lines(density_logspline$x, density_logspline$y, col = \"red\", lwd = 2) lines(density_KernSmooth$x, density_KernSmooth$y, col = \"blue\", lwd = 2) lines(density_mixture$x, density_mixture$y, col = \"green\", lwd = 2)   # Extension density_extended <- estimate_density(x, extend = TRUE) density_default <- estimate_density(x, extend = FALSE)  hist(x, prob = TRUE) lines(density_extended$x, density_extended$y, col = \"red\", lwd = 3) lines(density_default$x, density_default$y, col = \"black\", lwd = 3)   # Multiple columns head(estimate_density(iris)) #>      Parameter        x          y #> 1 Sepal.Length 4.300000 0.09650527 #> 2 Sepal.Length 4.303519 0.09766644 #> 3 Sepal.Length 4.307038 0.09883222 #> 4 Sepal.Length 4.310557 0.10001063 #> 5 Sepal.Length 4.314076 0.10119336 #> 6 Sepal.Length 4.317595 0.10238484 head(estimate_density(iris, select = \"Sepal.Width\")) #>     Parameter        x          y #> 1 Sepal.Width 2.000000 0.04651972 #> 2 Sepal.Width 2.002346 0.04733324 #> 3 Sepal.Width 2.004692 0.04816144 #> 4 Sepal.Width 2.007038 0.04899919 #> 5 Sepal.Width 2.009384 0.04984690 #> 6 Sepal.Width 2.011730 0.05071174  # Grouped data head(estimate_density(iris, at = \"Species\")) #>      Parameter        x         y Species #> 1 Sepal.Length 4.300000 0.2355986  setosa #> 2 Sepal.Length 4.301466 0.2375879  setosa #> 3 Sepal.Length 4.302933 0.2395766  setosa #> 4 Sepal.Length 4.304399 0.2415640  setosa #> 5 Sepal.Length 4.305865 0.2435508  setosa #> 6 Sepal.Length 4.307331 0.2455353  setosa head(estimate_density(iris$Petal.Width, at = iris$Species)) #>           x        y  Group #> 1 0.1000000 9.011872 setosa #> 2 0.1004888 8.955399 setosa #> 3 0.1009775 8.792239 setosa #> 4 0.1014663 8.528268 setosa #> 5 0.1019550 8.172719 setosa #> 6 0.1024438 7.737660 setosa # \\dontrun{ # rstanarm models # ----------------------------------------------- library(rstanarm) model <- stan_glm(mpg ~ wt + gear, data = mtcars, chains = 2, iter = 200, refresh = 0) #> Warning: The largest R-hat is 1.07, indicating chains have not mixed. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#r-hat #> Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#bulk-ess #> Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#tail-ess head(estimate_density(model)) #>     Parameter        x           y #> 1 (Intercept) 24.19242 0.002029767 #> 2 (Intercept) 24.22067 0.002052337 #> 3 (Intercept) 24.24892 0.002074994 #> 4 (Intercept) 24.27717 0.002097736 #> 5 (Intercept) 24.30542 0.002120651 #> 6 (Intercept) 24.33367 0.002143652  library(emmeans) head(estimate_density(emtrends(model, ~1, \"wt\"))) #>   Parameter         x          y #> 1   overall -7.810281 0.01754139 #> 2   overall -7.806283 0.01763125 #> 3   overall -7.802285 0.01771950 #> 4   overall -7.798287 0.01780524 #> 5   overall -7.794289 0.01788964 #> 6   overall -7.790292 0.01797242  # brms models # ----------------------------------------------- library(brms) model <- brms::brm(mpg ~ wt + cyl, data = mtcars) #> Compiling Stan program... #> Start sampling #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 1.1e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.030444 seconds (Warm-up) #> Chain 1:                0.033866 seconds (Sampling) #> Chain 1:                0.06431 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 8e-06 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.031612 seconds (Warm-up) #> Chain 2:                0.029842 seconds (Sampling) #> Chain 2:                0.061454 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 8e-06 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.034144 seconds (Warm-up) #> Chain 3:                0.02945 seconds (Sampling) #> Chain 3:                0.063594 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 8e-06 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.032226 seconds (Warm-up) #> Chain 4:                0.029308 seconds (Sampling) #> Chain 4:                0.061534 seconds (Total) #> Chain 4:  estimate_density(model) #>        Parameter             x            y #> 1    b_Intercept 32.4844925708 0.0002809111 #> 2    b_Intercept 32.4975104536 0.0002810493 #> 3    b_Intercept 32.5105283364 0.0002808713 #> 4    b_Intercept 32.5235462191 0.0002803812 #> 5    b_Intercept 32.5365641019 0.0002795345 #> 6    b_Intercept 32.5495819847 0.0002783613 #> 7    b_Intercept 32.5625998675 0.0002769030 #> 8    b_Intercept 32.5756177503 0.0002751698 #> 9    b_Intercept 32.5886356331 0.0002731728 #> 10   b_Intercept 32.6016535159 0.0002709240 #> 11   b_Intercept 32.6146713987 0.0002683799 #> 12   b_Intercept 32.6276892815 0.0002656211 #> 13   b_Intercept 32.6407071643 0.0002626661 #> 14   b_Intercept 32.6537250471 0.0002595316 #> 15   b_Intercept 32.6667429299 0.0002562355 #> 16   b_Intercept 32.6797608127 0.0002527860 #> 17   b_Intercept 32.6927786955 0.0002492013 #> 18   b_Intercept 32.7057965783 0.0002455282 #> 19   b_Intercept 32.7188144611 0.0002417877 #> 20   b_Intercept 32.7318323439 0.0002380012 #> 21   b_Intercept 32.7448502267 0.0002341904 #> 22   b_Intercept 32.7578681095 0.0002303839 #> 23   b_Intercept 32.7708859923 0.0002266151 #> 24   b_Intercept 32.7839038751 0.0002229065 #> 25   b_Intercept 32.7969217579 0.0002192809 #> 26   b_Intercept 32.8099396407 0.0002157612 #> 27   b_Intercept 32.8229575235 0.0002123714 #> 28   b_Intercept 32.8359754063 0.0002091844 #> 29   b_Intercept 32.8489932891 0.0002061881 #> 30   b_Intercept 32.8620111719 0.0002034045 #> 31   b_Intercept 32.8750290546 0.0002008555 #> 32   b_Intercept 32.8880469374 0.0001985624 #> 33   b_Intercept 32.9010648202 0.0001965789 #> 34   b_Intercept 32.9140827030 0.0001949553 #> 35   b_Intercept 32.9271005858 0.0001936640 #> 36   b_Intercept 32.9401184686 0.0001927239 #> 37   b_Intercept 32.9531363514 0.0001921535 #> 38   b_Intercept 32.9661542342 0.0001919706 #> 39   b_Intercept 32.9791721170 0.0001922802 #> 40   b_Intercept 32.9921899998 0.0001930525 #> 41   b_Intercept 33.0052078826 0.0001942733 #> 42   b_Intercept 33.0182257654 0.0001959571 #> 43   b_Intercept 33.0312436482 0.0001981179 #> 44   b_Intercept 33.0442615310 0.0002007820 #> 45   b_Intercept 33.0572794138 0.0002040799 #> 46   b_Intercept 33.0702972966 0.0002079006 #> 47   b_Intercept 33.0833151794 0.0002122545 #> 48   b_Intercept 33.0963330622 0.0002171512 #> 49   b_Intercept 33.1093509450 0.0002226001 #> 50   b_Intercept 33.1223688278 0.0002286799 #> 51   b_Intercept 33.1353867106 0.0002354187 #> 52   b_Intercept 33.1484045934 0.0002427379 #> 53   b_Intercept 33.1614224762 0.0002506435 #> 54   b_Intercept 33.1744403590 0.0002591409 #> 55   b_Intercept 33.1874582418 0.0002682353 #> 56   b_Intercept 33.2004761246 0.0002780649 #> 57   b_Intercept 33.2134940074 0.0002885351 #> 58   b_Intercept 33.2265118901 0.0002996162 #> 59   b_Intercept 33.2395297729 0.0003113108 #> 60   b_Intercept 33.2525476557 0.0003236213 #> 61   b_Intercept 33.2655655385 0.0003365763 #> 62   b_Intercept 33.2785834213 0.0003502945 #> 63   b_Intercept 33.2916013041 0.0003646342 #> 64   b_Intercept 33.3046191869 0.0003795958 #> 65   b_Intercept 33.3176370697 0.0003951800 #> 66   b_Intercept 33.3306549525 0.0004113870 #> 67   b_Intercept 33.3436728353 0.0004283043 #> 68   b_Intercept 33.3566907181 0.0004459278 #> 69   b_Intercept 33.3697086009 0.0004641736 #> 70   b_Intercept 33.3827264837 0.0004830411 #> 71   b_Intercept 33.3957443665 0.0005025299 #> 72   b_Intercept 33.4087622493 0.0005226390 #> 73   b_Intercept 33.4217801321 0.0005435146 #> 74   b_Intercept 33.4347980149 0.0005650311 #> 75   b_Intercept 33.4478158977 0.0005871646 #> 76   b_Intercept 33.4608337805 0.0006099140 #> 77   b_Intercept 33.4738516633 0.0006332782 #> 78   b_Intercept 33.4868695461 0.0006572927 #> 79   b_Intercept 33.4998874289 0.0006820498 #> 80   b_Intercept 33.5129053117 0.0007074166 #> 81   b_Intercept 33.5259231945 0.0007333915 #> 82   b_Intercept 33.5389410773 0.0007599728 #> 83   b_Intercept 33.5519589601 0.0007871589 #> 84   b_Intercept 33.5649768428 0.0008150422 #> 85   b_Intercept 33.5779947256 0.0008435949 #> 86   b_Intercept 33.5910126084 0.0008727444 #> 87   b_Intercept 33.6040304912 0.0009024880 #> 88   b_Intercept 33.6170483740 0.0009328230 #> 89   b_Intercept 33.6300662568 0.0009637466 #> 90   b_Intercept 33.6430841396 0.0009954030 #> 91   b_Intercept 33.6561020224 0.0010276502 #> 92   b_Intercept 33.6691199052 0.0010604727 #> 93   b_Intercept 33.6821377880 0.0010938664 #> 94   b_Intercept 33.6951556708 0.0011278268 #> 95   b_Intercept 33.7081735536 0.0011623923 #> 96   b_Intercept 33.7211914364 0.0011976218 #> 97   b_Intercept 33.7342093192 0.0012333990 #> 98   b_Intercept 33.7472272020 0.0012697185 #> 99   b_Intercept 33.7602450848 0.0013065742 #> 100  b_Intercept 33.7732629676 0.0013439600 #> 101  b_Intercept 33.7862808504 0.0013819594 #> 102  b_Intercept 33.7992987332 0.0014205231 #> 103  b_Intercept 33.8123166160 0.0014595924 #> 104  b_Intercept 33.8253344988 0.0014991605 #> 105  b_Intercept 33.8383523816 0.0015392206 #> 106  b_Intercept 33.8513702644 0.0015797660 #> 107  b_Intercept 33.8643881472 0.0016209168 #> 108  b_Intercept 33.8774060300 0.0016625360 #> 109  b_Intercept 33.8904239128 0.0017046163 #> 110  b_Intercept 33.9034417956 0.0017471520 #> 111  b_Intercept 33.9164596783 0.0017901378 #> 112  b_Intercept 33.9294775611 0.0018336101 #> 113  b_Intercept 33.9424954439 0.0018776001 #> 114  b_Intercept 33.9555133267 0.0019220252 #> 115  b_Intercept 33.9685312095 0.0019668833 #> 116  b_Intercept 33.9815490923 0.0020121732 #> 117  b_Intercept 33.9945669751 0.0020578944 #> 118  b_Intercept 34.0075848579 0.0021041313 #> 119  b_Intercept 34.0206027407 0.0021508405 #> 120  b_Intercept 34.0336206235 0.0021979910 #> 121  b_Intercept 34.0466385063 0.0022455889 #> 122  b_Intercept 34.0596563891 0.0022936416 #> 123  b_Intercept 34.0726742719 0.0023421650 #> 124  b_Intercept 34.0856921547 0.0023912919 #> 125  b_Intercept 34.0987100375 0.0024409165 #> 126  b_Intercept 34.1117279203 0.0024910552 #> 127  b_Intercept 34.1247458031 0.0025417259 #> 128  b_Intercept 34.1377636859 0.0025929482 #> 129  b_Intercept 34.1507815687 0.0026448120 #> 130  b_Intercept 34.1637994515 0.0026973861 #> 131  b_Intercept 34.1768173343 0.0027506051 #> 132  b_Intercept 34.1898352171 0.0028044993 #> 133  b_Intercept 34.2028530999 0.0028591007 #> 134  b_Intercept 34.2158709827 0.0029144433 #> 135  b_Intercept 34.2288888655 0.0029707390 #> 136  b_Intercept 34.2419067483 0.0030279323 #> 137  b_Intercept 34.2549246311 0.0030860142 #> 138  b_Intercept 34.2679425138 0.0031450289 #> 139  b_Intercept 34.2809603966 0.0032050223 #> 140  b_Intercept 34.2939782794 0.0032660768 #> 141  b_Intercept 34.3069961622 0.0033284941 #> 142  b_Intercept 34.3200140450 0.0033920777 #> 143  b_Intercept 34.3330319278 0.0034568818 #> 144  b_Intercept 34.3460498106 0.0035229615 #> 145  b_Intercept 34.3590676934 0.0035903729 #> 146  b_Intercept 34.3720855762 0.0036593645 #> 147  b_Intercept 34.3851034590 0.0037300511 #> 148  b_Intercept 34.3981213418 0.0038022894 #> 149  b_Intercept 34.4111392246 0.0038761398 #> 150  b_Intercept 34.4241571074 0.0039516626 #> 151  b_Intercept 34.4371749902 0.0040289186 #> 152  b_Intercept 34.4501928730 0.0041083980 #> 153  b_Intercept 34.4632107558 0.0041898648 #> 154  b_Intercept 34.4762286386 0.0042732919 #> 155  b_Intercept 34.4892465214 0.0043587388 #> 156  b_Intercept 34.5022644042 0.0044462642 #> 157  b_Intercept 34.5152822870 0.0045360356 #> 158  b_Intercept 34.5283001698 0.0046285244 #> 159  b_Intercept 34.5413180526 0.0047233032 #> 160  b_Intercept 34.5543359354 0.0048204247 #> 161  b_Intercept 34.5673538182 0.0049199403 #> 162  b_Intercept 34.5803717010 0.0050218999 #> 163  b_Intercept 34.5933895838 0.0051267272 #> 164  b_Intercept 34.6064074665 0.0052344457 #> 165  b_Intercept 34.6194253493 0.0053447771 #> 166  b_Intercept 34.6324432321 0.0054577608 #> 167  b_Intercept 34.6454611149 0.0055734343 #> 168  b_Intercept 34.6584789977 0.0056918331 #> 169  b_Intercept 34.6714968805 0.0058136744 #> 170  b_Intercept 34.6845147633 0.0059384096 #> 171  b_Intercept 34.6975326461 0.0060659785 #> 172  b_Intercept 34.7105505289 0.0061964038 #> 173  b_Intercept 34.7235684117 0.0063297062 #> 174  b_Intercept 34.7365862945 0.0064660977 #> 175  b_Intercept 34.7496041773 0.0066060116 #> 176  b_Intercept 34.7626220601 0.0067488559 #> 177  b_Intercept 34.7756399429 0.0068946387 #> 178  b_Intercept 34.7886578257 0.0070433657 #> 179  b_Intercept 34.8016757085 0.0071950410 #> 180  b_Intercept 34.8146935913 0.0073501493 #> 181  b_Intercept 34.8277114741 0.0075085318 #> 182  b_Intercept 34.8407293569 0.0076698578 #> 183  b_Intercept 34.8537472397 0.0078341216 #> 184  b_Intercept 34.8667651225 0.0080013160 #> 185  b_Intercept 34.8797830053 0.0081714328 #> 186  b_Intercept 34.8928008881 0.0083452170 #> 187  b_Intercept 34.9058187709 0.0085219329 #> 188  b_Intercept 34.9188366537 0.0087015288 #> 189  b_Intercept 34.9318545365 0.0088839916 #> 190  b_Intercept 34.9448724193 0.0090693078 #> 191  b_Intercept 34.9578903020 0.0092576999 #> 192  b_Intercept 34.9709081848 0.0094494480 #> 193  b_Intercept 34.9839260676 0.0096439979 #> 194  b_Intercept 34.9969439504 0.0098413363 #> 195  b_Intercept 35.0099618332 0.0100414507 #> 196  b_Intercept 35.0229797160 0.0102443289 #> 197  b_Intercept 35.0359975988 0.0104504546 #> 198  b_Intercept 35.0490154816 0.0106595724 #> 199  b_Intercept 35.0620333644 0.0108714175 #> 200  b_Intercept 35.0750512472 0.0110859834 #> 201  b_Intercept 35.0880691300 0.0113032643 #> 202  b_Intercept 35.1010870128 0.0115232663 #> 203  b_Intercept 35.1141048956 0.0117467082 #> 204  b_Intercept 35.1271227784 0.0119728576 #> 205  b_Intercept 35.1401406612 0.0122017165 #> 206  b_Intercept 35.1531585440 0.0124332884 #> 207  b_Intercept 35.1661764268 0.0126675779 #> 208  b_Intercept 35.1791943096 0.0129048676 #> 209  b_Intercept 35.1922121924 0.0131453679 #> 210  b_Intercept 35.2052300752 0.0133886173 #> 211  b_Intercept 35.2182479580 0.0136346273 #> 212  b_Intercept 35.2312658408 0.0138834107 #> 213  b_Intercept 35.2442837236 0.0141349807 #> 214  b_Intercept 35.2573016064 0.0143899117 #> 215  b_Intercept 35.2703194892 0.0146478842 #> 216  b_Intercept 35.2833373720 0.0149087004 #> 217  b_Intercept 35.2963552548 0.0151723764 #> 218  b_Intercept 35.3093731375 0.0154389283 #> 219  b_Intercept 35.3223910203 0.0157084333 #> 220  b_Intercept 35.3354089031 0.0159815895 #> 221  b_Intercept 35.3484267859 0.0162576786 #> 222  b_Intercept 35.3614446687 0.0165367138 #> 223  b_Intercept 35.3744625515 0.0168187072 #> 224  b_Intercept 35.3874804343 0.0171036700 #> 225  b_Intercept 35.4004983171 0.0173919656 #> 226  b_Intercept 35.4135161999 0.0176837180 #> 227  b_Intercept 35.4265340827 0.0179784659 #> 228  b_Intercept 35.4395519655 0.0182762115 #> 229  b_Intercept 35.4525698483 0.0185769547 #> 230  b_Intercept 35.4655877311 0.0188806937 #> 231  b_Intercept 35.4786056139 0.0191880659 #> 232  b_Intercept 35.4916234967 0.0194985908 #> 233  b_Intercept 35.5046413795 0.0198120799 #> 234  b_Intercept 35.5176592623 0.0201285184 #> 235  b_Intercept 35.5306771451 0.0204478892 #> 236  b_Intercept 35.5436950279 0.0207702832 #> 237  b_Intercept 35.5567129107 0.0210962360 #> 238  b_Intercept 35.5697307935 0.0214250325 #> 239  b_Intercept 35.5827486763 0.0217566424 #> 240  b_Intercept 35.5957665591 0.0220910334 #> 241  b_Intercept 35.6087844419 0.0224281711 #> 242  b_Intercept 35.6218023247 0.0227683780 #> 243  b_Intercept 35.6348202075 0.0231116006 #> 244  b_Intercept 35.6478380902 0.0234574210 #> 245  b_Intercept 35.6608559730 0.0238057951 #> 246  b_Intercept 35.6738738558 0.0241566774 #> 247  b_Intercept 35.6868917386 0.0245100214 #> 248  b_Intercept 35.6999096214 0.0248663216 #> 249  b_Intercept 35.7129275042 0.0252250490 #> 250  b_Intercept 35.7259453870 0.0255860538 #> 251  b_Intercept 35.7389632698 0.0259492855 #> 252  b_Intercept 35.7519811526 0.0263146935 #> 253  b_Intercept 35.7649990354 0.0266823412 #> 254  b_Intercept 35.7780169182 0.0270524902 #> 255  b_Intercept 35.7910348010 0.0274246270 #> 256  b_Intercept 35.8040526838 0.0277987026 #> 257  b_Intercept 35.8170705666 0.0281746690 #> 258  b_Intercept 35.8300884494 0.0285524790 #> 259  b_Intercept 35.8431063322 0.0289323501 #> 260  b_Intercept 35.8561242150 0.0293141586 #> 261  b_Intercept 35.8691420978 0.0296976453 #> 262  b_Intercept 35.8821599806 0.0300827703 #> 263  b_Intercept 35.8951778634 0.0304694947 #> 264  b_Intercept 35.9081957462 0.0308577815 #> 265  b_Intercept 35.9212136290 0.0312479630 #> 266  b_Intercept 35.9342315118 0.0316396525 #> 267  b_Intercept 35.9472493946 0.0320327840 #> 268  b_Intercept 35.9602672774 0.0324273305 #> 269  b_Intercept 35.9732851602 0.0328232669 #> 270  b_Intercept 35.9863030430 0.0332206671 #> 271  b_Intercept 35.9993209257 0.0336196736 #> 272  b_Intercept 36.0123388085 0.0340199956 #> 273  b_Intercept 36.0253566913 0.0344216187 #> 274  b_Intercept 36.0383745741 0.0348245310 #> 275  b_Intercept 36.0513924569 0.0352287223 #> 276  b_Intercept 36.0644103397 0.0356343990 #> 277  b_Intercept 36.0774282225 0.0360414712 #> 278  b_Intercept 36.0904461053 0.0364498060 #> 279  b_Intercept 36.1034639881 0.0368594047 #> 280  b_Intercept 36.1164818709 0.0372702710 #> 281  b_Intercept 36.1294997537 0.0376824104 #> 282  b_Intercept 36.1425176365 0.0380961789 #> 283  b_Intercept 36.1555355193 0.0385112597 #> 284  b_Intercept 36.1685534021 0.0389276601 #> 285  b_Intercept 36.1815712849 0.0393453990 #> 286  b_Intercept 36.1945891677 0.0397644978 #> 287  b_Intercept 36.2076070505 0.0401851090 #> 288  b_Intercept 36.2206249333 0.0406074102 #> 289  b_Intercept 36.2336428161 0.0410311805 #> 290  b_Intercept 36.2466606989 0.0414564577 #> 291  b_Intercept 36.2596785817 0.0418832825 #> 292  b_Intercept 36.2726964645 0.0423116994 #> 293  b_Intercept 36.2857143473 0.0427420796 #> 294  b_Intercept 36.2987322301 0.0431743397 #> 295  b_Intercept 36.3117501129 0.0436083975 #> 296  b_Intercept 36.3247679957 0.0440443190 #> 297  b_Intercept 36.3377858785 0.0444821740 #> 298  b_Intercept 36.3508037612 0.0449220584 #> 299  b_Intercept 36.3638216440 0.0453646241 #> 300  b_Intercept 36.3768395268 0.0458094317 #> 301  b_Intercept 36.3898574096 0.0462565772 #> 302  b_Intercept 36.4028752924 0.0467061614 #> 303  b_Intercept 36.4158931752 0.0471582895 #> 304  b_Intercept 36.4289110580 0.0476133762 #> 305  b_Intercept 36.4419289408 0.0480717721 #> 306  b_Intercept 36.4549468236 0.0485331592 #> 307  b_Intercept 36.4679647064 0.0489976702 #> 308  b_Intercept 36.4809825892 0.0494654421 #> 309  b_Intercept 36.4940004720 0.0499366162 #> 310  b_Intercept 36.5070183548 0.0504121153 #> 311  b_Intercept 36.5200362376 0.0508916817 #> 312  b_Intercept 36.5330541204 0.0513752232 #> 313  b_Intercept 36.5460720032 0.0518629015 #> 314  b_Intercept 36.5590898860 0.0523548803 #> 315  b_Intercept 36.5721077688 0.0528514543 #> 316  b_Intercept 36.5851256516 0.0533538947 #> 317  b_Intercept 36.5981435344 0.0538612668 #> 318  b_Intercept 36.6111614172 0.0543737396 #> 319  b_Intercept 36.6241793000 0.0548914811 #> 320  b_Intercept 36.6371971828 0.0554146578 #> 321  b_Intercept 36.6502150656 0.0559441625 #> 322  b_Intercept 36.6632329484 0.0564803886 #> 323  b_Intercept 36.6762508312 0.0570226441 #> 324  b_Intercept 36.6892687139 0.0575710740 #> 325  b_Intercept 36.7022865967 0.0581258172 #> 326  b_Intercept 36.7153044795 0.0586870061 #> 327  b_Intercept 36.7283223623 0.0592562631 #> 328  b_Intercept 36.7413402451 0.0598326269 #> 329  b_Intercept 36.7543581279 0.0604158409 #> 330  b_Intercept 36.7673760107 0.0610059851 #> 331  b_Intercept 36.7803938935 0.0616031285 #> 332  b_Intercept 36.7934117763 0.0622076455 #> 333  b_Intercept 36.8064296591 0.0628209040 #> 334  b_Intercept 36.8194475419 0.0634412914 #> 335  b_Intercept 36.8324654247 0.0640688024 #> 336  b_Intercept 36.8454833075 0.0647034176 #> 337  b_Intercept 36.8585011903 0.0653451037 #> 338  b_Intercept 36.8715190731 0.0659947919 #> 339  b_Intercept 36.8845369559 0.0666522995 #> 340  b_Intercept 36.8975548387 0.0673165953 #> 341  b_Intercept 36.9105727215 0.0679875630 #> 342  b_Intercept 36.9235906043 0.0686650725 #> 343  b_Intercept 36.9366084871 0.0693489806 #> 344  b_Intercept 36.9496263699 0.0700405483 #> 345  b_Intercept 36.9626442527 0.0707382573 #> 346  b_Intercept 36.9756621355 0.0714416898 #> 347  b_Intercept 36.9886800183 0.0721506370 #> 348  b_Intercept 37.0016979011 0.0728648817 #> 349  b_Intercept 37.0147157839 0.0735844912 #> 350  b_Intercept 37.0277336667 0.0743098323 #> 351  b_Intercept 37.0407515494 0.0750395875 #> 352  b_Intercept 37.0537694322 0.0757735112 #> 353  b_Intercept 37.0667873150 0.0765113576 #> 354  b_Intercept 37.0798051978 0.0772528813 #> 355  b_Intercept 37.0928230806 0.0779983246 #> 356  b_Intercept 37.1058409634 0.0787471774 #> 357  b_Intercept 37.1188588462 0.0794988269 #> 358  b_Intercept 37.1318767290 0.0802530595 #> 359  b_Intercept 37.1448946118 0.0810096718 #> 360  b_Intercept 37.1579124946 0.0817684713 #> 361  b_Intercept 37.1709303774 0.0825297269 #> 362  b_Intercept 37.1839482602 0.0832927659 #> 363  b_Intercept 37.1969661430 0.0840574335 #> 364  b_Intercept 37.2099840258 0.0848236286 #> 365  b_Intercept 37.2230019086 0.0855912693 #> 366  b_Intercept 37.2360197914 0.0863603999 #> 367  b_Intercept 37.2490376742 0.0871311398 #> 368  b_Intercept 37.2620555570 0.0879032392 #> 369  b_Intercept 37.2750734398 0.0886767404 #> 370  b_Intercept 37.2880913226 0.0894517082 #> 371  b_Intercept 37.3011092054 0.0902282300 #> 372  b_Intercept 37.3141270882 0.0910067530 #> 373  b_Intercept 37.3271449710 0.0917873764 #> 374  b_Intercept 37.3401628538 0.0925701258 #> 375  b_Intercept 37.3531807366 0.0933552126 #> 376  b_Intercept 37.3661986194 0.0941428677 #> 377  b_Intercept 37.3792165022 0.0949333396 #> 378  b_Intercept 37.3922343849 0.0957278814 #> 379  b_Intercept 37.4052522677 0.0965260326 #> 380  b_Intercept 37.4182701505 0.0973281074 #> 381  b_Intercept 37.4312880333 0.0981344316 #> 382  b_Intercept 37.4443059161 0.0989453387 #> 383  b_Intercept 37.4573237989 0.0997616937 #> 384  b_Intercept 37.4703416817 0.1005844624 #> 385  b_Intercept 37.4833595645 0.1014131052 #> 386  b_Intercept 37.4963774473 0.1022479612 #> 387  b_Intercept 37.5093953301 0.1030893631 #> 388  b_Intercept 37.5224132129 0.1039376356 #> 389  b_Intercept 37.5354310957 0.1047945764 #> 390  b_Intercept 37.5484489785 0.1056597934 #> 391  b_Intercept 37.5614668613 0.1065329559 #> 392  b_Intercept 37.5744847441 0.1074142987 #> 393  b_Intercept 37.5875026269 0.1083040345 #> 394  b_Intercept 37.6005205097 0.1092024971 #> 395  b_Intercept 37.6135383925 0.1101120143 #> 396  b_Intercept 37.6265562753 0.1110304574 #> 397  b_Intercept 37.6395741581 0.1119578803 #> 398  b_Intercept 37.6525920409 0.1128943056 #> 399  b_Intercept 37.6656099237 0.1138397233 #> 400  b_Intercept 37.6786278065 0.1147950895 #> 401  b_Intercept 37.6916456893 0.1157606568 #> 402  b_Intercept 37.7046635721 0.1167348181 #> 403  b_Intercept 37.7176814549 0.1177173682 #> 404  b_Intercept 37.7306993376 0.1187080688 #> 405  b_Intercept 37.7437172204 0.1197066487 #> 406  b_Intercept 37.7567351032 0.1207142723 #> 407  b_Intercept 37.7697529860 0.1217293026 #> 408  b_Intercept 37.7827708688 0.1227508497 #> 409  b_Intercept 37.7957887516 0.1237784729 #> 410  b_Intercept 37.8088066344 0.1248117071 #> 411  b_Intercept 37.8218245172 0.1258502119 #> 412  b_Intercept 37.8348424000 0.1268941521 #> 413  b_Intercept 37.8478602828 0.1279417456 #> 414  b_Intercept 37.8608781656 0.1289924270 #> 415  b_Intercept 37.8738960484 0.1300456225 #> 416  b_Intercept 37.8869139312 0.1311007511 #> 417  b_Intercept 37.8999318140 0.1321572835 #> 418  b_Intercept 37.9129496968 0.1332143120 #> 419  b_Intercept 37.9259675796 0.1342710811 #> 420  b_Intercept 37.9389854624 0.1353270234 #> 421  b_Intercept 37.9520033452 0.1363815828 #> 422  b_Intercept 37.9650212280 0.1374342165 #> 423  b_Intercept 37.9780391108 0.1384836502 #> 424  b_Intercept 37.9910569936 0.1395296589 #> 425  b_Intercept 38.0040748764 0.1405719198 #> 426  b_Intercept 38.0170927592 0.1416100170 #> 427  b_Intercept 38.0301106420 0.1426435631 #> 428  b_Intercept 38.0431285248 0.1436719288 #> 429  b_Intercept 38.0561464076 0.1446937554 #> 430  b_Intercept 38.0691642904 0.1457099153 #> 431  b_Intercept 38.0821821731 0.1467202163 #> 432  b_Intercept 38.0952000559 0.1477245030 #> 433  b_Intercept 38.1082179387 0.1487226575 #> 434  b_Intercept 38.1212358215 0.1497136774 #> 435  b_Intercept 38.1342537043 0.1506976870 #> 436  b_Intercept 38.1472715871 0.1516755328 #> 437  b_Intercept 38.1602894699 0.1526473131 #> 438  b_Intercept 38.1733073527 0.1536131611 #> 439  b_Intercept 38.1863252355 0.1545732446 #> 440  b_Intercept 38.1993431183 0.1555265131 #> 441  b_Intercept 38.2123610011 0.1564745285 #> 442  b_Intercept 38.2253788839 0.1574177619 #> 443  b_Intercept 38.2383967667 0.1583565499 #> 444  b_Intercept 38.2514146495 0.1592912516 #> 445  b_Intercept 38.2644325323 0.1602220355 #> 446  b_Intercept 38.2774504151 0.1611490770 #> 447  b_Intercept 38.2904682979 0.1620735680 #> 448  b_Intercept 38.3034861807 0.1629959512 #> 449  b_Intercept 38.3165040635 0.1639166742 #> 450  b_Intercept 38.3295219463 0.1648361874 #> 451  b_Intercept 38.3425398291 0.1657549348 #> 452  b_Intercept 38.3555577119 0.1666736293 #> 453  b_Intercept 38.3685755947 0.1675927678 #> 454  b_Intercept 38.3815934775 0.1685127609 #> 455  b_Intercept 38.3946113603 0.1694340033 #> 456  b_Intercept 38.4076292431 0.1703568713 #> 457  b_Intercept 38.4206471259 0.1712823802 #> 458  b_Intercept 38.4336650086 0.1722104253 #> 459  b_Intercept 38.4466828914 0.1731412361 #> 460  b_Intercept 38.4597007742 0.1740750341 #> 461  b_Intercept 38.4727186570 0.1750120079 #> 462  b_Intercept 38.4857365398 0.1759526072 #> 463  b_Intercept 38.4987544226 0.1768973202 #> 464  b_Intercept 38.5117723054 0.1778455238 #> 465  b_Intercept 38.5247901882 0.1787971845 #> 466  b_Intercept 38.5378080710 0.1797522270 #> 467  b_Intercept 38.5508259538 0.1807105342 #> 468  b_Intercept 38.5638438366 0.1816724410 #> 469  b_Intercept 38.5768617194 0.1826373024 #> 470  b_Intercept 38.5898796022 0.1836045179 #> 471  b_Intercept 38.6028974850 0.1845737254 #> 472  b_Intercept 38.6159153678 0.1855445219 #> 473  b_Intercept 38.6289332506 0.1865164669 #> 474  b_Intercept 38.6419511334 0.1874889782 #> 475  b_Intercept 38.6549690162 0.1884611639 #> 476  b_Intercept 38.6679868990 0.1894324071 #> 477  b_Intercept 38.6810047818 0.1904020600 #> 478  b_Intercept 38.6940226646 0.1913694452 #> 479  b_Intercept 38.7070405474 0.1923334349 #> 480  b_Intercept 38.7200584302 0.1932926061 #> 481  b_Intercept 38.7330763130 0.1942466975 #> 482  b_Intercept 38.7460941958 0.1951949044 #> 483  b_Intercept 38.7591120786 0.1961364093 #> 484  b_Intercept 38.7721299613 0.1970703832 #> 485  b_Intercept 38.7851478441 0.1979940359 #> 486  b_Intercept 38.7981657269 0.1989071944 #> 487  b_Intercept 38.8111836097 0.1998096562 #> 488  b_Intercept 38.8242014925 0.2007005847 #> 489  b_Intercept 38.8372193753 0.2015791513 #> 490  b_Intercept 38.8502372581 0.2024442124 #> 491  b_Intercept 38.8632551409 0.2032913752 #> 492  b_Intercept 38.8762730237 0.2041232072 #> 493  b_Intercept 38.8892909065 0.2049389785 #> 494  b_Intercept 38.9023087893 0.2057379857 #> 495  b_Intercept 38.9153266721 0.2065195525 #> 496  b_Intercept 38.9283445549 0.2072807549 #> 497  b_Intercept 38.9413624377 0.2080200349 #> 498  b_Intercept 38.9543803205 0.2087396850 #> 499  b_Intercept 38.9673982033 0.2094392295 #> 500  b_Intercept 38.9804160861 0.2101182329 #> 501  b_Intercept 38.9934339689 0.2107763002 #> 502  b_Intercept 39.0064518517 0.2114083619 #> 503  b_Intercept 39.0194697345 0.2120174526 #> 504  b_Intercept 39.0324876173 0.2126046103 #> 505  b_Intercept 39.0455055001 0.2131696957 #> 506  b_Intercept 39.0585233829 0.2137126146 #> 507  b_Intercept 39.0715412657 0.2142324315 #> 508  b_Intercept 39.0845591485 0.2147248739 #> 509  b_Intercept 39.0975770313 0.2151952667 #> 510  b_Intercept 39.1105949141 0.2156437714 #> 511  b_Intercept 39.1236127968 0.2160705903 #> 512  b_Intercept 39.1366306796 0.2164759659 #> 513  b_Intercept 39.1496485624 0.2168573534 #> 514  b_Intercept 39.1626664452 0.2172153033 #> 515  b_Intercept 39.1756843280 0.2175531242 #> 516  b_Intercept 39.1887022108 0.2178712611 #> 517  b_Intercept 39.2017200936 0.2181701873 #> 518  b_Intercept 39.2147379764 0.2184504027 #> 519  b_Intercept 39.2277558592 0.2187083982 #> 520  b_Intercept 39.2407737420 0.2189484867 #> 521  b_Intercept 39.2537916248 0.2191719828 #> 522  b_Intercept 39.2668095076 0.2193794939 #> 523  b_Intercept 39.2798273904 0.2195716371 #> 524  b_Intercept 39.2928452732 0.2197482479 #> 525  b_Intercept 39.3058631560 0.2199080394 #> 526  b_Intercept 39.3188810388 0.2200548344 #> 527  b_Intercept 39.3318989216 0.2201892608 #> 528  b_Intercept 39.3449168044 0.2203119397 #> 529  b_Intercept 39.3579346872 0.2204234836 #> 530  b_Intercept 39.3709525700 0.2205230098 #> 531  b_Intercept 39.3839704528 0.2206117638 #> 532  b_Intercept 39.3969883356 0.2206915350 #> 533  b_Intercept 39.4100062184 0.2207628434 #> 534  b_Intercept 39.4230241012 0.2208261865 #> 535  b_Intercept 39.4360419840 0.2208820386 #> 536  b_Intercept 39.4490598668 0.2209292477 #> 537  b_Intercept 39.4620777496 0.2209699444 #> 538  b_Intercept 39.4750956323 0.2210046279 #> 539  b_Intercept 39.4881135151 0.2210336100 #> 540  b_Intercept 39.5011313979 0.2210571717 #> 541  b_Intercept 39.5141492807 0.2210751992 #> 542  b_Intercept 39.5271671635 0.2210873670 #> 543  b_Intercept 39.5401850463 0.2210948324 #> 544  b_Intercept 39.5532029291 0.2210976918 #> 545  b_Intercept 39.5662208119 0.2210960104 #> 546  b_Intercept 39.5792386947 0.2210898224 #> 547  b_Intercept 39.5922565775 0.2210783431 #> 548  b_Intercept 39.6052744603 0.2210618010 #> 549  b_Intercept 39.6182923431 0.2210405546 #> 550  b_Intercept 39.6313102259 0.2210144730 #> 551  b_Intercept 39.6443281087 0.2209834007 #> 552  b_Intercept 39.6573459915 0.2209471581 #> 553  b_Intercept 39.6703638743 0.2209039834 #> 554  b_Intercept 39.6833817571 0.2208549756 #> 555  b_Intercept 39.6963996399 0.2207998921 #> 556  b_Intercept 39.7094175227 0.2207384465 #> 557  b_Intercept 39.7224354055 0.2206703392 #> 558  b_Intercept 39.7354532883 0.2205945741 #> 559  b_Intercept 39.7484711711 0.2205099802 #> 560  b_Intercept 39.7614890539 0.2204174996 #> 561  b_Intercept 39.7745069367 0.2203167911 #> 562  b_Intercept 39.7875248195 0.2202075131 #> 563  b_Intercept 39.8005427023 0.2200893239 #> 564  b_Intercept 39.8135605850 0.2199600334 #> 565  b_Intercept 39.8265784678 0.2198201134 #> 566  b_Intercept 39.8395963506 0.2196700442 #> 567  b_Intercept 39.8526142334 0.2195095184 #> 568  b_Intercept 39.8656321162 0.2193382395 #> 569  b_Intercept 39.8786499990 0.2191557883 #> 570  b_Intercept 39.8916678818 0.2189589387 #> 571  b_Intercept 39.9046857646 0.2187503602 #> 572  b_Intercept 39.9177036474 0.2185298410 #> 573  b_Intercept 39.9307215302 0.2182971870 #> 574  b_Intercept 39.9437394130 0.2180522227 #> 575  b_Intercept 39.9567572958 0.2177934127 #> 576  b_Intercept 39.9697751786 0.2175198592 #> 577  b_Intercept 39.9827930614 0.2172335458 #> 578  b_Intercept 39.9958109442 0.2169344119 #> 579  b_Intercept 40.0088288270 0.2166224176 #> 580  b_Intercept 40.0218467098 0.2162975431 #> 581  b_Intercept 40.0348645926 0.2159571483 #> 582  b_Intercept 40.0478824754 0.2156030691 #> 583  b_Intercept 40.0609003582 0.2152362571 #> 584  b_Intercept 40.0739182410 0.2148568049 #> 585  b_Intercept 40.0869361238 0.2144648225 #> 586  b_Intercept 40.0999540066 0.2140600834 #> 587  b_Intercept 40.1129718894 0.2136401702 #> 588  b_Intercept 40.1259897722 0.2132083081 #> 589  b_Intercept 40.1390076550 0.2127646939 #> 590  b_Intercept 40.1520255378 0.2123095356 #> 591  b_Intercept 40.1650434205 0.2118430521 #> 592  b_Intercept 40.1780613033 0.2113641201 #> 593  b_Intercept 40.1910791861 0.2108728835 #> 594  b_Intercept 40.2040970689 0.2103712353 #> 595  b_Intercept 40.2171149517 0.2098594353 #> 596  b_Intercept 40.2301328345 0.2093377465 #> 597  b_Intercept 40.2431507173 0.2088064344 #> 598  b_Intercept 40.2561686001 0.2082637763 #> 599  b_Intercept 40.2691864829 0.2077117566 #> 600  b_Intercept 40.2822043657 0.2071511098 #> 601  b_Intercept 40.2952222485 0.2065820939 #> 602  b_Intercept 40.3082401313 0.2060049630 #> 603  b_Intercept 40.3212580141 0.2054196182 #> 604  b_Intercept 40.3342758969 0.2048250364 #> 605  b_Intercept 40.3472937797 0.2042232313 #> 606  b_Intercept 40.3603116625 0.2036144180 #> 607  b_Intercept 40.3733295453 0.2029988030 #> 608  b_Intercept 40.3863474281 0.2023765841 #> 609  b_Intercept 40.3993653109 0.2017470682 #> 610  b_Intercept 40.4123831937 0.2011106010 #> 611  b_Intercept 40.4254010765 0.2004681697 #> 612  b_Intercept 40.4384189593 0.1998199169 #> 613  b_Intercept 40.4514368421 0.1991659758 #> 614  b_Intercept 40.4644547249 0.1985064700 #> 615  b_Intercept 40.4774726077 0.1978402424 #> 616  b_Intercept 40.4904904905 0.1971685517 #> 617  b_Intercept 40.5035083733 0.1964916638 #> 618  b_Intercept 40.5165262560 0.1958096552 #> 619  b_Intercept 40.5295441388 0.1951225958 #> 620  b_Intercept 40.5425620216 0.1944302378 #> 621  b_Intercept 40.5555799044 0.1937319390 #> 622  b_Intercept 40.5685977872 0.1930287959 #> 623  b_Intercept 40.5816156700 0.1923208525 #> 624  b_Intercept 40.5946335528 0.1916081508 #> 625  b_Intercept 40.6076514356 0.1908907310 #> 626  b_Intercept 40.6206693184 0.1901678905 #> 627  b_Intercept 40.6336872012 0.1894399097 #> 628  b_Intercept 40.6467050840 0.1887073611 #> 629  b_Intercept 40.6597229668 0.1879702887 #> 630  b_Intercept 40.6727408496 0.1872287390 #> 631  b_Intercept 40.6857587324 0.1864827615 #> 632  b_Intercept 40.6987766152 0.1857313086 #> 633  b_Intercept 40.7117944980 0.1849755175 #> 634  b_Intercept 40.7248123808 0.1842155287 #> 635  b_Intercept 40.7378302636 0.1834514159 #> 636  b_Intercept 40.7508481464 0.1826832575 #> 637  b_Intercept 40.7638660292 0.1819108276 #> 638  b_Intercept 40.7768839120 0.1811338307 #> 639  b_Intercept 40.7899017948 0.1803531316 #> 640  b_Intercept 40.8029196776 0.1795688341 #> 641  b_Intercept 40.8159375604 0.1787810450 #> 642  b_Intercept 40.8289554432 0.1779898743 #> 643  b_Intercept 40.8419733260 0.1771948862 #> 644  b_Intercept 40.8549912087 0.1763965195 #> 645  b_Intercept 40.8680090915 0.1755952025 #> 646  b_Intercept 40.8810269743 0.1747910500 #> 647  b_Intercept 40.8940448571 0.1739841755 #> 648  b_Intercept 40.9070627399 0.1731746879 #> 649  b_Intercept 40.9200806227 0.1723620609 #> 650  b_Intercept 40.9330985055 0.1715471033 #> 651  b_Intercept 40.9461163883 0.1707299030 #> 652  b_Intercept 40.9591342711 0.1699105405 #> 653  b_Intercept 40.9721521539 0.1690890882 #> 654  b_Intercept 40.9851700367 0.1682654180 #> 655  b_Intercept 40.9981879195 0.1674394455 #> 656  b_Intercept 41.0112058023 0.1666115501 #> 657  b_Intercept 41.0242236851 0.1657817424 #> 658  b_Intercept 41.0372415679 0.1649500203 #> 659  b_Intercept 41.0502594507 0.1641163688 #> 660  b_Intercept 41.0632773335 0.1632803589 #> 661  b_Intercept 41.0762952163 0.1624421347 #> 662  b_Intercept 41.0893130991 0.1616017699 #> 663  b_Intercept 41.1023309819 0.1607591679 #> 664  b_Intercept 41.1153488647 0.1599142180 #> 665  b_Intercept 41.1283667475 0.1590667470 #> 666  b_Intercept 41.1413846303 0.1582159109 #> 667  b_Intercept 41.1544025131 0.1573621696 #> 668  b_Intercept 41.1674203959 0.1565053372 #> 669  b_Intercept 41.1804382787 0.1556452171 #> 670  b_Intercept 41.1934561615 0.1547816029 #> 671  b_Intercept 41.2064740442 0.1539138100 #> 672  b_Intercept 41.2194919270 0.1530413065 #> 673  b_Intercept 41.2325098098 0.1521644470 #> 674  b_Intercept 41.2455276926 0.1512829866 #> 675  b_Intercept 41.2585455754 0.1503966786 #> 676  b_Intercept 41.2715634582 0.1495052746 #> 677  b_Intercept 41.2845813410 0.1486073038 #> 678  b_Intercept 41.2975992238 0.1477032257 #> 679  b_Intercept 41.3106171066 0.1467931405 #> 680  b_Intercept 41.3236349894 0.1458768212 #> 681  b_Intercept 41.3366528722 0.1449540492 #> 682  b_Intercept 41.3496707550 0.1440243701 #> 683  b_Intercept 41.3626886378 0.1430861016 #> 684  b_Intercept 41.3757065206 0.1421406778 #> 685  b_Intercept 41.3887244034 0.1411879552 #> 686  b_Intercept 41.4017422862 0.1402278079 #> 687  b_Intercept 41.4147601690 0.1392601279 #> 688  b_Intercept 41.4277780518 0.1382838057 #> 689  b_Intercept 41.4407959346 0.1372986914 #> 690  b_Intercept 41.4538138174 0.1363058588 #> 691  b_Intercept 41.4668317002 0.1353053224 #> 692  b_Intercept 41.4798495830 0.1342971199 #> 693  b_Intercept 41.4928674658 0.1332813124 #> 694  b_Intercept 41.5058853486 0.1322563102 #> 695  b_Intercept 41.5189032314 0.1312236917 #> 696  b_Intercept 41.5319211142 0.1301839662 #> 697  b_Intercept 41.5449389969 0.1291373329 #> 698  b_Intercept 41.5579568797 0.1280840138 #> 699  b_Intercept 41.5709747625 0.1270239366 #> 700  b_Intercept 41.5839926453 0.1259564904 #> 701  b_Intercept 41.5970105281 0.1248834243 #> 702  b_Intercept 41.6100284109 0.1238050823 #> 703  b_Intercept 41.6230462937 0.1227218264 #> 704  b_Intercept 41.6360641765 0.1216340355 #> 705  b_Intercept 41.6490820593 0.1205415858 #> 706  b_Intercept 41.6620999421 0.1194452348 #> 707  b_Intercept 41.6751178249 0.1183459328 #> 708  b_Intercept 41.6881357077 0.1172441351 #> 709  b_Intercept 41.7011535905 0.1161403055 #> 710  b_Intercept 41.7141714733 0.1150349154 #> 711  b_Intercept 41.7271893561 0.1139283826 #> 712  b_Intercept 41.7402072389 0.1128215967 #> 713  b_Intercept 41.7532251217 0.1117150713 #> 714  b_Intercept 41.7662430045 0.1106092946 #> 715  b_Intercept 41.7792608873 0.1095047528 #> 716  b_Intercept 41.7922787701 0.1084020993 #> 717  b_Intercept 41.8052966529 0.1073023620 #> 718  b_Intercept 41.8183145357 0.1062056433 #> 719  b_Intercept 41.8313324185 0.1051123977 #> 720  b_Intercept 41.8443503013 0.1040230706 #> 721  b_Intercept 41.8573681841 0.1029380976 #> 722  b_Intercept 41.8703860669 0.1018588097 #> 723  b_Intercept 41.8834039497 0.1007855292 #> 724  b_Intercept 41.8964218324 0.0997181109 #> 725  b_Intercept 41.9094397152 0.0986569187 #> 726  b_Intercept 41.9224575980 0.0976023020 #> 727  b_Intercept 41.9354754808 0.0965545962 #> 728  b_Intercept 41.9484933636 0.0955161640 #> 729  b_Intercept 41.9615112464 0.0944855416 #> 730  b_Intercept 41.9745291292 0.0934629168 #> 731  b_Intercept 41.9875470120 0.0924485365 #> 732  b_Intercept 42.0005648948 0.0914426321 #> 733  b_Intercept 42.0135827776 0.0904462014 #> 734  b_Intercept 42.0266006604 0.0894604274 #> 735  b_Intercept 42.0396185432 0.0884838381 #> 736  b_Intercept 42.0526364260 0.0875165797 #> 737  b_Intercept 42.0656543088 0.0865587846 #> 738  b_Intercept 42.0786721916 0.0856105719 #> 739  b_Intercept 42.0916900744 0.0846738567 #> 740  b_Intercept 42.1047079572 0.0837478468 #> 741  b_Intercept 42.1177258400 0.0828317280 #> 742  b_Intercept 42.1307437228 0.0819255503 #> 743  b_Intercept 42.1437616056 0.0810293531 #> 744  b_Intercept 42.1567794884 0.0801432372 #> 745  b_Intercept 42.1697973712 0.0792698309 #> 746  b_Intercept 42.1828152540 0.0784064552 #> 747  b_Intercept 42.1958331368 0.0775530983 #> 748  b_Intercept 42.2088510196 0.0767097413 #> 749  b_Intercept 42.2218689024 0.0758763582 #> 750  b_Intercept 42.2348867852 0.0750539465 #> 751  b_Intercept 42.2479046679 0.0742430890 #> 752  b_Intercept 42.2609225507 0.0734420428 #> 753  b_Intercept 42.2739404335 0.0726507494 #> 754  b_Intercept 42.2869583163 0.0718691457 #> 755  b_Intercept 42.2999761991 0.0710971645 #> 756  b_Intercept 42.3129940819 0.0703366325 #> 757  b_Intercept 42.3260119647 0.0695862071 #> 758  b_Intercept 42.3390298475 0.0688451101 #> 759  b_Intercept 42.3520477303 0.0681132541 #> 760  b_Intercept 42.3650656131 0.0673905486 #> 761  b_Intercept 42.3780834959 0.0666771182 #> 762  b_Intercept 42.3911013787 0.0659748376 #> 763  b_Intercept 42.4041192615 0.0652813414 #> 764  b_Intercept 42.4171371443 0.0645965251 #> 765  b_Intercept 42.4301550271 0.0639202820 #> 766  b_Intercept 42.4431729099 0.0632525032 #> 767  b_Intercept 42.4561907927 0.0625940719 #> 768  b_Intercept 42.4692086755 0.0619450660 #> 769  b_Intercept 42.4822265583 0.0613040945 #> 770  b_Intercept 42.4952444411 0.0606710375 #> 771  b_Intercept 42.5082623239 0.0600457732 #> 772  b_Intercept 42.5212802067 0.0594281784 #> 773  b_Intercept 42.5342980895 0.0588197310 #> 774  b_Intercept 42.5473159723 0.0582190197 #> 775  b_Intercept 42.5603338551 0.0576254966 #> 776  b_Intercept 42.5733517379 0.0570390295 #> 777  b_Intercept 42.5863696206 0.0564594848 #> 778  b_Intercept 42.5993875034 0.0558870019 #> 779  b_Intercept 42.6124053862 0.0553226440 #> 780  b_Intercept 42.6254232690 0.0547646946 #> 781  b_Intercept 42.6384411518 0.0542130141 #> 782  b_Intercept 42.6514590346 0.0536674620 #> 783  b_Intercept 42.6644769174 0.0531278976 #> 784  b_Intercept 42.6774948002 0.0525949594 #> 785  b_Intercept 42.6905126830 0.0520684059 #> 786  b_Intercept 42.7035305658 0.0515473062 #> 787  b_Intercept 42.7165484486 0.0510315179 #> 788  b_Intercept 42.7295663314 0.0505208989 #> 789  b_Intercept 42.7425842142 0.0500153071 #> 790  b_Intercept 42.7556020970 0.0495156979 #> 791  b_Intercept 42.7686199798 0.0490209139 #> 792  b_Intercept 42.7816378626 0.0485306311 #> 793  b_Intercept 42.7946557454 0.0480447120 #> 794  b_Intercept 42.8076736282 0.0475630201 #> 795  b_Intercept 42.8206915110 0.0470856498 #> 796  b_Intercept 42.8337093938 0.0466130180 #> 797  b_Intercept 42.8467272766 0.0461441162 #> 798  b_Intercept 42.8597451594 0.0456788169 #> 799  b_Intercept 42.8727630422 0.0452169947 #> 800  b_Intercept 42.8857809250 0.0447585260 #> 801  b_Intercept 42.8987988078 0.0443037646 #> 802  b_Intercept 42.9118166906 0.0438524085 #> 803  b_Intercept 42.9248345734 0.0434039656 #> 804  b_Intercept 42.9378524561 0.0429583254 #> 805  b_Intercept 42.9508703389 0.0425153797 #> 806  b_Intercept 42.9638882217 0.0420750228 #> 807  b_Intercept 42.9769061045 0.0416377383 #> 808  b_Intercept 42.9899239873 0.0412028196 #> 809  b_Intercept 43.0029418701 0.0407701216 #> 810  b_Intercept 43.0159597529 0.0403395535 #> 811  b_Intercept 43.0289776357 0.0399110271 #> 812  b_Intercept 43.0419955185 0.0394845964 #> 813  b_Intercept 43.0550134013 0.0390603654 #> 814  b_Intercept 43.0680312841 0.0386378723 #> 815  b_Intercept 43.0810491669 0.0382170438 #> 816  b_Intercept 43.0940670497 0.0377978090 #> 817  b_Intercept 43.1070849325 0.0373800996 #> 818  b_Intercept 43.1201028153 0.0369640856 #> 819  b_Intercept 43.1331206981 0.0365495700 #> 820  b_Intercept 43.1461385809 0.0361363502 #> 821  b_Intercept 43.1591564637 0.0357243717 #> 822  b_Intercept 43.1721743465 0.0353135826 #> 823  b_Intercept 43.1851922293 0.0349039332 #> 824  b_Intercept 43.1982101121 0.0344956513 #> 825  b_Intercept 43.2112279949 0.0340883913 #> 826  b_Intercept 43.2242458777 0.0336821101 #> 827  b_Intercept 43.2372637605 0.0332767714 #> 828  b_Intercept 43.2502816433 0.0328723408 #> 829  b_Intercept 43.2632995261 0.0324688642 #> 830  b_Intercept 43.2763174089 0.0320663728 #> 831  b_Intercept 43.2893352916 0.0316646857 #> 832  b_Intercept 43.3023531744 0.0312637823 #> 833  b_Intercept 43.3153710572 0.0308636443 #> 834  b_Intercept 43.3283889400 0.0304642562 #> 835  b_Intercept 43.3414068228 0.0300657430 #> 836  b_Intercept 43.3544247056 0.0296680150 #> 837  b_Intercept 43.3674425884 0.0292710081 #> 838  b_Intercept 43.3804604712 0.0288747230 #> 839  b_Intercept 43.3934783540 0.0284791632 #> 840  b_Intercept 43.4064962368 0.0280843456 #> 841  b_Intercept 43.4195141196 0.0276904726 #> 842  b_Intercept 43.4325320024 0.0272973729 #> 843  b_Intercept 43.4455498852 0.0269050700 #> 844  b_Intercept 43.4585677680 0.0265135913 #> 845  b_Intercept 43.4715856508 0.0261229680 #> 846  b_Intercept 43.4846035336 0.0257333409 #> 847  b_Intercept 43.4976214164 0.0253448265 #> 848  b_Intercept 43.5106392992 0.0249573291 #> 849  b_Intercept 43.5236571820 0.0245709045 #> 850  b_Intercept 43.5366750648 0.0241856131 #> 851  b_Intercept 43.5496929476 0.0238015197 #> 852  b_Intercept 43.5627108304 0.0234189843 #> 853  b_Intercept 43.5757287132 0.0230379411 #> 854  b_Intercept 43.5887465960 0.0226583916 #> 855  b_Intercept 43.6017644788 0.0222804276 #> 856  b_Intercept 43.6147823616 0.0219041459 #> 857  b_Intercept 43.6278002443 0.0215297070 #> 858  b_Intercept 43.6408181271 0.0211576781 #> 859  b_Intercept 43.6538360099 0.0207877415 #> 860  b_Intercept 43.6668538927 0.0204200190 #> 861  b_Intercept 43.6798717755 0.0200546366 #> 862  b_Intercept 43.6928896583 0.0196917238 #> 863  b_Intercept 43.7059075411 0.0193317738 #> 864  b_Intercept 43.7189254239 0.0189750553 #> 865  b_Intercept 43.7319433067 0.0186213275 #> 866  b_Intercept 43.7449611895 0.0182707367 #> 867  b_Intercept 43.7579790723 0.0179234307 #> 868  b_Intercept 43.7709969551 0.0175795592 #> 869  b_Intercept 43.7840148379 0.0172401373 #> 870  b_Intercept 43.7970327207 0.0169047462 #> 871  b_Intercept 43.8100506035 0.0165733583 #> 872  b_Intercept 43.8230684863 0.0162461238 #> 873  b_Intercept 43.8360863691 0.0159231918 #> 874  b_Intercept 43.8491042519 0.0156049328 #> 875  b_Intercept 43.8621221347 0.0152923849 #> 876  b_Intercept 43.8751400175 0.0149846751 #> 877  b_Intercept 43.8881579003 0.0146819359 #> 878  b_Intercept 43.9011757831 0.0143842950 #> 879  b_Intercept 43.9141936659 0.0140918753 #> 880  b_Intercept 43.9272115487 0.0138055936 #> 881  b_Intercept 43.9402294315 0.0135255298 #> 882  b_Intercept 43.9532473143 0.0132510845 #> 883  b_Intercept 43.9662651971 0.0129823437 #> 884  b_Intercept 43.9792830798 0.0127193853 #> 885  b_Intercept 43.9923009626 0.0124622795 #> 886  b_Intercept 44.0053188454 0.0122125387 #> 887  b_Intercept 44.0183367282 0.0119689776 #> 888  b_Intercept 44.0313546110 0.0117314336 #> 889  b_Intercept 44.0443724938 0.0114999245 #> 890  b_Intercept 44.0573903766 0.0112744585 #> 891  b_Intercept 44.0704082594 0.0110554261 #> 892  b_Intercept 44.0834261422 0.0108436657 #> 893  b_Intercept 44.0964440250 0.0106378749 #> 894  b_Intercept 44.1094619078 0.0104380047 #> 895  b_Intercept 44.1224797906 0.0102439966 #> 896  b_Intercept 44.1354976734 0.0100557823 #> 897  b_Intercept 44.1485155562 0.0098741872 #> 898  b_Intercept 44.1615334390 0.0096987781 #> 899  b_Intercept 44.1745513218 0.0095288058 #> 900  b_Intercept 44.1875692046 0.0093641506 #> 901  b_Intercept 44.2005870874 0.0092046852 #> 902  b_Intercept 44.2136049702 0.0090502747 #> 903  b_Intercept 44.2266228530 0.0089019815 #> 904  b_Intercept 44.2396407358 0.0087583991 #> 905  b_Intercept 44.2526586186 0.0086192908 #> 906  b_Intercept 44.2656765014 0.0084844874 #> 907  b_Intercept 44.2786943842 0.0083538150 #> 908  b_Intercept 44.2917122670 0.0082273992 #> 909  b_Intercept 44.3047301498 0.0081053700 #> 910  b_Intercept 44.3177480326 0.0079867855 #> 911  b_Intercept 44.3307659153 0.0078714577 #> 912  b_Intercept 44.3437837981 0.0077591979 #> 913  b_Intercept 44.3568016809 0.0076498175 #> 914  b_Intercept 44.3698195637 0.0075435569 #> 915  b_Intercept 44.3828374465 0.0074399116 #> 916  b_Intercept 44.3958553293 0.0073384492 #> 917  b_Intercept 44.4088732121 0.0072389912 #> 918  b_Intercept 44.4218910949 0.0071413623 #> 919  b_Intercept 44.4349089777 0.0070453945 #> 920  b_Intercept 44.4479268605 0.0069512460 #> 921  b_Intercept 44.4609447433 0.0068583127 #> 922  b_Intercept 44.4739626261 0.0067664469 #> 923  b_Intercept 44.4869805089 0.0066755071 #> 924  b_Intercept 44.4999983917 0.0065853576 #> 925  b_Intercept 44.5130162745 0.0064959176 #> 926  b_Intercept 44.5260341573 0.0064070461 #> 927  b_Intercept 44.5390520401 0.0063185276 #> 928  b_Intercept 44.5520699229 0.0062302664 #> 929  b_Intercept 44.5650878057 0.0061421740 #> 930  b_Intercept 44.5781056885 0.0060541692 #> 931  b_Intercept 44.5911235713 0.0059661618 #> 932  b_Intercept 44.6041414541 0.0058780646 #> 933  b_Intercept 44.6171593369 0.0057898314 #> 934  b_Intercept 44.6301772197 0.0057014231 #> 935  b_Intercept 44.6431951025 0.0056128075 #> 936  b_Intercept 44.6562129853 0.0055239540 #> 937  b_Intercept 44.6692308680 0.0054347824 #> 938  b_Intercept 44.6822487508 0.0053453509 #> 939  b_Intercept 44.6952666336 0.0052556638 #> 940  b_Intercept 44.7082845164 0.0051657309 #> 941  b_Intercept 44.7213023992 0.0050755675 #> 942  b_Intercept 44.7343202820 0.0049851743 #> 943  b_Intercept 44.7473381648 0.0048945889 #> 944  b_Intercept 44.7603560476 0.0048038804 #> 945  b_Intercept 44.7733739304 0.0047130890 #> 946  b_Intercept 44.7863918132 0.0046222582 #> 947  b_Intercept 44.7994096960 0.0045314344 #> 948  b_Intercept 44.8124275788 0.0044406984 #> 949  b_Intercept 44.8254454616 0.0043501177 #> 950  b_Intercept 44.8384633444 0.0042597447 #> 951  b_Intercept 44.8514812272 0.0041696373 #> 952  b_Intercept 44.8644991100 0.0040798545 #> 953  b_Intercept 44.8775169928 0.0039904733 #> 954  b_Intercept 44.8905348756 0.0039016743 #> 955  b_Intercept 44.9035527584 0.0038134258 #> 956  b_Intercept 44.9165706412 0.0037257875 #> 957  b_Intercept 44.9295885240 0.0036388185 #> 958  b_Intercept 44.9426064068 0.0035525768 #> 959  b_Intercept 44.9556242896 0.0034672364 #> 960  b_Intercept 44.9686421724 0.0033828881 #> 961  b_Intercept 44.9816600552 0.0032994722 #> 962  b_Intercept 44.9946779380 0.0032170387 #> 963  b_Intercept 45.0076958208 0.0031356355 #> 964  b_Intercept 45.0207137035 0.0030553087 #> 965  b_Intercept 45.0337315863 0.0029763769 #> 966  b_Intercept 45.0467494691 0.0028986834 #> 967  b_Intercept 45.0597673519 0.0028222147 #> 968  b_Intercept 45.0727852347 0.0027470041 #> 969  b_Intercept 45.0858031175 0.0026730821 #> 970  b_Intercept 45.0988210003 0.0026005503 #> 971  b_Intercept 45.1118388831 0.0025296681 #> 972  b_Intercept 45.1248567659 0.0024601643 #> 973  b_Intercept 45.1378746487 0.0023920555 #> 974  b_Intercept 45.1508925315 0.0023253563 #> 975  b_Intercept 45.1639104143 0.0022600786 #> 976  b_Intercept 45.1769282971 0.0021964503 #> 977  b_Intercept 45.1899461799 0.0021344391 #> 978  b_Intercept 45.2029640627 0.0020738706 #> 979  b_Intercept 45.2159819455 0.0020147443 #> 980  b_Intercept 45.2289998283 0.0019570578 #> 981  b_Intercept 45.2420177111 0.0019008070 #> 982  b_Intercept 45.2550355939 0.0018463368 #> 983  b_Intercept 45.2680534767 0.0017933156 #> 984  b_Intercept 45.2810713595 0.0017416943 #> 985  b_Intercept 45.2940892423 0.0016914591 #> 986  b_Intercept 45.3071071251 0.0016425948 #> 987  b_Intercept 45.3201250079 0.0015951796 #> 988  b_Intercept 45.3331428907 0.0015493599 #> 989  b_Intercept 45.3461607735 0.0015048405 #> 990  b_Intercept 45.3591786563 0.0014615994 #> 991  b_Intercept 45.3721965390 0.0014196136 #> 992  b_Intercept 45.3852144218 0.0013788595 #> 993  b_Intercept 45.3982323046 0.0013395089 #> 994  b_Intercept 45.4112501874 0.0013014468 #> 995  b_Intercept 45.4242680702 0.0012645193 #> 996  b_Intercept 45.4372859530 0.0012286990 #> 997  b_Intercept 45.4503038358 0.0011939580 #> 998  b_Intercept 45.4633217186 0.0011602682 #> 999  b_Intercept 45.4763396014 0.0011278594 #> 1000 b_Intercept 45.4893574842 0.0010964309 #> 1001 b_Intercept 45.5023753670 0.0010659449 #> 1002 b_Intercept 45.5153932498 0.0010363721 #> 1003 b_Intercept 45.5284111326 0.0010076835 #> 1004 b_Intercept 45.5414290154 0.0009799216 #> 1005 b_Intercept 45.5544468982 0.0009531216 #> 1006 b_Intercept 45.5674647810 0.0009270970 #> 1007 b_Intercept 45.5804826638 0.0009018192 #> 1008 b_Intercept 45.5935005466 0.0008772604 #> 1009 b_Intercept 45.6065184294 0.0008533927 #> 1010 b_Intercept 45.6195363122 0.0008303042 #> 1011 b_Intercept 45.6325541950 0.0008078912 #> 1012 b_Intercept 45.6455720778 0.0007860698 #> 1013 b_Intercept 45.6585899606 0.0007648152 #> 1014 b_Intercept 45.6716078434 0.0007441029 #> 1015 b_Intercept 45.6846257262 0.0007239132 #> 1016 b_Intercept 45.6976436090 0.0007043395 #> 1017 b_Intercept 45.7106614917 0.0006852228 #> 1018 b_Intercept 45.7236793745 0.0006665425 #> 1019 b_Intercept 45.7366972573 0.0006482787 #> 1020 b_Intercept 45.7497151401 0.0006304125 #> 1021 b_Intercept 45.7627330229 0.0006129627 #> 1022 b_Intercept 45.7757509057 0.0005959259 #> 1023 b_Intercept 45.7887687885 0.0005792233 #> 1024 b_Intercept 45.8017866713 0.0005628405 #> 1025        b_wt -6.6622536685 0.0019320947 #> 1026        b_wt -6.6557470324 0.0019421133 #> 1027        b_wt -6.6492403963 0.0019493385 #> 1028        b_wt -6.6427337602 0.0019537788 #> 1029        b_wt -6.6362271241 0.0019554547 #> 1030        b_wt -6.6297204880 0.0019543994 #> 1031        b_wt -6.6232138519 0.0019505607 #> 1032        b_wt -6.6167072157 0.0019436180 #> 1033        b_wt -6.6102005796 0.0019341753 #> 1034        b_wt -6.6036939435 0.0019223274 #> 1035        b_wt -6.5971873074 0.0019081791 #> 1036        b_wt -6.5906806713 0.0018918449 #> 1037        b_wt -6.5841740352 0.0018734487 #> 1038        b_wt -6.5776673991 0.0018527907 #> 1039        b_wt -6.5711607630 0.0018303661 #> 1040        b_wt -6.5646541269 0.0018063985 #> 1041        b_wt -6.5581474908 0.0017810465 #> 1042        b_wt -6.5516408547 0.0017544729 #> 1043        b_wt -6.5451342186 0.0017268435 #> 1044        b_wt -6.5386275825 0.0016982496 #> 1045        b_wt -6.5321209464 0.0016689448 #> 1046        b_wt -6.5256143102 0.0016391939 #> 1047        b_wt -6.5191076741 0.0016091640 #> 1048        b_wt -6.5126010380 0.0015790198 #> 1049        b_wt -6.5060944019 0.0015489227 #> 1050        b_wt -6.4995877658 0.0015190492 #> 1051        b_wt -6.4930811297 0.0014896602 #> 1052        b_wt -6.4865744936 0.0014608556 #> 1053        b_wt -6.4800678575 0.0014327678 #> 1054        b_wt -6.4735612214 0.0014055217 #> 1055        b_wt -6.4670545853 0.0013792345 #> 1056        b_wt -6.4605479492 0.0013540153 #> 1057        b_wt -6.4540413131 0.0013302296 #> 1058        b_wt -6.4475346770 0.0013077817 #> 1059        b_wt -6.4410280409 0.0012867094 #> 1060        b_wt -6.4345214048 0.0012670751 #> 1061        b_wt -6.4280147686 0.0012489319 #> 1062        b_wt -6.4215081325 0.0012323239 #> 1063        b_wt -6.4150014964 0.0012174935 #> 1064        b_wt -6.4084948603 0.0012044281 #> 1065        b_wt -6.4019882242 0.0011929748 #> 1066        b_wt -6.3954815881 0.0011831339 #> 1067        b_wt -6.3889749520 0.0011748985 #> 1068        b_wt -6.3824683159 0.0011682550 #> 1069        b_wt -6.3759616798 0.0011632676 #> 1070        b_wt -6.3694550437 0.0011600828 #> 1071        b_wt -6.3629484076 0.0011583863 #> 1072        b_wt -6.3564417715 0.0011581369 #> 1073        b_wt -6.3499351354 0.0011592898 #> 1074        b_wt -6.3434284993 0.0011617978 #> 1075        b_wt -6.3369218632 0.0011656107 #> 1076        b_wt -6.3304152270 0.0011709246 #> 1077        b_wt -6.3239085909 0.0011774297 #> 1078        b_wt -6.3174019548 0.0011850484 #> 1079        b_wt -6.3108953187 0.0011937278 #> 1080        b_wt -6.3043886826 0.0012034160 #> 1081        b_wt -6.2978820465 0.0012140623 #> 1082        b_wt -6.2913754104 0.0012257346 #> 1083        b_wt -6.2848687743 0.0012383294 #> 1084        b_wt -6.2783621382 0.0012517228 #> 1085        b_wt -6.2718555021 0.0012658788 #> 1086        b_wt -6.2653488660 0.0012807649 #> 1087        b_wt -6.2588422299 0.0012963526 #> 1088        b_wt -6.2523355938 0.0013126592 #> 1089        b_wt -6.2458289577 0.0013297278 #> 1090        b_wt -6.2393223215 0.0013474350 #> 1091        b_wt -6.2328156854 0.0013657753 #> 1092        b_wt -6.2263090493 0.0013847475 #> 1093        b_wt -6.2198024132 0.0014043550 #> 1094        b_wt -6.2132957771 0.0014246053 #> 1095        b_wt -6.2067891410 0.0014456618 #> 1096        b_wt -6.2002825049 0.0014674109 #> 1097        b_wt -6.1937758688 0.0014898701 #> 1098        b_wt -6.1872692327 0.0015130674 #> 1099        b_wt -6.1807625966 0.0015370337 #> 1100        b_wt -6.1742559605 0.0015618030 #> 1101        b_wt -6.1677493244 0.0015875436 #> 1102        b_wt -6.1612426883 0.0016142593 #> 1103        b_wt -6.1547360522 0.0016419207 #> 1104        b_wt -6.1482294161 0.0016705711 #> 1105        b_wt -6.1417227799 0.0017002540 #> 1106        b_wt -6.1352161438 0.0017310135 #> 1107        b_wt -6.1287095077 0.0017629791 #> 1108        b_wt -6.1222028716 0.0017963122 #> 1109        b_wt -6.1156962355 0.0018308747 #> 1110        b_wt -6.1091895994 0.0018667047 #> 1111        b_wt -6.1026829633 0.0019038382 #> 1112        b_wt -6.0961763272 0.0019423083 #> 1113        b_wt -6.0896696911 0.0019821453 #> 1114        b_wt -6.0831630550 0.0020237032 #> 1115        b_wt -6.0766564189 0.0020666883 #> 1116        b_wt -6.0701497828 0.0021111113 #> 1117        b_wt -6.0636431467 0.0021569802 #> 1118        b_wt -6.0571365106 0.0022042982 #> 1119        b_wt -6.0506298745 0.0022530632 #> 1120        b_wt -6.0441232383 0.0023034874 #> 1121        b_wt -6.0376166022 0.0023554326 #> 1122        b_wt -6.0311099661 0.0024087630 #> 1123        b_wt -6.0246033300 0.0024634463 #> 1124        b_wt -6.0180966939 0.0025194440 #> 1125        b_wt -6.0115900578 0.0025767117 #> 1126        b_wt -6.0050834217 0.0026352914 #> 1127        b_wt -5.9985767856 0.0026951782 #> 1128        b_wt -5.9920701495 0.0027561208 #> 1129        b_wt -5.9855635134 0.0028180451 #> 1130        b_wt -5.9790568773 0.0028808722 #> 1131        b_wt -5.9725502412 0.0029445185 #> 1132        b_wt -5.9660436051 0.0030088999 #> 1133        b_wt -5.9595369690 0.0030740260 #> 1134        b_wt -5.9530303328 0.0031396327 #> 1135        b_wt -5.9465236967 0.0032056177 #> 1136        b_wt -5.9400170606 0.0032718764 #> 1137        b_wt -5.9335104245 0.0033383026 #> 1138        b_wt -5.9270037884 0.0034047892 #> 1139        b_wt -5.9204971523 0.0034711933 #> 1140        b_wt -5.9139905162 0.0035373725 #> 1141        b_wt -5.9074838801 0.0036032258 #> 1142        b_wt -5.9009772440 0.0036686496 #> 1143        b_wt -5.8944706079 0.0037335426 #> 1144        b_wt -5.8879639718 0.0037978069 #> 1145        b_wt -5.8814573357 0.0038612718 #> 1146        b_wt -5.8749506996 0.0039237696 #> 1147        b_wt -5.8684440635 0.0039853260 #> 1148        b_wt -5.8619374274 0.0040458688 #> 1149        b_wt -5.8554307912 0.0041053325 #> 1150        b_wt -5.8489241551 0.0041636588 #> 1151        b_wt -5.8424175190 0.0042207774 #> 1152        b_wt -5.8359108829 0.0042763881 #> 1153        b_wt -5.8294042468 0.0043307239 #> 1154        b_wt -5.8228976107 0.0043837704 #> 1155        b_wt -5.8163909746 0.0044355224 #> 1156        b_wt -5.8098843385 0.0044859844 #> 1157        b_wt -5.8033777024 0.0045351704 #> 1158        b_wt -5.7968710663 0.0045828983 #> 1159        b_wt -5.7903644302 0.0046293670 #> 1160        b_wt -5.7838577941 0.0046747053 #> 1161        b_wt -5.7773511580 0.0047189794 #> 1162        b_wt -5.7708445219 0.0047622648 #> 1163        b_wt -5.7643378858 0.0048046466 #> 1164        b_wt -5.7578312496 0.0048461550 #> 1165        b_wt -5.7513246135 0.0048869231 #> 1166        b_wt -5.7448179774 0.0049271799 #> 1167        b_wt -5.7383113413 0.0049670564 #> 1168        b_wt -5.7318047052 0.0050066912 #> 1169        b_wt -5.7252980691 0.0050462302 #> 1170        b_wt -5.7187914330 0.0050858326 #> 1171        b_wt -5.7122847969 0.0051257641 #> 1172        b_wt -5.7057781608 0.0051661835 #> 1173        b_wt -5.6992715247 0.0052072672 #> 1174        b_wt -5.6927648886 0.0052491956 #> 1175        b_wt -5.6862582525 0.0052921530 #> 1176        b_wt -5.6797516164 0.0053363267 #> 1177        b_wt -5.6732449803 0.0053822163 #> 1178        b_wt -5.6667383441 0.0054298952 #> 1179        b_wt -5.6602317080 0.0054794863 #> 1180        b_wt -5.6537250719 0.0055311854 #> 1181        b_wt -5.6472184358 0.0055851886 #> 1182        b_wt -5.6407117997 0.0056416915 #> 1183        b_wt -5.6342051636 0.0057012082 #> 1184        b_wt -5.6276985275 0.0057640663 #> 1185        b_wt -5.6211918914 0.0058301177 #> 1186        b_wt -5.6146852553 0.0058995493 #> 1187        b_wt -5.6081786192 0.0059725454 #> 1188        b_wt -5.6016719831 0.0060492870 #> 1189        b_wt -5.5951653470 0.0061300892 #> 1190        b_wt -5.5886587109 0.0062158696 #> 1191        b_wt -5.5821520748 0.0063060129 #> 1192        b_wt -5.5756454387 0.0064006782 #> 1193        b_wt -5.5691388025 0.0065000201 #> 1194        b_wt -5.5626321664 0.0066041876 #> 1195        b_wt -5.5561255303 0.0067133244 #> 1196        b_wt -5.5496188942 0.0068285777 #> 1197        b_wt -5.5431122581 0.0069493535 #> 1198        b_wt -5.5366056220 0.0070755578 #> 1199        b_wt -5.5300989859 0.0072073011 #> 1200        b_wt -5.5235923498 0.0073446874 #> 1201        b_wt -5.5170857137 0.0074878134 #> 1202        b_wt -5.5105790776 0.0076374579 #> 1203        b_wt -5.5040724415 0.0077937291 #> 1204        b_wt -5.4975658054 0.0079560174 #> 1205        b_wt -5.4910591693 0.0081243791 #> 1206        b_wt -5.4845525332 0.0082988627 #> 1207        b_wt -5.4780458971 0.0084795088 #> 1208        b_wt -5.4715392609 0.0086666097 #> 1209        b_wt -5.4650326248 0.0088611127 #> 1210        b_wt -5.4585259887 0.0090618491 #> 1211        b_wt -5.4520193526 0.0092688174 #> 1212        b_wt -5.4455127165 0.0094820085 #> 1213        b_wt -5.4390060804 0.0097014065 #> 1214        b_wt -5.4324994443 0.0099269883 #> 1215        b_wt -5.4259928082 0.0101599367 #> 1216        b_wt -5.4194861721 0.0103991747 #> 1217        b_wt -5.4129795360 0.0106444559 #> 1218        b_wt -5.4064728999 0.0108957294 #> 1219        b_wt -5.3999662638 0.0111529412 #> 1220        b_wt -5.3934596277 0.0114160348 #> 1221        b_wt -5.3869529916 0.0116856676 #> 1222        b_wt -5.3804463554 0.0119616503 #> 1223        b_wt -5.3739397193 0.0122433061 #> 1224        b_wt -5.3674330832 0.0125305821 #> 1225        b_wt -5.3609264471 0.0128234293 #> 1226        b_wt -5.3544198110 0.0131218040 #> 1227        b_wt -5.3479131749 0.0134259438 #> 1228        b_wt -5.3414065388 0.0137365252 #> 1229        b_wt -5.3348999027 0.0140525466 #> 1230        b_wt -5.3283932666 0.0143740109 #> 1231        b_wt -5.3218866305 0.0147009340 #> 1232        b_wt -5.3153799944 0.0150333448 #> 1233        b_wt -5.3088733583 0.0153712865 #> 1234        b_wt -5.3023667222 0.0157160070 #> 1235        b_wt -5.2958600861 0.0160665902 #> 1236        b_wt -5.2893534500 0.0164230162 #> 1237        b_wt -5.2828468138 0.0167854153 #> 1238        b_wt -5.2763401777 0.0171539358 #> 1239        b_wt -5.2698335416 0.0175287435 #> 1240        b_wt -5.2633269055 0.0179109273 #> 1241        b_wt -5.2568202694 0.0183005589 #> 1242        b_wt -5.2503136333 0.0186972388 #> 1243        b_wt -5.2438069972 0.0191012193 #> 1244        b_wt -5.2373003611 0.0195127666 #> 1245        b_wt -5.2307937250 0.0199321590 #> 1246        b_wt -5.2242870889 0.0203601930 #> 1247        b_wt -5.2177804528 0.0207982526 #> 1248        b_wt -5.2112738167 0.0212452465 #> 1249        b_wt -5.2047671806 0.0217014904 #> 1250        b_wt -5.1982605445 0.0221673000 #> 1251        b_wt -5.1917539084 0.0226429891 #> 1252        b_wt -5.1852472722 0.0231288679 #> 1253        b_wt -5.1787406361 0.0236276090 #> 1254        b_wt -5.1722340000 0.0241374579 #> 1255        b_wt -5.1657273639 0.0246585188 #> 1256        b_wt -5.1592207278 0.0251910363 #> 1257        b_wt -5.1527140917 0.0257352348 #> 1258        b_wt -5.1462074556 0.0262913166 #> 1259        b_wt -5.1397008195 0.0268612028 #> 1260        b_wt -5.1331941834 0.0274444397 #> 1261        b_wt -5.1266875473 0.0280400319 #> 1262        b_wt -5.1201809112 0.0286480298 #> 1263        b_wt -5.1136742751 0.0292684508 #> 1264        b_wt -5.1071676390 0.0299012779 #> 1265        b_wt -5.1006610029 0.0305472889 #> 1266        b_wt -5.0941543667 0.0312075023 #> 1267        b_wt -5.0876477306 0.0318797329 #> 1268        b_wt -5.0811410945 0.0325637804 #> 1269        b_wt -5.0746344584 0.0332594118 #> 1270        b_wt -5.0681278223 0.0339663634 #> 1271        b_wt -5.0616211862 0.0346843410 #> 1272        b_wt -5.0551145501 0.0354153017 #> 1273        b_wt -5.0486079140 0.0361564481 #> 1274        b_wt -5.0421012779 0.0369073214 #> 1275        b_wt -5.0355946418 0.0376675154 #> 1276        b_wt -5.0290880057 0.0384366129 #> 1277        b_wt -5.0225813696 0.0392141888 #> 1278        b_wt -5.0160747335 0.0400009320 #> 1279        b_wt -5.0095680974 0.0407956851 #> 1280        b_wt -5.0030614613 0.0415973964 #> 1281        b_wt -4.9965548251 0.0424056722 #> 1282        b_wt -4.9900481890 0.0432201388 #> 1283        b_wt -4.9835415529 0.0440404461 #> 1284        b_wt -4.9770349168 0.0448666649 #> 1285        b_wt -4.9705282807 0.0456988181 #> 1286        b_wt -4.9640216446 0.0465358534 #> 1287        b_wt -4.9575150085 0.0473776134 #> 1288        b_wt -4.9510083724 0.0482239891 #> 1289        b_wt -4.9445017363 0.0490749212 #> 1290        b_wt -4.9379951002 0.0499304141 #> 1291        b_wt -4.9314884641 0.0507916138 #> 1292        b_wt -4.9249818280 0.0516576565 #> 1293        b_wt -4.9184751919 0.0525287786 #> 1294        b_wt -4.9119685558 0.0534052748 #> 1295        b_wt -4.9054619197 0.0542874975 #> 1296        b_wt -4.8989552835 0.0551758560 #> 1297        b_wt -4.8924486474 0.0560720035 #> 1298        b_wt -4.8859420113 0.0569761254 #> 1299        b_wt -4.8794353752 0.0578883710 #> 1300        b_wt -4.8729287391 0.0588094148 #> 1301        b_wt -4.8664221030 0.0597399708 #> 1302        b_wt -4.8599154669 0.0606807889 #> 1303        b_wt -4.8534088308 0.0616336932 #> 1304        b_wt -4.8469021947 0.0626005828 #> 1305        b_wt -4.8403955586 0.0635806789 #> 1306        b_wt -4.8338889225 0.0645748348 #> 1307        b_wt -4.8273822864 0.0655839042 #> 1308        b_wt -4.8208756503 0.0666087365 #> 1309        b_wt -4.8143690142 0.0676503682 #> 1310        b_wt -4.8078623780 0.0687135646 #> 1311        b_wt -4.8013557419 0.0697954278 #> 1312        b_wt -4.7948491058 0.0708966803 #> 1313        b_wt -4.7883424697 0.0720180005 #> 1314        b_wt -4.7818358336 0.0731600182 #> 1315        b_wt -4.7753291975 0.0743233106 #> 1316        b_wt -4.7688225614 0.0755121704 #> 1317        b_wt -4.7623159253 0.0767248560 #> 1318        b_wt -4.7558092892 0.0779602830 #> 1319        b_wt -4.7493026531 0.0792186717 #> 1320        b_wt -4.7427960170 0.0805001646 #> 1321        b_wt -4.7362893809 0.0818048249 #> 1322        b_wt -4.7297827448 0.0831347930 #> 1323        b_wt -4.7232761087 0.0844908467 #> 1324        b_wt -4.7167694726 0.0858695193 #> 1325        b_wt -4.7102628364 0.0872704398 #> 1326        b_wt -4.7037562003 0.0886931571 #> 1327        b_wt -4.6972495642 0.0901371422 #> 1328        b_wt -4.6907429281 0.0916021811 #> 1329        b_wt -4.6842362920 0.0930911109 #> 1330        b_wt -4.6777296559 0.0945987313 #> 1331        b_wt -4.6712230198 0.0961241516 #> 1332        b_wt -4.6647163837 0.0976664279 #> 1333        b_wt -4.6582097476 0.0992245668 #> 1334        b_wt -4.6517031115 0.1007975301 #> 1335        b_wt -4.6451964754 0.1023863672 #> 1336        b_wt -4.6386898393 0.1039878757 #> 1337        b_wt -4.6321832032 0.1056001641 #> 1338        b_wt -4.6256765671 0.1072220652 #> 1339        b_wt -4.6191699310 0.1088524085 #> 1340        b_wt -4.6126632948 0.1104900249 #> 1341        b_wt -4.6061566587 0.1121342144 #> 1342        b_wt -4.5996500226 0.1137834685 #> 1343        b_wt -4.5931433865 0.1154359344 #> 1344        b_wt -4.5866367504 0.1170905637 #> 1345        b_wt -4.5801301143 0.1187463508 #> 1346        b_wt -4.5736234782 0.1204023370 #> 1347        b_wt -4.5671168421 0.1220575631 #> 1348        b_wt -4.5606102060 0.1237106161 #> 1349        b_wt -4.5541035699 0.1253609423 #> 1350        b_wt -4.5475969338 0.1270079102 #> 1351        b_wt -4.5410902977 0.1286509624 #> 1352        b_wt -4.5345836616 0.1302896179 #> 1353        b_wt -4.5280770255 0.1319234741 #> 1354        b_wt -4.5215703893 0.1335512065 #> 1355        b_wt -4.5150637532 0.1351732992 #> 1356        b_wt -4.5085571171 0.1367899386 #> 1357        b_wt -4.5020504810 0.1384011582 #> 1358        b_wt -4.4955438449 0.1400070744 #> 1359        b_wt -4.4890372088 0.1416078853 #> 1360        b_wt -4.4825305727 0.1432033975 #> 1361        b_wt -4.4760239366 0.1447941638 #> 1362        b_wt -4.4695173005 0.1463812858 #> 1363        b_wt -4.4630106644 0.1479653632 #> 1364        b_wt -4.4565040283 0.1495470620 #> 1365        b_wt -4.4499973922 0.1511271125 #> 1366        b_wt -4.4434907561 0.1527063173 #> 1367        b_wt -4.4369841200 0.1542859050 #> 1368        b_wt -4.4304774839 0.1558669934 #> 1369        b_wt -4.4239708477 0.1574505819 #> 1370        b_wt -4.4174642116 0.1590377054 #> 1371        b_wt -4.4109575755 0.1606294304 #> 1372        b_wt -4.4044509394 0.1622268509 #> 1373        b_wt -4.3979443033 0.1638327550 #> 1374        b_wt -4.3914376672 0.1654475651 #> 1375        b_wt -4.3849310311 0.1670721708 #> 1376        b_wt -4.3784243950 0.1687077234 #> 1377        b_wt -4.3719177589 0.1703553689 #> 1378        b_wt -4.3654111228 0.1720162447 #> 1379        b_wt -4.3589044867 0.1736933925 #> 1380        b_wt -4.3523978506 0.1753882568 #> 1381        b_wt -4.3458912145 0.1771002683 #> 1382        b_wt -4.3393845784 0.1788304389 #> 1383        b_wt -4.3328779423 0.1805797430 #> 1384        b_wt -4.3263713061 0.1823491150 #> 1385        b_wt -4.3198646700 0.1841404615 #> 1386        b_wt -4.3133580339 0.1859579738 #> 1387        b_wt -4.3068513978 0.1877984811 #> 1388        b_wt -4.3003447617 0.1896626511 #> 1389        b_wt -4.2938381256 0.1915510924 #> 1390        b_wt -4.2873314895 0.1934643526 #> 1391        b_wt -4.2808248534 0.1954029175 #> 1392        b_wt -4.2743182173 0.1973725329 #> 1393        b_wt -4.2678115812 0.1993690886 #> 1394        b_wt -4.2613049451 0.2013920587 #> 1395        b_wt -4.2547983090 0.2034415785 #> 1396        b_wt -4.2482916729 0.2055177154 #> 1397        b_wt -4.2417850368 0.2076204690 #> 1398        b_wt -4.2352784006 0.2097531611 #> 1399        b_wt -4.2287717645 0.2119148325 #> 1400        b_wt -4.2222651284 0.2141024809 #> 1401        b_wt -4.2157584923 0.2163157587 #> 1402        b_wt -4.2092518562 0.2185542569 #> 1403        b_wt -4.2027452201 0.2208175068 #> 1404        b_wt -4.1962385840 0.2231062607 #> 1405        b_wt -4.1897319479 0.2254225331 #> 1406        b_wt -4.1832253118 0.2277613497 #> 1407        b_wt -4.1767186757 0.2301219635 #> 1408        b_wt -4.1702120396 0.2325035844 #> 1409        b_wt -4.1637054035 0.2349053816 #> 1410        b_wt -4.1571987674 0.2373264865 #> 1411        b_wt -4.1506921313 0.2397695761 #> 1412        b_wt -4.1441854952 0.2422298939 #> 1413        b_wt -4.1376788590 0.2447060975 #> 1414        b_wt -4.1311722229 0.2471971822 #> 1415        b_wt -4.1246655868 0.2497021295 #> 1416        b_wt -4.1181589507 0.2522199109 #> 1417        b_wt -4.1116523146 0.2547509038 #> 1418        b_wt -4.1051456785 0.2572931249 #> 1419        b_wt -4.0986390424 0.2598444596 #> 1420        b_wt -4.0921324063 0.2624038949 #> 1421        b_wt -4.0856257702 0.2649704338 #> 1422        b_wt -4.0791191341 0.2675430982 #> 1423        b_wt -4.0726124980 0.2701211802 #> 1424        b_wt -4.0661058619 0.2727038260 #> 1425        b_wt -4.0595992258 0.2752893252 #> 1426        b_wt -4.0530925897 0.2778768624 #> 1427        b_wt -4.0465859536 0.2804656640 #> 1428        b_wt -4.0400793174 0.2830550002 #> 1429        b_wt -4.0335726813 0.2856441877 #> 1430        b_wt -4.0270660452 0.2882322301 #> 1431        b_wt -4.0205594091 0.2908186172 #> 1432        b_wt -4.0140527730 0.2934029019 #> 1433        b_wt -4.0075461369 0.2959846844 #> 1434        b_wt -4.0010395008 0.2985636232 #> 1435        b_wt -3.9945328647 0.3011394361 #> 1436        b_wt -3.9880262286 0.3037113838 #> 1437        b_wt -3.9815195925 0.3062795032 #> 1438        b_wt -3.9750129564 0.3088440553 #> 1439        b_wt -3.9685063203 0.3114050727 #> 1440        b_wt -3.9619996842 0.3139626454 #> 1441        b_wt -3.9554930481 0.3165169193 #> 1442        b_wt -3.9489864119 0.3190678998 #> 1443        b_wt -3.9424797758 0.3216157428 #> 1444        b_wt -3.9359731397 0.3241613003 #> 1445        b_wt -3.9294665036 0.3267049769 #> 1446        b_wt -3.9229598675 0.3292472155 #> 1447        b_wt -3.9164532314 0.3317884947 #> 1448        b_wt -3.9099465953 0.3343293253 #> 1449        b_wt -3.9034399592 0.3368704790 #> 1450        b_wt -3.8969333231 0.3394126466 #> 1451        b_wt -3.8904266870 0.3419564204 #> 1452        b_wt -3.8839200509 0.3445023979 #> 1453        b_wt -3.8774134148 0.3470511765 #> 1454        b_wt -3.8709067787 0.3496033492 #> 1455        b_wt -3.8644001426 0.3521602447 #> 1456        b_wt -3.8578935065 0.3547223105 #> 1457        b_wt -3.8513868703 0.3572897397 #> 1458        b_wt -3.8448802342 0.3598629993 #> 1459        b_wt -3.8383735981 0.3624425138 #> 1460        b_wt -3.8318669620 0.3650286608 #> 1461        b_wt -3.8253603259 0.3676223545 #> 1462        b_wt -3.8188536898 0.3702244254 #> 1463        b_wt -3.8123470537 0.3728339355 #> 1464        b_wt -3.8058404176 0.3754509217 #> 1465        b_wt -3.7993337815 0.3780753408 #> 1466        b_wt -3.7928271454 0.3807070668 #> 1467        b_wt -3.7863205093 0.3833459316 #> 1468        b_wt -3.7798138732 0.3859929363 #> 1469        b_wt -3.7733072371 0.3886459695 #> 1470        b_wt -3.7668006010 0.3913044165 #> 1471        b_wt -3.7602939648 0.3939675640 #> 1472        b_wt -3.7537873287 0.3966346004 #> 1473        b_wt -3.7472806926 0.3993046157 #> 1474        b_wt -3.7407740565 0.4019766321 #> 1475        b_wt -3.7342674204 0.4046488516 #> 1476        b_wt -3.7277607843 0.4073198378 #> 1477        b_wt -3.7212541482 0.4099881827 #> 1478        b_wt -3.7147475121 0.4126523931 #> 1479        b_wt -3.7082408760 0.4153108947 #> 1480        b_wt -3.7017342399 0.4179611520 #> 1481        b_wt -3.6952276038 0.4206001503 #> 1482        b_wt -3.6887209677 0.4232270978 #> 1483        b_wt -3.6822143316 0.4258400806 #> 1484        b_wt -3.6757076955 0.4284371420 #> 1485        b_wt -3.6692010594 0.4310162889 #> 1486        b_wt -3.6626944232 0.4335751503 #> 1487        b_wt -3.6561877871 0.4361064693 #> 1488        b_wt -3.6496811510 0.4386124991 #> 1489        b_wt -3.6431745149 0.4410911846 #> 1490        b_wt -3.6366678788 0.4435404867 #> 1491        b_wt -3.6301612427 0.4459583891 #> 1492        b_wt -3.6236546066 0.4483429051 #> 1493        b_wt -3.6171479705 0.4506854968 #> 1494        b_wt -3.6106413344 0.4529876609 #> 1495        b_wt -3.6041346983 0.4552497349 #> 1496        b_wt -3.5976280622 0.4574700215 #> 1497        b_wt -3.5911214261 0.4596468996 #> 1498        b_wt -3.5846147900 0.4617788288 #> 1499        b_wt -3.5781081539 0.4638596099 #> 1500        b_wt -3.5716015178 0.4658856959 #> 1501        b_wt -3.5650948816 0.4678621480 #> 1502        b_wt -3.5585882455 0.4697879300 #> 1503        b_wt -3.5520816094 0.4716621101 #> 1504        b_wt -3.5455749733 0.4734838622 #> 1505        b_wt -3.5390683372 0.4752511704 #> 1506        b_wt -3.5325617011 0.4769534033 #> 1507        b_wt -3.5260550650 0.4786012154 #> 1508        b_wt -3.5195484289 0.4801943369 #> 1509        b_wt -3.5130417928 0.4817325974 #> 1510        b_wt -3.5065351567 0.4832159231 #> 1511        b_wt -3.5000285206 0.4846443341 #> 1512        b_wt -3.4935218845 0.4860080800 #> 1513        b_wt -3.4870152484 0.4873146858 #> 1514        b_wt -3.4805086123 0.4885672708 #> 1515        b_wt -3.4740019761 0.4897662736 #> 1516        b_wt -3.4674953400 0.4909121950 #> 1517        b_wt -3.4609887039 0.4920055939 #> 1518        b_wt -3.4544820678 0.4930416330 #> 1519        b_wt -3.4479754317 0.4940204085 #> 1520        b_wt -3.4414687956 0.4949491102 #> 1521        b_wt -3.4349621595 0.4958285061 #> 1522        b_wt -3.4284555234 0.4966593862 #> 1523        b_wt -3.4219488873 0.4974425586 #> 1524        b_wt -3.4154422512 0.4981773459 #> 1525        b_wt -3.4089356151 0.4988572749 #> 1526        b_wt -3.4024289790 0.4994924909 #> 1527        b_wt -3.3959223429 0.5000838270 #> 1528        b_wt -3.3894157068 0.5006321103 #> 1529        b_wt -3.3829090707 0.5011381598 #> 1530        b_wt -3.3764024345 0.5016027852 #> 1531        b_wt -3.3698957984 0.5020192962 #> 1532        b_wt -3.3633891623 0.5023947473 #> 1533        b_wt -3.3568825262 0.5027315708 #> 1534        b_wt -3.3503758901 0.5030305081 #> 1535        b_wt -3.3438692540 0.5032922873 #> 1536        b_wt -3.3373626179 0.5035176237 #> 1537        b_wt -3.3308559818 0.5037031994 #> 1538        b_wt -3.3243493457 0.5038500094 #> 1539        b_wt -3.3178427096 0.5039628479 #> 1540        b_wt -3.3113360735 0.5040423784 #> 1541        b_wt -3.3048294374 0.5040892583 #> 1542        b_wt -3.2983228013 0.5041041403 #> 1543        b_wt -3.2918161652 0.5040864087 #> 1544        b_wt -3.2853095291 0.5040324124 #> 1545        b_wt -3.2788028929 0.5039487423 #> 1546        b_wt -3.2722962568 0.5038360407 #> 1547        b_wt -3.2657896207 0.5036949495 #> 1548        b_wt -3.2592829846 0.5035261102 #> 1549        b_wt -3.2527763485 0.5033301640 #> 1550        b_wt -3.2462697124 0.5031026901 #> 1551        b_wt -3.2397630763 0.5028489072 #> 1552        b_wt -3.2332564402 0.5025703069 #> 1553        b_wt -3.2267498041 0.5022675128 #> 1554        b_wt -3.2202431680 0.5019411404 #> 1555        b_wt -3.2137365319 0.5015917955 #> 1556        b_wt -3.2072298958 0.5012173919 #> 1557        b_wt -3.2007232597 0.5008191379 #> 1558        b_wt -3.1942166236 0.5003999568 #> 1559        b_wt -3.1877099874 0.4999603642 #> 1560        b_wt -3.1812033513 0.4995008510 #> 1561        b_wt -3.1746967152 0.4990218806 #> 1562        b_wt -3.1681900791 0.4985229597 #> 1563        b_wt -3.1616834430 0.4980021909 #> 1564        b_wt -3.1551768069 0.4974633410 #> 1565        b_wt -3.1486701708 0.4969066951 #> 1566        b_wt -3.1421635347 0.4963324993 #> 1567        b_wt -3.1356568986 0.4957409601 #> 1568        b_wt -3.1291502625 0.4951322433 #> 1569        b_wt -3.1226436264 0.4945029540 #> 1570        b_wt -3.1161369903 0.4938562841 #> 1571        b_wt -3.1096303542 0.4931926355 #> 1572        b_wt -3.1031237181 0.4925119624 #> 1573        b_wt -3.0966170820 0.4918141824 #> 1574        b_wt -3.0901104458 0.4910991778 #> 1575        b_wt -3.0836038097 0.4903644444 #> 1576        b_wt -3.0770971736 0.4896103031 #> 1577        b_wt -3.0705905375 0.4888382250 #> 1578        b_wt -3.0640839014 0.4880479505 #> 1579        b_wt -3.0575772653 0.4872392037 #> 1580        b_wt -3.0510706292 0.4864116952 #> 1581        b_wt -3.0445639931 0.4855639772 #> 1582        b_wt -3.0380573570 0.4846934130 #> 1583        b_wt -3.0315507209 0.4838029900 #> 1584        b_wt -3.0250440848 0.4828924109 #> 1585        b_wt -3.0185374487 0.4819613888 #> 1586        b_wt -3.0120308126 0.4810096502 #> 1587        b_wt -3.0055241765 0.4800369374 #> 1588        b_wt -2.9990175404 0.4790383603 #> 1589        b_wt -2.9925109042 0.4780179303 #> 1590        b_wt -2.9860042681 0.4769758166 #> 1591        b_wt -2.9794976320 0.4759118910 #> 1592        b_wt -2.9729909959 0.4748260550 #> 1593        b_wt -2.9664843598 0.4737182400 #> 1594        b_wt -2.9599777237 0.4725852882 #> 1595        b_wt -2.9534710876 0.4714283706 #> 1596        b_wt -2.9469644515 0.4702495198 #> 1597        b_wt -2.9404578154 0.4690488238 #> 1598        b_wt -2.9339511793 0.4678263958 #> 1599        b_wt -2.9274445432 0.4665823726 #> 1600        b_wt -2.9209379071 0.4653154805 #> 1601        b_wt -2.9144312710 0.4640239392 #> 1602        b_wt -2.9079246349 0.4627114641 #> 1603        b_wt -2.9014179987 0.4613782674 #> 1604        b_wt -2.8949113626 0.4600245638 #> 1605        b_wt -2.8884047265 0.4586505671 #> 1606        b_wt -2.8818980904 0.4572564870 #> 1607        b_wt -2.8753914543 0.4558381188 #> 1608        b_wt -2.8688848182 0.4544000169 #> 1609        b_wt -2.8623781821 0.4529424559 #> 1610        b_wt -2.8558715460 0.4514655529 #> 1611        b_wt -2.8493649099 0.4499693955 #> 1612        b_wt -2.8428582738 0.4484540395 #> 1613        b_wt -2.8363516377 0.4469166083 #> 1614        b_wt -2.8298450016 0.4453583750 #> 1615        b_wt -2.8233383655 0.4437807785 #> 1616        b_wt -2.8168317294 0.4421836618 #> 1617        b_wt -2.8103250933 0.4405668226 #> 1618        b_wt -2.8038184571 0.4389300130 #> 1619        b_wt -2.7973118210 0.4372713534 #> 1620        b_wt -2.7908051849 0.4355886966 #> 1621        b_wt -2.7842985488 0.4338847518 #> 1622        b_wt -2.7777919127 0.4321590391 #> 1623        b_wt -2.7712852766 0.4304110409 #> 1624        b_wt -2.7647786405 0.4286402042 #> 1625        b_wt -2.7582720044 0.4268458919 #> 1626        b_wt -2.7517653683 0.4250217796 #> 1627        b_wt -2.7452587322 0.4231725738 #> 1628        b_wt -2.7387520961 0.4212975964 #> 1629        b_wt -2.7322454600 0.4193961613 #> 1630        b_wt -2.7257388239 0.4174675791 #> 1631        b_wt -2.7192321878 0.4155111616 #> 1632        b_wt -2.7127255517 0.4135215259 #> 1633        b_wt -2.7062189155 0.4115002559 #> 1634        b_wt -2.6997122794 0.4094488025 #> 1635        b_wt -2.6932056433 0.4073665834 #> 1636        b_wt -2.6866990072 0.4052530548 #> 1637        b_wt -2.6801923711 0.4031077160 #> 1638        b_wt -2.6736857350 0.4009273256 #> 1639        b_wt -2.6671790989 0.3987093591 #> 1640        b_wt -2.6606724628 0.3964582881 #> 1641        b_wt -2.6541658267 0.3941739258 #> 1642        b_wt -2.6476591906 0.3918561590 #> 1643        b_wt -2.6411525545 0.3895049508 #> 1644        b_wt -2.6346459184 0.3871199844 #> 1645        b_wt -2.6281392823 0.3846945095 #> 1646        b_wt -2.6216326462 0.3822362140 #> 1647        b_wt -2.6151260100 0.3797455050 #> 1648        b_wt -2.6086193739 0.3772228779 #> 1649        b_wt -2.6021127378 0.3746689161 #> 1650        b_wt -2.5956061017 0.3720842910 #> 1651        b_wt -2.5890994656 0.3694649682 #> 1652        b_wt -2.5825928295 0.3668153087 #> 1653        b_wt -2.5760861934 0.3641382297 #> 1654        b_wt -2.5695795573 0.3614348415 #> 1655        b_wt -2.5630729212 0.3587063290 #> 1656        b_wt -2.5565662851 0.3559539488 #> 1657        b_wt -2.5500596490 0.3531771025 #> 1658        b_wt -2.5435530129 0.3503769166 #> 1659        b_wt -2.5370463768 0.3475579875 #> 1660        b_wt -2.5305397407 0.3447218782 #> 1661        b_wt -2.5240331046 0.3418701935 #> 1662        b_wt -2.5175264684 0.3390045753 #> 1663        b_wt -2.5110198323 0.3361265032 #> 1664        b_wt -2.5045131962 0.3332363250 #> 1665        b_wt -2.4980065601 0.3303383699 #> 1666        b_wt -2.4914999240 0.3274343841 #> 1667        b_wt -2.4849932879 0.3245261144 #> 1668        b_wt -2.4784866518 0.3216153041 #> 1669        b_wt -2.4719800157 0.3187036882 #> 1670        b_wt -2.4654733796 0.3157936070 #> 1671        b_wt -2.4589667435 0.3128872123 #> 1672        b_wt -2.4524601074 0.3099860726 #> 1673        b_wt -2.4459534713 0.3070917923 #> 1674        b_wt -2.4394468352 0.3042059366 #> 1675        b_wt -2.4329401991 0.3013300290 #> 1676        b_wt -2.4264335630 0.2984669239 #> 1677        b_wt -2.4199269268 0.2956189801 #> 1678        b_wt -2.4134202907 0.2927860052 #> 1679        b_wt -2.4069136546 0.2899692458 #> 1680        b_wt -2.4004070185 0.2871698887 #> 1681        b_wt -2.3939003824 0.2843890593 #> 1682        b_wt -2.3873937463 0.2816284102 #> 1683        b_wt -2.3808871102 0.2788928910 #> 1684        b_wt -2.3743804741 0.2761793644 #> 1685        b_wt -2.3678738380 0.2734886166 #> 1686        b_wt -2.3613672019 0.2708213689 #> 1687        b_wt -2.3548605658 0.2681782786 #> 1688        b_wt -2.3483539297 0.2655599392 #> 1689        b_wt -2.3418472936 0.2629716906 #> 1690        b_wt -2.3353406575 0.2604105685 #> 1691        b_wt -2.3288340213 0.2578757391 #> 1692        b_wt -2.3223273852 0.2553674804 #> 1693        b_wt -2.3158207491 0.2528860154 #> 1694        b_wt -2.3093141130 0.2504315145 #> 1695        b_wt -2.3028074769 0.2480071144 #> 1696        b_wt -2.2963008408 0.2456131188 #> 1697        b_wt -2.2897942047 0.2432462363 #> 1698        b_wt -2.2832875686 0.2409063915 #> 1699        b_wt -2.2767809325 0.2385934694 #> 1700        b_wt -2.2702742964 0.2363073171 #> 1701        b_wt -2.2637676603 0.2340487157 #> 1702        b_wt -2.2572610242 0.2318214787 #> 1703        b_wt -2.2507543881 0.2296201447 #> 1704        b_wt -2.2442477520 0.2274443923 #> 1705        b_wt -2.2377411159 0.2252938739 #> 1706        b_wt -2.2312344797 0.2231682173 #> 1707        b_wt -2.2247278436 0.2210670263 #> 1708        b_wt -2.2182212075 0.2189944145 #> 1709        b_wt -2.2117145714 0.2169460143 #> 1710        b_wt -2.2052079353 0.2149204514 #> 1711        b_wt -2.1987012992 0.2129172270 #> 1712        b_wt -2.1921946631 0.2109358262 #> 1713        b_wt -2.1856880270 0.2089757188 #> 1714        b_wt -2.1791813909 0.2070387492 #> 1715        b_wt -2.1726747548 0.2051239252 #> 1716        b_wt -2.1661681187 0.2032283571 #> 1717        b_wt -2.1596614826 0.2013514492 #> 1718        b_wt -2.1531548465 0.1994925964 #> 1719        b_wt -2.1466482104 0.1976511850 #> 1720        b_wt -2.1401415743 0.1958273223 #> 1721        b_wt -2.1336349381 0.1940223861 #> 1722        b_wt -2.1271283020 0.1922326198 #> 1723        b_wt -2.1206216659 0.1904573827 #> 1724        b_wt -2.1141150298 0.1886960330 #> 1725        b_wt -2.1076083937 0.1869479297 #> 1726        b_wt -2.1011017576 0.1852124330 #> 1727        b_wt -2.0945951215 0.1834911363 #> 1728        b_wt -2.0880884854 0.1817811452 #> 1729        b_wt -2.0815818493 0.1800815103 #> 1730        b_wt -2.0750752132 0.1783916312 #> 1731        b_wt -2.0685685771 0.1767109207 #> 1732        b_wt -2.0620619410 0.1750388065 #> 1733        b_wt -2.0555553049 0.1733756674 #> 1734        b_wt -2.0490486688 0.1717205473 #> 1735        b_wt -2.0425420326 0.1700721407 #> 1736        b_wt -2.0360353965 0.1684299854 #> 1737        b_wt -2.0295287604 0.1667936450 #> 1738        b_wt -2.0230221243 0.1651627107 #> 1739        b_wt -2.0165154882 0.1635370509 #> 1740        b_wt -2.0100088521 0.1619167684 #> 1741        b_wt -2.0035022160 0.1603007003 #> 1742        b_wt -1.9969955799 0.1586886023 #> 1743        b_wt -1.9904889438 0.1570802637 #> 1744        b_wt -1.9839823077 0.1554755072 #> 1745        b_wt -1.9774756716 0.1538741893 #> 1746        b_wt -1.9709690355 0.1522768875 #> 1747        b_wt -1.9644623994 0.1506828996 #> 1748        b_wt -1.9579557633 0.1490921559 #> 1749        b_wt -1.9514491272 0.1475046826 #> 1750        b_wt -1.9449424910 0.1459205347 #> 1751        b_wt -1.9384358549 0.1443397950 #> 1752        b_wt -1.9319292188 0.1427630861 #> 1753        b_wt -1.9254225827 0.1411904658 #> 1754        b_wt -1.9189159466 0.1396217818 #> 1755        b_wt -1.9124093105 0.1380572317 #> 1756        b_wt -1.9059026744 0.1364970278 #> 1757        b_wt -1.8993960383 0.1349413952 #> 1758        b_wt -1.8928894022 0.1333908946 #> 1759        b_wt -1.8863827661 0.1318464088 #> 1760        b_wt -1.8798761300 0.1303073776 #> 1761        b_wt -1.8733694939 0.1287740520 #> 1762        b_wt -1.8668628578 0.1272466802 #> 1763        b_wt -1.8603562217 0.1257255055 #> 1764        b_wt -1.8538495856 0.1242107642 #> 1765        b_wt -1.8473429494 0.1227042236 #> 1766        b_wt -1.8408363133 0.1212047478 #> 1767        b_wt -1.8343296772 0.1197124455 #> 1768        b_wt -1.8278230411 0.1182274834 #> 1769        b_wt -1.8213164050 0.1167500098 #> 1770        b_wt -1.8148097689 0.1152801544 #> 1771        b_wt -1.8083031328 0.1138191716 #> 1772        b_wt -1.8017964967 0.1123666957 #> 1773        b_wt -1.7952898606 0.1109221048 #> 1774        b_wt -1.7887832245 0.1094854159 #> 1775        b_wt -1.7822765884 0.1080566242 #> 1776        b_wt -1.7757699523 0.1066357032 #> 1777        b_wt -1.7692633162 0.1052231585 #> 1778        b_wt -1.7627566801 0.1038195560 #> 1779        b_wt -1.7562500439 0.1024235351 #> 1780        b_wt -1.7497434078 0.1010349657 #> 1781        b_wt -1.7432367717 0.0996537017 #> 1782        b_wt -1.7367301356 0.0982795820 #> 1783        b_wt -1.7302234995 0.0969124321 #> 1784        b_wt -1.7237168634 0.0955535439 #> 1785        b_wt -1.7172102273 0.0942011293 #> 1786        b_wt -1.7107035912 0.0928549600 #> 1787        b_wt -1.7041969551 0.0915148209 #> 1788        b_wt -1.6976903190 0.0901804952 #> 1789        b_wt -1.6911836829 0.0888517665 #> 1790        b_wt -1.6846770468 0.0875292050 #> 1791        b_wt -1.6781704107 0.0862121060 #> 1792        b_wt -1.6716637746 0.0848998680 #> 1793        b_wt -1.6651571385 0.0835923106 #> 1794        b_wt -1.6586505023 0.0822892667 #> 1795        b_wt -1.6521438662 0.0809905839 #> 1796        b_wt -1.6456372301 0.0796964544 #> 1797        b_wt -1.6391305940 0.0784070158 #> 1798        b_wt -1.6326239579 0.0771215623 #> 1799        b_wt -1.6261173218 0.0758400478 #> 1800        b_wt -1.6196106857 0.0745624507 #> 1801        b_wt -1.6131040496 0.0732887748 #> 1802        b_wt -1.6065974135 0.0720190756 #> 1803        b_wt -1.6000907774 0.0707543283 #> 1804        b_wt -1.5935841413 0.0694937645 #> 1805        b_wt -1.5870775052 0.0682375339 #> 1806        b_wt -1.5805708691 0.0669858148 #> 1807        b_wt -1.5740642330 0.0657388137 #> 1808        b_wt -1.5675575969 0.0644967651 #> 1809        b_wt -1.5610509607 0.0632608604 #> 1810        b_wt -1.5545443246 0.0620310236 #> 1811        b_wt -1.5480376885 0.0608072432 #> 1812        b_wt -1.5415310524 0.0595898900 #> 1813        b_wt -1.5350244163 0.0583793568 #> 1814        b_wt -1.5285177802 0.0571760572 #> 1815        b_wt -1.5220111441 0.0559811670 #> 1816        b_wt -1.5155045080 0.0547957784 #> 1817        b_wt -1.5089978719 0.0536192773 #> 1818        b_wt -1.5024912358 0.0524521579 #> 1819        b_wt -1.4959845997 0.0512949225 #> 1820        b_wt -1.4894779636 0.0501480807 #> 1821        b_wt -1.4829713275 0.0490123206 #> 1822        b_wt -1.4764646914 0.0478906596 #> 1823        b_wt -1.4699580552 0.0467812489 #> 1824        b_wt -1.4634514191 0.0456845970 #> 1825        b_wt -1.4569447830 0.0446012046 #> 1826        b_wt -1.4504381469 0.0435315633 #> 1827        b_wt -1.4439315108 0.0424761539 #> 1828        b_wt -1.4374248747 0.0414380914 #> 1829        b_wt -1.4309182386 0.0404163107 #> 1830        b_wt -1.4244116025 0.0394103471 #> 1831        b_wt -1.4179049664 0.0384205921 #> 1832        b_wt -1.4113983303 0.0374474153 #> 1833        b_wt -1.4048916942 0.0364911640 #> 1834        b_wt -1.3983850581 0.0355538914 #> 1835        b_wt -1.3918784220 0.0346366297 #> 1836        b_wt -1.3853717859 0.0337373121 #> 1837        b_wt -1.3788651498 0.0328561528 #> 1838        b_wt -1.3723585136 0.0319933392 #> 1839        b_wt -1.3658518775 0.0311490327 #> 1840        b_wt -1.3593452414 0.0303238131 #> 1841        b_wt -1.3528386053 0.0295212624 #> 1842        b_wt -1.3463319692 0.0287375577 #> 1843        b_wt -1.3398253331 0.0279727248 #> 1844        b_wt -1.3333186970 0.0272267657 #> 1845        b_wt -1.3268120609 0.0264996598 #> 1846        b_wt -1.3203054248 0.0257913640 #> 1847        b_wt -1.3137987887 0.0251051777 #> 1848        b_wt -1.3072921526 0.0244385245 #> 1849        b_wt -1.3007855165 0.0237903318 #> 1850        b_wt -1.2942788804 0.0231604568 #> 1851        b_wt -1.2877722443 0.0225487413 #> 1852        b_wt -1.2812656082 0.0219550121 #> 1853        b_wt -1.2747589720 0.0213809497 #> 1854        b_wt -1.2682523359 0.0208265732 #> 1855        b_wt -1.2617456998 0.0202894409 #> 1856        b_wt -1.2552390637 0.0197693190 #> 1857        b_wt -1.2487324276 0.0192659649 #> 1858        b_wt -1.2422257915 0.0187791274 #> 1859        b_wt -1.2357191554 0.0183090635 #> 1860        b_wt -1.2292125193 0.0178580594 #> 1861        b_wt -1.2227058832 0.0174226010 #> 1862        b_wt -1.2161992471 0.0170024017 #> 1863        b_wt -1.2096926110 0.0165971687 #> 1864        b_wt -1.2031859749 0.0162066039 #> 1865        b_wt -1.1966793388 0.0158304032 #> 1866        b_wt -1.1901727027 0.0154708282 #> 1867        b_wt -1.1836660665 0.0151253792 #> 1868        b_wt -1.1771594304 0.0147931429 #> 1869        b_wt -1.1706527943 0.0144737842 #> 1870        b_wt -1.1641461582 0.0141669615 #> 1871        b_wt -1.1576395221 0.0138723269 #> 1872        b_wt -1.1511328860 0.0135908300 #> 1873        b_wt -1.1446262499 0.0133219473 #> 1874        b_wt -1.1381196138 0.0130639349 #> 1875        b_wt -1.1316129777 0.0128164085 #> 1876        b_wt -1.1251063416 0.0125789772 #> 1877        b_wt -1.1185997055 0.0123512430 #> 1878        b_wt -1.1120930694 0.0121331610 #> 1879        b_wt -1.1055864333 0.0119254729 #> 1880        b_wt -1.0990797972 0.0117259907 #> 1881        b_wt -1.0925731611 0.0115342858 #> 1882        b_wt -1.0860665249 0.0113499259 #> 1883        b_wt -1.0795598888 0.0111724747 #> 1884        b_wt -1.0730532527 0.0110014936 #> 1885        b_wt -1.0665466166 0.0108376012 #> 1886        b_wt -1.0600399805 0.0106792269 #> 1887        b_wt -1.0535333444 0.0105257360 #> 1888        b_wt -1.0470267083 0.0103766928 #> 1889        b_wt -1.0405200722 0.0102316660 #> 1890        b_wt -1.0340134361 0.0100902296 #> 1891        b_wt -1.0275068000 0.0099522841 #> 1892        b_wt -1.0210001639 0.0098171942 #> 1893        b_wt -1.0144935278 0.0096842394 #> 1894        b_wt -1.0079868917 0.0095530457 #> 1895        b_wt -1.0014802556 0.0094232532 #> 1896        b_wt -0.9949736195 0.0092945175 #> 1897        b_wt -0.9884669833 0.0091665261 #> 1898        b_wt -0.9819603472 0.0090389102 #> 1899        b_wt -0.9754537111 0.0089112755 #> 1900        b_wt -0.9689470750 0.0087833776 #> 1901        b_wt -0.9624404389 0.0086549944 #> 1902        b_wt -0.9559338028 0.0085259264 #> 1903        b_wt -0.9494271667 0.0083959973 #> 1904        b_wt -0.9429205306 0.0082648091 #> 1905        b_wt -0.9364138945 0.0081324077 #> 1906        b_wt -0.9299072584 0.0079987458 #> 1907        b_wt -0.9234006223 0.0078637750 #> 1908        b_wt -0.9168939862 0.0077274711 #> 1909        b_wt -0.9103873501 0.0075898335 #> 1910        b_wt -0.9038807140 0.0074507226 #> 1911        b_wt -0.8973740778 0.0073102691 #> 1912        b_wt -0.8908674417 0.0071686953 #> 1913        b_wt -0.8843608056 0.0070261149 #> 1914        b_wt -0.8778541695 0.0068826597 #> 1915        b_wt -0.8713475334 0.0067384792 #> 1916        b_wt -0.8648408973 0.0065937215 #> 1917        b_wt -0.8583342612 0.0064486103 #> 1918        b_wt -0.8518276251 0.0063034483 #> 1919        b_wt -0.8453209890 0.0061584532 #> 1920        b_wt -0.8388143529 0.0060138500 #> 1921        b_wt -0.8323077168 0.0058698702 #> 1922        b_wt -0.8258010807 0.0057267502 #> 1923        b_wt -0.8192944446 0.0055850495 #> 1924        b_wt -0.8127878085 0.0054448521 #> 1925        b_wt -0.8062811724 0.0053063773 #> 1926        b_wt -0.7997745362 0.0051698580 #> 1927        b_wt -0.7932679001 0.0050355216 #> 1928        b_wt -0.7867612640 0.0049035888 #> 1929        b_wt -0.7802546279 0.0047746834 #> 1930        b_wt -0.7737479918 0.0046489452 #> 1931        b_wt -0.7672413557 0.0045263117 #> 1932        b_wt -0.7607347196 0.0044069448 #> 1933        b_wt -0.7542280835 0.0042909924 #> 1934        b_wt -0.7477214474 0.0041785870 #> 1935        b_wt -0.7412148113 0.0040701003 #> 1936        b_wt -0.7347081752 0.0039660258 #> 1937        b_wt -0.7282015391 0.0038658216 #> 1938        b_wt -0.7216949030 0.0037695308 #> 1939        b_wt -0.7151882669 0.0036771789 #> 1940        b_wt -0.7086816308 0.0035887737 #> 1941        b_wt -0.7021749946 0.0035043054 #> 1942        b_wt -0.6956683585 0.0034246033 #> 1943        b_wt -0.6891617224 0.0033487443 #> 1944        b_wt -0.6826550863 0.0032766199 #> 1945        b_wt -0.6761484502 0.0032081339 #> 1946        b_wt -0.6696418141 0.0031431754 #> 1947        b_wt -0.6631351780 0.0030816201 #> 1948        b_wt -0.6566285419 0.0030237859 #> 1949        b_wt -0.6501219058 0.0029692291 #> 1950        b_wt -0.6436152697 0.0029175150 #> 1951        b_wt -0.6371086336 0.0028684639 #> 1952        b_wt -0.6306019975 0.0028218890 #> 1953        b_wt -0.6240953614 0.0027775979 #> 1954        b_wt -0.6175887253 0.0027355300 #> 1955        b_wt -0.6110820891 0.0026955458 #> 1956        b_wt -0.6045754530 0.0026571198 #> 1957        b_wt -0.5980688169 0.0026200490 #> 1958        b_wt -0.5915621808 0.0025841323 #> 1959        b_wt -0.5850555447 0.0025491713 #> 1960        b_wt -0.5785489086 0.0025149724 #> 1961        b_wt -0.5720422725 0.0024814099 #> 1962        b_wt -0.5655356364 0.0024481347 #> 1963        b_wt -0.5590290003 0.0024149807 #> 1964        b_wt -0.5525223642 0.0023817912 #> 1965        b_wt -0.5460157281 0.0023484187 #> 1966        b_wt -0.5395090920 0.0023147265 #> 1967        b_wt -0.5330024559 0.0022804917 #> 1968        b_wt -0.5264958198 0.0022456068 #> 1969        b_wt -0.5199891837 0.0022100184 #> 1970        b_wt -0.5134825475 0.0021736516 #> 1971        b_wt -0.5069759114 0.0021364443 #> 1972        b_wt -0.5004692753 0.0020983469 #> 1973        b_wt -0.4939626392 0.0020592417 #> 1974        b_wt -0.4874560031 0.0020190454 #> 1975        b_wt -0.4809493670 0.0019778979 #> 1976        b_wt -0.4744427309 0.0019358141 #> 1977        b_wt -0.4679360948 0.0018928201 #> 1978        b_wt -0.4614294587 0.0018489528 #> 1979        b_wt -0.4549228226 0.0018042520 #> 1980        b_wt -0.4484161865 0.0017586410 #> 1981        b_wt -0.4419095504 0.0017123794 #> 1982        b_wt -0.4354029143 0.0016655517 #> 1983        b_wt -0.4288962782 0.0016182490 #> 1984        b_wt -0.4223896421 0.0015705685 #> 1985        b_wt -0.4158830059 0.0015226131 #> 1986        b_wt -0.4093763698 0.0014744909 #> 1987        b_wt -0.4028697337 0.0014263741 #> 1988        b_wt -0.3963630976 0.0013783886 #> 1989        b_wt -0.3898564615 0.0013306527 #> 1990        b_wt -0.3833498254 0.0012832851 #> 1991        b_wt -0.3768431893 0.0012364043 #> 1992        b_wt -0.3703365532 0.0011902000 #> 1993        b_wt -0.3638299171 0.0011448704 #> 1994        b_wt -0.3573232810 0.0011004406 #> 1995        b_wt -0.3508166449 0.0010570176 #> 1996        b_wt -0.3443100088 0.0010147044 #> 1997        b_wt -0.3378033727 0.0009735994 #> 1998        b_wt -0.3312967366 0.0009338234 #> 1999        b_wt -0.3247901004 0.0008957632 #> 2000        b_wt -0.3182834643 0.0008592169 #> 2001        b_wt -0.3117768282 0.0008242530 #> 2002        b_wt -0.3052701921 0.0007909331 #> 2003        b_wt -0.2987635560 0.0007593122 #> 2004        b_wt -0.2922569199 0.0007294383 #> 2005        b_wt -0.2857502838 0.0007016756 #> 2006        b_wt -0.2792436477 0.0006758452 #> 2007        b_wt -0.2727370116 0.0006518659 #> 2008        b_wt -0.2662303755 0.0006297480 #> 2009        b_wt -0.2597237394 0.0006094946 #> 2010        b_wt -0.2532171033 0.0005911023 #> 2011        b_wt -0.2467104672 0.0005747461 #> 2012        b_wt -0.2402038311 0.0005604449 #> 2013        b_wt -0.2336971950 0.0005479277 #> 2014        b_wt -0.2271905588 0.0005371565 #> 2015        b_wt -0.2206839227 0.0005280875 #> 2016        b_wt -0.2141772866 0.0005206713 #> 2017        b_wt -0.2076706505 0.0005148952 #> 2018        b_wt -0.2011640144 0.0005109422 #> 2019        b_wt -0.1946573783 0.0005084170 #> 2020        b_wt -0.1881507422 0.0005072455 #> 2021        b_wt -0.1816441061 0.0005073497 #> 2022        b_wt -0.1751374700 0.0005086483 #> 2023        b_wt -0.1686308339 0.0005110570 #> 2024        b_wt -0.1621241978 0.0005146520 #> 2025        b_wt -0.1556175617 0.0005191706 #> 2026        b_wt -0.1491109256 0.0005244730 #> 2027        b_wt -0.1426042895 0.0005304650 #> 2028        b_wt -0.1360976534 0.0005370513 #> 2029        b_wt -0.1295910172 0.0005441366 #> 2030        b_wt -0.1230843811 0.0005516544 #> 2031        b_wt -0.1165777450 0.0005594704 #> 2032        b_wt -0.1100711089 0.0005674442 #> 2033        b_wt -0.1035644728 0.0005754842 #> 2034        b_wt -0.0970578367 0.0005835005 #> 2035        b_wt -0.0905512006 0.0005914056 #> 2036        b_wt -0.0840445645 0.0005991030 #> 2037        b_wt -0.0775379284 0.0006064349 #> 2038        b_wt -0.0710312923 0.0006133654 #> 2039        b_wt -0.0645246562 0.0006198231 #> 2040        b_wt -0.0580180201 0.0006257407 #> 2041        b_wt -0.0515113840 0.0006310548 #> 2042        b_wt -0.0450047479 0.0006357066 #> 2043        b_wt -0.0384981117 0.0006394889 #> 2044        b_wt -0.0319914756 0.0006424534 #> 2045        b_wt -0.0254848395 0.0006445892 #> 2046        b_wt -0.0189782034 0.0006458627 #> 2047        b_wt -0.0124715673 0.0006462457 #> 2048        b_wt -0.0059649312 0.0006457149 #> 2049       b_cyl -3.1577054482 0.0016466464 #> 2050       b_cyl -3.1540297081 0.0016853322 #> 2051       b_cyl -3.1503539680 0.0017241907 #> 2052       b_cyl -3.1466782278 0.0017633448 #> 2053       b_cyl -3.1430024877 0.0018028160 #> 2054       b_cyl -3.1393267476 0.0018426669 #> 2055       b_cyl -3.1356510075 0.0018829622 #> 2056       b_cyl -3.1319752674 0.0019237687 #> 2057       b_cyl -3.1282995273 0.0019651544 #> 2058       b_cyl -3.1246237872 0.0020072820 #> 2059       b_cyl -3.1209480471 0.0020502330 #> 2060       b_cyl -3.1172723070 0.0020940103 #> 2061       b_cyl -3.1135965669 0.0021386812 #> 2062       b_cyl -3.1099208268 0.0021843113 #> 2063       b_cyl -3.1062450867 0.0022309646 #> 2064       b_cyl -3.1025693466 0.0022787308 #> 2065       b_cyl -3.0988936065 0.0023278917 #> 2066       b_cyl -3.0952178664 0.0023782821 #> 2067       b_cyl -3.0915421263 0.0024299520 #> 2068       b_cyl -3.0878663862 0.0024829484 #> 2069       b_cyl -3.0841906461 0.0025373148 #> 2070       b_cyl -3.0805149060 0.0025930922 #> 2071       b_cyl -3.0768391659 0.0026505397 #> 2072       b_cyl -3.0731634258 0.0027096005 #> 2073       b_cyl -3.0694876857 0.0027701925 #> 2074       b_cyl -3.0658119456 0.0028323438 #> 2075       b_cyl -3.0621362055 0.0028960811 #> 2076       b_cyl -3.0584604654 0.0029614299 #> 2077       b_cyl -3.0547847253 0.0030284931 #> 2078       b_cyl -3.0511089852 0.0030975228 #> 2079       b_cyl -3.0474332451 0.0031682528 #> 2080       b_cyl -3.0437575050 0.0032407100 #> 2081       b_cyl -3.0400817649 0.0033149228 #> 2082       b_cyl -3.0364060248 0.0033909214 #> 2083       b_cyl -3.0327302847 0.0034687385 #> 2084       b_cyl -3.0290545446 0.0035487358 #> 2085       b_cyl -3.0253788045 0.0036307515 #> 2086       b_cyl -3.0217030644 0.0037147301 #> 2087       b_cyl -3.0180273243 0.0038007205 #> 2088       b_cyl -3.0143515842 0.0038887752 #> 2089       b_cyl -3.0106758441 0.0039789500 #> 2090       b_cyl -3.0070001040 0.0040714620 #> 2091       b_cyl -3.0033243639 0.0041665938 #> 2092       b_cyl -2.9996486238 0.0042640751 #> 2093       b_cyl -2.9959728837 0.0043639767 #> 2094       b_cyl -2.9922971436 0.0044663712 #> 2095       b_cyl -2.9886214035 0.0045713325 #> 2096       b_cyl -2.9849456634 0.0046789356 #> 2097       b_cyl -2.9812699233 0.0047898030 #> 2098       b_cyl -2.9775941832 0.0049035904 #> 2099       b_cyl -2.9739184431 0.0050202837 #> 2100       b_cyl -2.9702427030 0.0051399525 #> 2101       b_cyl -2.9665669629 0.0052626633 #> 2102       b_cyl -2.9628912228 0.0053884793 #> 2103       b_cyl -2.9592154827 0.0055177589 #> 2104       b_cyl -2.9555397426 0.0056507054 #> 2105       b_cyl -2.9518640025 0.0057869425 #> 2106       b_cyl -2.9481882623 0.0059265082 #> 2107       b_cyl -2.9445125222 0.0060694347 #> 2108       b_cyl -2.9408367821 0.0062157485 #> 2109       b_cyl -2.9371610420 0.0063654701 #> 2110       b_cyl -2.9334853019 0.0065193554 #> 2111       b_cyl -2.9298095618 0.0066767009 #> 2112       b_cyl -2.9261338217 0.0068374751 #> 2113       b_cyl -2.9224580816 0.0070016720 #> 2114       b_cyl -2.9187823415 0.0071692823 #> 2115       b_cyl -2.9151066014 0.0073402937 #> 2116       b_cyl -2.9114308613 0.0075150785 #> 2117       b_cyl -2.9077551212 0.0076935960 #> 2118       b_cyl -2.9040793811 0.0078754604 #> 2119       b_cyl -2.9004036410 0.0080606593 #> 2120       b_cyl -2.8967279009 0.0082491839 #> 2121       b_cyl -2.8930521608 0.0084410297 #> 2122       b_cyl -2.8893764207 0.0086362441 #> 2123       b_cyl -2.8857006806 0.0088354973 #> 2124       b_cyl -2.8820249405 0.0090381148 #> 2125       b_cyl -2.8783492004 0.0092441326 #> 2126       b_cyl -2.8746734603 0.0094535977 #> 2127       b_cyl -2.8709977202 0.0096665686 #> 2128       b_cyl -2.8673219801 0.0098831164 #> 2129       b_cyl -2.8636462400 0.0101038504 #> 2130       b_cyl -2.8599704999 0.0103287326 #> 2131       b_cyl -2.8562947598 0.0105575708 #> 2132       b_cyl -2.8526190197 0.0107905068 #> 2133       b_cyl -2.8489432796 0.0110276957 #> 2134       b_cyl -2.8452675395 0.0112693060 #> 2135       b_cyl -2.8415917994 0.0115156982 #> 2136       b_cyl -2.8379160593 0.0117678742 #> 2137       b_cyl -2.8342403192 0.0120251842 #> 2138       b_cyl -2.8305645791 0.0122878533 #> 2139       b_cyl -2.8268888390 0.0125561138 #> 2140       b_cyl -2.8232130989 0.0128302033 #> 2141       b_cyl -2.8195373588 0.0131103639 #> 2142       b_cyl -2.8158616187 0.0133979127 #> 2143       b_cyl -2.8121858786 0.0136925716 #> 2144       b_cyl -2.8085101385 0.0139941700 #> 2145       b_cyl -2.8048343984 0.0143029396 #> 2146       b_cyl -2.8011586583 0.0146191035 #> 2147       b_cyl -2.7974829182 0.0149428746 #> 2148       b_cyl -2.7938071781 0.0152749324 #> 2149       b_cyl -2.7901314380 0.0156163714 #> 2150       b_cyl -2.7864556979 0.0159660454 #> 2151       b_cyl -2.7827799578 0.0163240797 #> 2152       b_cyl -2.7791042177 0.0166905768 #> 2153       b_cyl -2.7754284776 0.0170656149 #> 2154       b_cyl -2.7717527375 0.0174492473 #> 2155       b_cyl -2.7680769974 0.0178430739 #> 2156       b_cyl -2.7644012573 0.0182458443 #> 2157       b_cyl -2.7607255172 0.0186571352 #> 2158       b_cyl -2.7570497771 0.0190768518 #> 2159       b_cyl -2.7533740370 0.0195048696 #> 2160       b_cyl -2.7496982968 0.0199410348 #> 2161       b_cyl -2.7460225567 0.0203857934 #> 2162       b_cyl -2.7423468166 0.0208392906 #> 2163       b_cyl -2.7386710765 0.0213001239 #> 2164       b_cyl -2.7349953364 0.0217679988 #> 2165       b_cyl -2.7313195963 0.0222425981 #> 2166       b_cyl -2.7276438562 0.0227235842 #> 2167       b_cyl -2.7239681161 0.0232106002 #> 2168       b_cyl -2.7202923760 0.0237043099 #> 2169       b_cyl -2.7166166359 0.0242031543 #> 2170       b_cyl -2.7129408958 0.0247066211 #> 2171       b_cyl -2.7092651557 0.0252142984 #> 2172       b_cyl -2.7055894156 0.0257257728 #> 2173       b_cyl -2.7019136755 0.0262406325 #> 2174       b_cyl -2.6982379354 0.0267587227 #> 2175       b_cyl -2.6945621953 0.0272795189 #> 2176       b_cyl -2.6908864552 0.0278022999 #> 2177       b_cyl -2.6872107151 0.0283267141 #> 2178       b_cyl -2.6835349750 0.0288524317 #> 2179       b_cyl -2.6798592349 0.0293791470 #> 2180       b_cyl -2.6761834948 0.0299065826 #> 2181       b_cyl -2.6725077547 0.0304345222 #> 2182       b_cyl -2.6688320146 0.0309626325 #> 2183       b_cyl -2.6651562745 0.0314907764 #> 2184       b_cyl -2.6614805344 0.0320188563 #> 2185       b_cyl -2.6578047943 0.0325468150 #> 2186       b_cyl -2.6541290542 0.0330746368 #> 2187       b_cyl -2.6504533141 0.0336023530 #> 2188       b_cyl -2.6467775740 0.0341301062 #> 2189       b_cyl -2.6431018339 0.0346580740 #> 2190       b_cyl -2.6394260938 0.0351864686 #> 2191       b_cyl -2.6357503537 0.0357155455 #> 2192       b_cyl -2.6320746136 0.0362456024 #> 2193       b_cyl -2.6283988735 0.0367770277 #> 2194       b_cyl -2.6247231334 0.0373106635 #> 2195       b_cyl -2.6210473933 0.0378467076 #> 2196       b_cyl -2.6173716532 0.0383856550 #> 2197       b_cyl -2.6136959131 0.0389280310 #> 2198       b_cyl -2.6100201730 0.0394743894 #> 2199       b_cyl -2.6063444329 0.0400253098 #> 2200       b_cyl -2.6026686928 0.0405823131 #> 2201       b_cyl -2.5989929527 0.0411458750 #> 2202       b_cyl -2.5953172126 0.0417162553 #> 2203       b_cyl -2.5916414725 0.0422941152 #> 2204       b_cyl -2.5879657324 0.0428801205 #> 2205       b_cyl -2.5842899923 0.0434749389 #> 2206       b_cyl -2.5806142522 0.0440797493 #> 2207       b_cyl -2.5769385121 0.0446967493 #> 2208       b_cyl -2.5732627720 0.0453249087 #> 2209       b_cyl -2.5695870319 0.0459648513 #> 2210       b_cyl -2.5659112918 0.0466171824 #> 2211       b_cyl -2.5622355517 0.0472824870 #> 2212       b_cyl -2.5585598116 0.0479613274 #> 2213       b_cyl -2.5548840715 0.0486568059 #> 2214       b_cyl -2.5512083313 0.0493678642 #> 2215       b_cyl -2.5475325912 0.0500942139 #> 2216       b_cyl -2.5438568511 0.0508362674 #> 2217       b_cyl -2.5401811110 0.0515944047 #> 2218       b_cyl -2.5365053709 0.0523689726 #> 2219       b_cyl -2.5328296308 0.0531615179 #> 2220       b_cyl -2.5291538907 0.0539737822 #> 2221       b_cyl -2.5254781506 0.0548034130 #> 2222       b_cyl -2.5218024105 0.0556505868 #> 2223       b_cyl -2.5181266704 0.0565154501 #> 2224       b_cyl -2.5144509303 0.0573981204 #> 2225       b_cyl -2.5107751902 0.0582986879 #> 2226       b_cyl -2.5070994501 0.0592207411 #> 2227       b_cyl -2.5034237100 0.0601613146 #> 2228       b_cyl -2.4997479699 0.0611198856 #> 2229       b_cyl -2.4960722298 0.0620964359 #> 2230       b_cyl -2.4923964897 0.0630909344 #> 2231       b_cyl -2.4887207496 0.0641033390 #> 2232       b_cyl -2.4850450095 0.0651352842 #> 2233       b_cyl -2.4813692694 0.0661873142 #> 2234       b_cyl -2.4776935293 0.0672570529 #> 2235       b_cyl -2.4740177892 0.0683444482 #> 2236       b_cyl -2.4703420491 0.0694494552 #> 2237       b_cyl -2.4666663090 0.0705720384 #> 2238       b_cyl -2.4629905689 0.0717121737 #> 2239       b_cyl -2.4593148288 0.0728736925 #> 2240       b_cyl -2.4556390887 0.0740528984 #> 2241       b_cyl -2.4519633486 0.0752497363 #> 2242       b_cyl -2.4482876085 0.0764642840 #> 2243       b_cyl -2.4446118684 0.0776966456 #> 2244       b_cyl -2.4409361283 0.0789469525 #> 2245       b_cyl -2.4372603882 0.0802175282 #> 2246       b_cyl -2.4335846481 0.0815084870 #> 2247       b_cyl -2.4299089080 0.0828181473 #> 2248       b_cyl -2.4262331679 0.0841468071 #> 2249       b_cyl -2.4225574278 0.0854947993 #> 2250       b_cyl -2.4188816877 0.0868624922 #> 2251       b_cyl -2.4152059476 0.0882506322 #> 2252       b_cyl -2.4115302075 0.0896637877 #> 2253       b_cyl -2.4078544674 0.0910982925 #> 2254       b_cyl -2.4041787273 0.0925547058 #> 2255       b_cyl -2.4005029872 0.0940336232 #> 2256       b_cyl -2.3968272471 0.0955356763 #> 2257       b_cyl -2.3931515070 0.0970615323 #> 2258       b_cyl -2.3894757669 0.0986154877 #> 2259       b_cyl -2.3858000268 0.1001972308 #> 2260       b_cyl -2.3821242867 0.1018054904 #> 2261       b_cyl -2.3784485466 0.1034411087 #> 2262       b_cyl -2.3747728065 0.1051049580 #> 2263       b_cyl -2.3710970664 0.1067979400 #> 2264       b_cyl -2.3674213263 0.1085222150 #> 2265       b_cyl -2.3637455862 0.1102836674 #> 2266       b_cyl -2.3600698461 0.1120777197 #> 2267       b_cyl -2.3563941060 0.1139054001 #> 2268       b_cyl -2.3527183659 0.1157677543 #> 2269       b_cyl -2.3490426257 0.1176658439 #> 2270       b_cyl -2.3453668856 0.1196007440 #> 2271       b_cyl -2.3416911455 0.1215799857 #> 2272       b_cyl -2.3380154054 0.1236012127 #> 2273       b_cyl -2.3343396653 0.1256631689 #> 2274       b_cyl -2.3306639252 0.1277669494 #> 2275       b_cyl -2.3269881851 0.1299136418 #> 2276       b_cyl -2.3233124450 0.1321043230 #> 2277       b_cyl -2.3196367049 0.1343429346 #> 2278       b_cyl -2.3159609648 0.1366355726 #> 2279       b_cyl -2.3122852247 0.1389758725 #> 2280       b_cyl -2.3086094846 0.1413647799 #> 2281       b_cyl -2.3049337445 0.1438031988 #> 2282       b_cyl -2.3012580044 0.1462919869 #> 2283       b_cyl -2.2975822643 0.1488319512 #> 2284       b_cyl -2.2939065242 0.1514336668 #> 2285       b_cyl -2.2902307841 0.1540903488 #> 2286       b_cyl -2.2865550440 0.1568004900 #> 2287       b_cyl -2.2828793039 0.1595645341 #> 2288       b_cyl -2.2792035638 0.1623828375 #> 2289       b_cyl -2.2755278237 0.1652556647 #> 2290       b_cyl -2.2718520836 0.1681878163 #> 2291       b_cyl -2.2681763435 0.1711823293 #> 2292       b_cyl -2.2645006034 0.1742313649 #> 2293       b_cyl -2.2608248633 0.1773346274 #> 2294       b_cyl -2.2571491232 0.1804917012 #> 2295       b_cyl -2.2534733831 0.1837020484 #> 2296       b_cyl -2.2497976430 0.1869650072 #> 2297       b_cyl -2.2461219029 0.1902902607 #> 2298       b_cyl -2.2424461628 0.1936665755 #> 2299       b_cyl -2.2387704227 0.1970919123 #> 2300       b_cyl -2.2350946826 0.2005649495 #> 2301       b_cyl -2.2314189425 0.2040842432 #> 2302       b_cyl -2.2277432024 0.2076482301 #> 2303       b_cyl -2.2240674623 0.2112595667 #> 2304       b_cyl -2.2203917222 0.2149160083 #> 2305       b_cyl -2.2167159821 0.2186105263 #> 2306       b_cyl -2.2130402420 0.2223410240 #> 2307       b_cyl -2.2093645019 0.2261053277 #> 2308       b_cyl -2.2056887618 0.2299011944 #> 2309       b_cyl -2.2020130217 0.2337264878 #> 2310       b_cyl -2.1983372816 0.2375836852 #> 2311       b_cyl -2.1946615415 0.2414639849 #> 2312       b_cyl -2.1909858014 0.2453649674 #> 2313       b_cyl -2.1873100613 0.2492842228 #> 2314       b_cyl -2.1836343212 0.2532193624 #> 2315       b_cyl -2.1799585811 0.2571680304 #> 2316       b_cyl -2.1762828410 0.2611289593 #> 2317       b_cyl -2.1726071009 0.2650986641 #> 2318       b_cyl -2.1689313608 0.2690740654 #> 2319       b_cyl -2.1652556207 0.2730532237 #> 2320       b_cyl -2.1615798806 0.2770343238 #> 2321       b_cyl -2.1579041405 0.2810156855 #> 2322       b_cyl -2.1542284004 0.2849956869 #> 2323       b_cyl -2.1505526602 0.2889721070 #> 2324       b_cyl -2.1468769201 0.2929442193 #> 2325       b_cyl -2.1432011800 0.2969112318 #> 2326       b_cyl -2.1395254399 0.3008725516 #> 2327       b_cyl -2.1358496998 0.3048277888 #> 2328       b_cyl -2.1321739597 0.3087767606 #> 2329       b_cyl -2.1284982196 0.3127186299 #> 2330       b_cyl -2.1248224795 0.3166544311 #> 2331       b_cyl -2.1211467394 0.3205853204 #> 2332       b_cyl -2.1174709993 0.3245122183 #> 2333       b_cyl -2.1137952592 0.3284362474 #> 2334       b_cyl -2.1101195191 0.3323587273 #> 2335       b_cyl -2.1064437790 0.3362812916 #> 2336       b_cyl -2.1027680389 0.3402066365 #> 2337       b_cyl -2.0990922988 0.3441368002 #> 2338       b_cyl -2.0954165587 0.3480739699 #> 2339       b_cyl -2.0917408186 0.3520204558 #> 2340       b_cyl -2.0880650785 0.3559786766 #> 2341       b_cyl -2.0843893384 0.3599511460 #> 2342       b_cyl -2.0807135983 0.3639441274 #> 2343       b_cyl -2.0770378582 0.3679590333 #> 2344       b_cyl -2.0733621181 0.3719977067 #> 2345       b_cyl -2.0696863780 0.3760628364 #> 2346       b_cyl -2.0660106379 0.3801570823 #> 2347       b_cyl -2.0623348978 0.3842830584 #> 2348       b_cyl -2.0586591577 0.3884461218 #> 2349       b_cyl -2.0549834176 0.3926524541 #> 2350       b_cyl -2.0513076775 0.3968990783 #> 2351       b_cyl -2.0476319374 0.4011880488 #> 2352       b_cyl -2.0439561973 0.4055212555 #> 2353       b_cyl -2.0402804572 0.4099004112 #> 2354       b_cyl -2.0366047171 0.4143270387 #> 2355       b_cyl -2.0329289770 0.4188124199 #> 2356       b_cyl -2.0292532369 0.4233493572 #> 2357       b_cyl -2.0255774968 0.4279371394 #> 2358       b_cyl -2.0219017567 0.4325761070 #> 2359       b_cyl -2.0182260166 0.4372663523 #> 2360       b_cyl -2.0145502765 0.4420077163 #> 2361       b_cyl -2.0108745364 0.4468046084 #> 2362       b_cyl -2.0071987963 0.4516573746 #> 2363       b_cyl -2.0035230562 0.4565583467 #> 2364       b_cyl -1.9998473161 0.4615060676 #> 2365       b_cyl -1.9961715760 0.4664988490 #> 2366       b_cyl -1.9924958359 0.4715347787 #> 2367       b_cyl -1.9888200958 0.4766117289 #> 2368       b_cyl -1.9851443557 0.4817349888 #> 2369       b_cyl -1.9814686156 0.4868928721 #> 2370       b_cyl -1.9777928755 0.4920823513 #> 2371       b_cyl -1.9741171354 0.4973004046 #> 2372       b_cyl -1.9704413953 0.5025438885 #> 2373       b_cyl -1.9667656552 0.5078095528 #> 2374       b_cyl -1.9630899151 0.5130957199 #> 2375       b_cyl -1.9594141750 0.5183972082 #> 2376       b_cyl -1.9557384349 0.5237086067 #> 2377       b_cyl -1.9520626947 0.5290264190 #> 2378       b_cyl -1.9483869546 0.5343471614 #> 2379       b_cyl -1.9447112145 0.5396673774 #> 2380       b_cyl -1.9410354744 0.5449834946 #> 2381       b_cyl -1.9373597343 0.5502895998 #> 2382       b_cyl -1.9336839942 0.5555833583 #> 2383       b_cyl -1.9300082541 0.5608617233 #> 2384       b_cyl -1.9263325140 0.5661217727 #> 2385       b_cyl -1.9226567739 0.5713607186 #> 2386       b_cyl -1.9189810338 0.5765759161 #> 2387       b_cyl -1.9153052937 0.5817606582 #> 2388       b_cyl -1.9116295536 0.5869134636 #> 2389       b_cyl -1.9079538135 0.5920345638 #> 2390       b_cyl -1.9042780734 0.5971221809 #> 2391       b_cyl -1.9006023333 0.6021747142 #> 2392       b_cyl -1.8969265932 0.6071907402 #> 2393       b_cyl -1.8932508531 0.6121673820 #> 2394       b_cyl -1.8895751130 0.6170977879 #> 2395       b_cyl -1.8858993729 0.6219881796 #> 2396       b_cyl -1.8822236328 0.6268380161 #> 2397       b_cyl -1.8785478927 0.6316469061 #> 2398       b_cyl -1.8748721526 0.6364146014 #> 2399       b_cyl -1.8711964125 0.6411409894 #> 2400       b_cyl -1.8675206724 0.6458193039 #> 2401       b_cyl -1.8638449323 0.6504541994 #> 2402       b_cyl -1.8601691922 0.6550484643 #> 2403       b_cyl -1.8564934521 0.6596025161 #> 2404       b_cyl -1.8528177120 0.6641168372 #> 2405       b_cyl -1.8491419719 0.6685919647 #> 2406       b_cyl -1.8454662318 0.6730260472 #> 2407       b_cyl -1.8417904917 0.6774163186 #> 2408       b_cyl -1.8381147516 0.6817695979 #> 2409       b_cyl -1.8344390115 0.6860865120 #> 2410       b_cyl -1.8307632714 0.6903676686 #> 2411       b_cyl -1.8270875313 0.6946136477 #> 2412       b_cyl -1.8234117912 0.6988249938 #> 2413       b_cyl -1.8197360511 0.7029958999 #> 2414       b_cyl -1.8160603110 0.7071320595 #> 2415       b_cyl -1.8123845709 0.7112350160 #> 2416       b_cyl -1.8087088308 0.7153049919 #> 2417       b_cyl -1.8050330907 0.7193421302 #> 2418       b_cyl -1.8013573506 0.7233464905 #> 2419       b_cyl -1.7976816105 0.7273151674 #> 2420       b_cyl -1.7940058704 0.7312462638 #> 2421       b_cyl -1.7903301303 0.7351439815 #> 2422       b_cyl -1.7866543902 0.7390079183 #> 2423       b_cyl -1.7829786501 0.7428375853 #> 2424       b_cyl -1.7793029100 0.7466324098 #> 2425       b_cyl -1.7756271699 0.7503917372 #> 2426       b_cyl -1.7719514298 0.7541069114 #> 2427       b_cyl -1.7682756897 0.7577839989 #> 2428       b_cyl -1.7645999496 0.7614225632 #> 2429       b_cyl -1.7609242095 0.7650216138 #> 2430       b_cyl -1.7572484694 0.7685801173 #> 2431       b_cyl -1.7535727292 0.7720970042 #> 2432       b_cyl -1.7498969891 0.7755663059 #> 2433       b_cyl -1.7462212490 0.7789861985 #> 2434       b_cyl -1.7425455089 0.7823604859 #> 2435       b_cyl -1.7388697688 0.7856880650 #> 2436       b_cyl -1.7351940287 0.7889678578 #> 2437       b_cyl -1.7315182886 0.7921988205 #> 2438       b_cyl -1.7278425485 0.7953794987 #> 2439       b_cyl -1.7241668084 0.7984981329 #> 2440       b_cyl -1.7204910683 0.8015646908 #> 2441       b_cyl -1.7168153282 0.8045784712 #> 2442       b_cyl -1.7131395881 0.8075388704 #> 2443       b_cyl -1.7094638480 0.8104453895 #> 2444       b_cyl -1.7057881079 0.8132976416 #> 2445       b_cyl -1.7021123678 0.8160881162 #> 2446       b_cyl -1.6984366277 0.8188189474 #> 2447       b_cyl -1.6947608876 0.8214953276 #> 2448       b_cyl -1.6910851475 0.8241175707 #> 2449       b_cyl -1.6874094074 0.8266861408 #> 2450       b_cyl -1.6837336673 0.8292016538 #> 2451       b_cyl -1.6800579272 0.8316632693 #> 2452       b_cyl -1.6763821871 0.8340640821 #> 2453       b_cyl -1.6727064470 0.8364154512 #> 2454       b_cyl -1.6690307069 0.8387187940 #> 2455       b_cyl -1.6653549668 0.8409756727 #> 2456       b_cyl -1.6616792267 0.8431877901 #> 2457       b_cyl -1.6580034866 0.8453569843 #> 2458       b_cyl -1.6543277465 0.8474793612 #> 2459       b_cyl -1.6506520064 0.8495613821 #> 2460       b_cyl -1.6469762663 0.8516081706 #> 2461       b_cyl -1.6433005262 0.8536221168 #> 2462       b_cyl -1.6396247861 0.8556056786 #> 2463       b_cyl -1.6359490460 0.8575613700 #> 2464       b_cyl -1.6322733059 0.8594905641 #> 2465       b_cyl -1.6285975658 0.8613942476 #> 2466       b_cyl -1.6249218257 0.8632792814 #> 2467       b_cyl -1.6212460856 0.8651482057 #> 2468       b_cyl -1.6175703455 0.8670035084 #> 2469       b_cyl -1.6138946054 0.8688476091 #> 2470       b_cyl -1.6102188653 0.8706828437 #> 2471       b_cyl -1.6065431252 0.8725108118 #> 2472       b_cyl -1.6028673851 0.8743350329 #> 2473       b_cyl -1.5991916450 0.8761575651 #> 2474       b_cyl -1.5955159049 0.8779799943 #> 2475       b_cyl -1.5918401648 0.8798037090 #> 2476       b_cyl -1.5881644247 0.8816298872 #> 2477       b_cyl -1.5844886846 0.8834598089 #> 2478       b_cyl -1.5808129445 0.8852945735 #> 2479       b_cyl -1.5771372044 0.8871338079 #> 2480       b_cyl -1.5734614643 0.8889773745 #> 2481       b_cyl -1.5697857242 0.8908248533 #> 2482       b_cyl -1.5661099841 0.8926755366 #> 2483       b_cyl -1.5624342440 0.8945284254 #> 2484       b_cyl -1.5587585039 0.8963818433 #> 2485       b_cyl -1.5550827638 0.8982333117 #> 2486       b_cyl -1.5514070236 0.9000805793 #> 2487       b_cyl -1.5477312835 0.9019211208 #> 2488       b_cyl -1.5440555434 0.9037521335 #> 2489       b_cyl -1.5403798033 0.9055705441 #> 2490       b_cyl -1.5367040632 0.9073709001 #> 2491       b_cyl -1.5330283231 0.9091473896 #> 2492       b_cyl -1.5293525830 0.9108979834 #> 2493       b_cyl -1.5256768429 0.9126184094 #> 2494       b_cyl -1.5220011028 0.9143042202 #> 2495       b_cyl -1.5183253627 0.9159508103 #> 2496       b_cyl -1.5146496226 0.9175534328 #> 2497       b_cyl -1.5109738825 0.9190945341 #> 2498       b_cyl -1.5072981424 0.9205788176 #> 2499       b_cyl -1.5036224023 0.9220012714 #> 2500       b_cyl -1.4999466622 0.9233568114 #> 2501       b_cyl -1.4962709221 0.9246403639 #> 2502       b_cyl -1.4925951820 0.9258468862 #> 2503       b_cyl -1.4889194419 0.9269604813 #> 2504       b_cyl -1.4852437018 0.9279759428 #> 2505       b_cyl -1.4815679617 0.9288970950 #> 2506       b_cyl -1.4778922216 0.9297194590 #> 2507       b_cyl -1.4742164815 0.9304387348 #> 2508       b_cyl -1.4705407414 0.9310508178 #> 2509       b_cyl -1.4668650013 0.9315493278 #> 2510       b_cyl -1.4631892612 0.9319085419 #> 2511       b_cyl -1.4595135211 0.9321479677 #> 2512       b_cyl -1.4558377810 0.9322647960 #> 2513       b_cyl -1.4521620409 0.9322565083 #> 2514       b_cyl -1.4484863008 0.9321208838 #> 2515       b_cyl -1.4448105607 0.9318560061 #> 2516       b_cyl -1.4411348206 0.9314410338 #> 2517       b_cyl -1.4374590805 0.9308830738 #> 2518       b_cyl -1.4337833404 0.9301919624 #> 2519       b_cyl -1.4301076003 0.9293674267 #> 2520       b_cyl -1.4264318602 0.9284095044 #> 2521       b_cyl -1.4227561201 0.9273185389 #> 2522       b_cyl -1.4190803800 0.9260893861 #> 2523       b_cyl -1.4154046399 0.9247056393 #> 2524       b_cyl -1.4117288998 0.9231927410 #> 2525       b_cyl -1.4080531597 0.9215524937 #> 2526       b_cyl -1.4043774196 0.9197869400 #> 2527       b_cyl -1.4007016795 0.9178983509 #> 2528       b_cyl -1.3970259394 0.9158892135 #> 2529       b_cyl -1.3933501993 0.9137431806 #> 2530       b_cyl -1.3896744592 0.9114773503 #> 2531       b_cyl -1.3859987191 0.9091015938 #> 2532       b_cyl -1.3823229790 0.9066192465 #> 2533       b_cyl -1.3786472389 0.9040337521 #> 2534       b_cyl -1.3749714988 0.9013486478 #> 2535       b_cyl -1.3712957587 0.8985614662 #> 2536       b_cyl -1.3676200186 0.8956690330 #> 2537       b_cyl -1.3639442785 0.8926901947 #> 2538       b_cyl -1.3602685384 0.8896287000 #> 2539       b_cyl -1.3565927983 0.8864882866 #> 2540       b_cyl -1.3529170581 0.8832726697 #> 2541       b_cyl -1.3492413180 0.8799855302 #> 2542       b_cyl -1.3455655779 0.8766185903 #> 2543       b_cyl -1.3418898378 0.8731870351 #> 2544       b_cyl -1.3382140977 0.8696965447 #> 2545       b_cyl -1.3345383576 0.8661504184 #> 2546       b_cyl -1.3308626175 0.8625518522 #> 2547       b_cyl -1.3271868774 0.8589039328 #> 2548       b_cyl -1.3235111373 0.8552058232 #> 2549       b_cyl -1.3198353972 0.8514594941 #> 2550       b_cyl -1.3161596571 0.8476737275 #> 2551       b_cyl -1.3124839170 0.8438509461 #> 2552       b_cyl -1.3088081769 0.8399934349 #> 2553       b_cyl -1.3051324368 0.8361033393 #> 2554       b_cyl -1.3014566967 0.8321826652 #> 2555       b_cyl -1.2977809566 0.8282276895 #> 2556       b_cyl -1.2941052165 0.8242462298 #> 2557       b_cyl -1.2904294764 0.8202400262 #> 2558       b_cyl -1.2867537363 0.8162103637 #> 2559       b_cyl -1.2830779962 0.8121583959 #> 2560       b_cyl -1.2794022561 0.8080851478 #> 2561       b_cyl -1.2757265160 0.8039893424 #> 2562       b_cyl -1.2720507759 0.7998719944 #> 2563       b_cyl -1.2683750358 0.7957359084 #> 2564       b_cyl -1.2646992957 0.7915815062 #> 2565       b_cyl -1.2610235556 0.7874091096 #> 2566       b_cyl -1.2573478155 0.7832189455 #> 2567       b_cyl -1.2536720754 0.7790109501 #> 2568       b_cyl -1.2499963353 0.7747815909 #> 2569       b_cyl -1.2463205952 0.7705345166 #> 2570       b_cyl -1.2426448551 0.7662695515 #> 2571       b_cyl -1.2389691150 0.7619864635 #> 2572       b_cyl -1.2352933749 0.7576849690 #> 2573       b_cyl -1.2316176348 0.7533647384 #> 2574       b_cyl -1.2279418947 0.7490227366 #> 2575       b_cyl -1.2242661546 0.7446592451 #> 2576       b_cyl -1.2205904145 0.7402755304 #> 2577       b_cyl -1.2169146744 0.7358711287 #> 2578       b_cyl -1.2132389343 0.7314455705 #> 2579       b_cyl -1.2095631942 0.7269983854 #> 2580       b_cyl -1.2058874541 0.7225283342 #> 2581       b_cyl -1.2022117140 0.7180312683 #> 2582       b_cyl -1.1985359739 0.7135109939 #> 2583       b_cyl -1.1948602338 0.7089671326 #> 2584       b_cyl -1.1911844937 0.7043993419 #> 2585       b_cyl -1.1875087536 0.6998073190 #> 2586       b_cyl -1.1838330135 0.6951908052 #> 2587       b_cyl -1.1801572734 0.6905456648 #> 2588       b_cyl -1.1764815333 0.6858739863 #> 2589       b_cyl -1.1728057932 0.6811774042 #> 2590       b_cyl -1.1691300531 0.6764559661 #> 2591       b_cyl -1.1654543130 0.6717097903 #> 2592       b_cyl -1.1617785729 0.6669390686 #> 2593       b_cyl -1.1581028328 0.6621427195 #> 2594       b_cyl -1.1544270926 0.6573186337 #> 2595       b_cyl -1.1507513525 0.6524714146 #> 2596       b_cyl -1.1470756124 0.6476016794 #> 2597       b_cyl -1.1433998723 0.6427101323 #> 2598       b_cyl -1.1397241322 0.6377975653 #> 2599       b_cyl -1.1360483921 0.6328648588 #> 2600       b_cyl -1.1323726520 0.6279098296 #> 2601       b_cyl -1.1286969119 0.6229365676 #> 2602       b_cyl -1.1250211718 0.6179471609 #> 2603       b_cyl -1.1213454317 0.6129429471 #> 2604       b_cyl -1.1176696916 0.6079253474 #> 2605       b_cyl -1.1139939515 0.6028958646 #> 2606       b_cyl -1.1103182114 0.5978554507 #> 2607       b_cyl -1.1066424713 0.5928059509 #> 2608       b_cyl -1.1029667312 0.5877506524 #> 2609       b_cyl -1.0992909911 0.5826914346 #> 2610       b_cyl -1.0956152510 0.5776302358 #> 2611       b_cyl -1.0919395109 0.5725690487 #> 2612       b_cyl -1.0882637708 0.5675099164 #> 2613       b_cyl -1.0845880307 0.5624564476 #> 2614       b_cyl -1.0809122906 0.5574106182 #> 2615       b_cyl -1.0772365505 0.5523745115 #> 2616       b_cyl -1.0735608104 0.5473503261 #> 2617       b_cyl -1.0698850703 0.5423402687 #> 2618       b_cyl -1.0662093302 0.5373465480 #> 2619       b_cyl -1.0625335901 0.5323735804 #> 2620       b_cyl -1.0588578500 0.5274248975 #> 2621       b_cyl -1.0551821099 0.5225003215 #> 2622       b_cyl -1.0515063698 0.5176019378 #> 2623       b_cyl -1.0478306297 0.5127317821 #> 2624       b_cyl -1.0441548896 0.5078918332 #> 2625       b_cyl -1.0404791495 0.5030840495 #> 2626       b_cyl -1.0368034094 0.4983184290 #> 2627       b_cyl -1.0331276693 0.4935894480 #> 2628       b_cyl -1.0294519292 0.4888986641 #> 2629       b_cyl -1.0257761891 0.4842475305 #> 2630       b_cyl -1.0221004490 0.4796373911 #> 2631       b_cyl -1.0184247089 0.4750694759 #> 2632       b_cyl -1.0147489688 0.4705504763 #> 2633       b_cyl -1.0110732287 0.4660805798 #> 2634       b_cyl -1.0073974886 0.4616561444 #> 2635       b_cyl -1.0037217485 0.4572777310 #> 2636       b_cyl -1.0000460084 0.4529457636 #> 2637       b_cyl -0.9963702683 0.4486605296 #> 2638       b_cyl -0.9926945282 0.4444233005 #> 2639       b_cyl -0.9890187881 0.4402422907 #> 2640       b_cyl -0.9853430480 0.4361078013 #> 2641       b_cyl -0.9816673079 0.4320194185 #> 2642       b_cyl -0.9779915678 0.4279766041 #> 2643       b_cyl -0.9743158277 0.4239787003 #> 2644       b_cyl -0.9706400876 0.4200249337 #> 2645       b_cyl -0.9669643475 0.4161205798 #> 2646       b_cyl -0.9632886074 0.4122611405 #> 2647       b_cyl -0.9596128673 0.4084421159 #> 2648       b_cyl -0.9559371271 0.4046622505 #> 2649       b_cyl -0.9522613870 0.4009202232 #> 2650       b_cyl -0.9485856469 0.3972146555 #> 2651       b_cyl -0.9449099068 0.3935456616 #> 2652       b_cyl -0.9412341667 0.3899155833 #> 2653       b_cyl -0.9375584266 0.3863166839 #> 2654       b_cyl -0.9338826865 0.3827474458 #> 2655       b_cyl -0.9302069464 0.3792063578 #> 2656       b_cyl -0.9265312063 0.3756919222 #> 2657       b_cyl -0.9228554662 0.3722026625 #> 2658       b_cyl -0.9191797261 0.3687408225 #> 2659       b_cyl -0.9155039860 0.3653018165 #> 2660       b_cyl -0.9118282459 0.3618830820 #> 2661       b_cyl -0.9081525058 0.3584833931 #> 2662       b_cyl -0.9044767657 0.3551015926 #> 2663       b_cyl -0.9008010256 0.3517365960 #> 2664       b_cyl -0.8971252855 0.3483884075 #> 2665       b_cyl -0.8934495454 0.3450570624 #> 2666       b_cyl -0.8897738053 0.3417393800 #> 2667       b_cyl -0.8860980652 0.3384347077 #> 2668       b_cyl -0.8824223251 0.3351424786 #> 2669       b_cyl -0.8787465850 0.3318622110 #> 2670       b_cyl -0.8750708449 0.3285935072 #> 2671       b_cyl -0.8713951048 0.3253381708 #> 2672       b_cyl -0.8677193647 0.3220941518 #> 2673       b_cyl -0.8640436246 0.3188610193 #> 2674       b_cyl -0.8603678845 0.3156387604 #> 2675       b_cyl -0.8566921444 0.3124274215 #> 2676       b_cyl -0.8530164043 0.3092271036 #> 2677       b_cyl -0.8493406642 0.3060390149 #> 2678       b_cyl -0.8456649241 0.3028639211 #> 2679       b_cyl -0.8419891840 0.2997006024 #> 2680       b_cyl -0.8383134439 0.2965493441 #> 2681       b_cyl -0.8346377038 0.2934104460 #> 2682       b_cyl -0.8309619637 0.2902842182 #> 2683       b_cyl -0.8272862236 0.2871709752 #> 2684       b_cyl -0.8236104835 0.2840740033 #> 2685       b_cyl -0.8199347434 0.2809909180 #> 2686       b_cyl -0.8162590033 0.2779218732 #> 2687       b_cyl -0.8125832632 0.2748671192 #> 2688       b_cyl -0.8089075231 0.2718268814 #> 2689       b_cyl -0.8052317830 0.2688013573 #> 2690       b_cyl -0.8015560429 0.2657924430 #> 2691       b_cyl -0.7978803028 0.2628002283 #> 2692       b_cyl -0.7942045627 0.2598231353 #> 2693       b_cyl -0.7905288226 0.2568611883 #> 2694       b_cyl -0.7868530825 0.2539143771 #> 2695       b_cyl -0.7831773424 0.2509826580 #> 2696       b_cyl -0.7795016023 0.2480661606 #> 2697       b_cyl -0.7758258622 0.2451676562 #> 2698       b_cyl -0.7721501221 0.2422838104 #> 2699       b_cyl -0.7684743820 0.2394144331 #> 2700       b_cyl -0.7647986419 0.2365593155 #> 2701       b_cyl -0.7611229018 0.2337182325 #> 2702       b_cyl -0.7574471616 0.2308909457 #> 2703       b_cyl -0.7537714215 0.2280790188 #> 2704       b_cyl -0.7500956814 0.2252814137 #> 2705       b_cyl -0.7464199413 0.2224967005 #> 2706       b_cyl -0.7427442012 0.2197246339 #> 2707       b_cyl -0.7390684611 0.2169649789 #> 2708       b_cyl -0.7353927210 0.2142175136 #> 2709       b_cyl -0.7317169809 0.2114824615 #> 2710       b_cyl -0.7280412408 0.2087613689 #> 2711       b_cyl -0.7243655007 0.2060518491 #> 2712       b_cyl -0.7206897606 0.2033537977 #> 2713       b_cyl -0.7170140205 0.2006671408 #> 2714       b_cyl -0.7133382804 0.1979918358 #> 2715       b_cyl -0.7096625403 0.1953278735 #> 2716       b_cyl -0.7059868002 0.1926771210 #> 2717       b_cyl -0.7023110601 0.1900386036 #> 2718       b_cyl -0.6986353200 0.1874117276 #> 2719       b_cyl -0.6949595799 0.1847966688 #> 2720       b_cyl -0.6912838398 0.1821936389 #> 2721       b_cyl -0.6876080997 0.1796028847 #> 2722       b_cyl -0.6839323596 0.1770254591 #> 2723       b_cyl -0.6802566195 0.1744631970 #> 2724       b_cyl -0.6765808794 0.1719144021 #> 2725       b_cyl -0.6729051393 0.1693794890 #> 2726       b_cyl -0.6692293992 0.1668588995 #> 2727       b_cyl -0.6655536591 0.1643531015 #> 2728       b_cyl -0.6618779190 0.1618625872 #> 2729       b_cyl -0.6582021789 0.1593909163 #> 2730       b_cyl -0.6545264388 0.1569365751 #> 2731       b_cyl -0.6508506987 0.1544994715 #> 2732       b_cyl -0.6471749586 0.1520801910 #> 2733       b_cyl -0.6434992185 0.1496793304 #> 2734       b_cyl -0.6398234784 0.1472974961 #> 2735       b_cyl -0.6361477383 0.1449369839 #> 2736       b_cyl -0.6324719982 0.1425999054 #> 2737       b_cyl -0.6287962581 0.1402840708 #> 2738       b_cyl -0.6251205180 0.1379901008 #> 2739       b_cyl -0.6214447779 0.1357186117 #> 2740       b_cyl -0.6177690378 0.1334702142 #> 2741       b_cyl -0.6140932977 0.1312455115 #> 2742       b_cyl -0.6104175576 0.1290502775 #> 2743       b_cyl -0.6067418175 0.1268806979 #> 2744       b_cyl -0.6030660774 0.1247368564 #> 2745       b_cyl -0.5993903373 0.1226192821 #> 2746       b_cyl -0.5957145972 0.1205284841 #> 2747       b_cyl -0.5920388571 0.1184649497 #> 2748       b_cyl -0.5883631170 0.1164321100 #> 2749       b_cyl -0.5846873769 0.1144309993 #> 2750       b_cyl -0.5810116368 0.1124586589 #> 2751       b_cyl -0.5773358967 0.1105154329 #> 2752       b_cyl -0.5736601566 0.1086016333 #> 2753       b_cyl -0.5699844165 0.1067175379 #> 2754       b_cyl -0.5663086764 0.1048635022 #> 2755       b_cyl -0.5626329363 0.1030463352 #> 2756       b_cyl -0.5589571962 0.1012595398 #> 2757       b_cyl -0.5552814560 0.0995031981 #> 2758       b_cyl -0.5516057159 0.0977773518 #> 2759       b_cyl -0.5479299758 0.0960820019 #> 2760       b_cyl -0.5442542357 0.0944171080 #> 2761       b_cyl -0.5405784956 0.0927864322 #> 2762       b_cyl -0.5369027555 0.0911888466 #> 2763       b_cyl -0.5332270154 0.0896211834 #> 2764       b_cyl -0.5295512753 0.0880831874 #> 2765       b_cyl -0.5258755352 0.0865745632 #> 2766       b_cyl -0.5221997951 0.0850949769 #> 2767       b_cyl -0.5185240550 0.0836447996 #> 2768       b_cyl -0.5148483149 0.0822282984 #> 2769       b_cyl -0.5111725748 0.0808393066 #> 2770       b_cyl -0.5074968347 0.0794773083 #> 2771       b_cyl -0.5038210946 0.0781417585 #> 2772       b_cyl -0.5001453545 0.0768320848 #> 2773       b_cyl -0.4964696144 0.0755476895 #> 2774       b_cyl -0.4927938743 0.0742915083 #> 2775       b_cyl -0.4891181342 0.0730607800 #> 2776       b_cyl -0.4854423941 0.0718529972 #> 2777       b_cyl -0.4817666540 0.0706674698 #> 2778       b_cyl -0.4780909139 0.0695034986 #> 2779       b_cyl -0.4744151738 0.0683603768 #> 2780       b_cyl -0.4707394337 0.0672383357 #> 2781       b_cyl -0.4670636936 0.0661388336 #> 2782       b_cyl -0.4633879535 0.0650576341 #> 2783       b_cyl -0.4597122134 0.0639940358 #> 2784       b_cyl -0.4560364733 0.0629473464 #> 2785       b_cyl -0.4523607332 0.0619168847 #> 2786       b_cyl -0.4486849931 0.0609019824 #> 2787       b_cyl -0.4450092530 0.0599044005 #> 2788       b_cyl -0.4413335129 0.0589215072 #> 2789       b_cyl -0.4376577728 0.0579519461 #> 2790       b_cyl -0.4339820327 0.0569951493 #> 2791       b_cyl -0.4303062926 0.0560505734 #> 2792       b_cyl -0.4266305525 0.0551177000 #> 2793       b_cyl -0.4229548124 0.0541967982 #> 2794       b_cyl -0.4192790723 0.0532881038 #> 2795       b_cyl -0.4156033322 0.0523895083 #> 2796       b_cyl -0.4119275921 0.0515006400 #> 2797       b_cyl -0.4082518520 0.0506211566 #> 2798       b_cyl -0.4045761119 0.0497507458 #> 2799       b_cyl -0.4009003718 0.0488891248 #> 2800       b_cyl -0.3972246317 0.0480376500 #> 2801       b_cyl -0.3935488916 0.0471946408 #> 2802       b_cyl -0.3898731515 0.0463596900 #> 2803       b_cyl -0.3861974114 0.0455326650 #> 2804       b_cyl -0.3825216713 0.0447134599 #> 2805       b_cyl -0.3788459312 0.0439019948 #> 2806       b_cyl -0.3751701911 0.0430989401 #> 2807       b_cyl -0.3714944510 0.0423045431 #> 2808       b_cyl -0.3678187109 0.0415178222 #> 2809       b_cyl -0.3641429708 0.0407388153 #> 2810       b_cyl -0.3604672307 0.0399675788 #> 2811       b_cyl -0.3567914905 0.0392041859 #> 2812       b_cyl -0.3531157504 0.0384487260 #> 2813       b_cyl -0.3494400103 0.0377031012 #> 2814       b_cyl -0.3457642702 0.0369657629 #> 2815       b_cyl -0.3420885301 0.0362367983 #> 2816       b_cyl -0.3384127900 0.0355163532 #> 2817       b_cyl -0.3347370499 0.0348045793 #> 2818       b_cyl -0.3310613098 0.0341016331 #> 2819       b_cyl -0.3273855697 0.0334087557 #> 2820       b_cyl -0.3237098296 0.0327260873 #> 2821       b_cyl -0.3200340895 0.0320528182 #> 2822       b_cyl -0.3163583494 0.0313891028 #> 2823       b_cyl -0.3126826093 0.0307350903 #> 2824       b_cyl -0.3090068692 0.0300909242 #> 2825       b_cyl -0.3053311291 0.0294569062 #> 2826       b_cyl -0.3016553890 0.0288351465 #> 2827       b_cyl -0.2979796489 0.0282236688 #> 2828       b_cyl -0.2943039088 0.0276225657 #> 2829       b_cyl -0.2906281687 0.0270319169 #> 2830       b_cyl -0.2869524286 0.0264517885 #> 2831       b_cyl -0.2832766885 0.0258822328 #> 2832       b_cyl -0.2796009484 0.0253247764 #> 2833       b_cyl -0.2759252083 0.0247788489 #> 2834       b_cyl -0.2722494682 0.0242435400 #> 2835       b_cyl -0.2685737281 0.0237188223 #> 2836       b_cyl -0.2648979880 0.0232046529 #> 2837       b_cyl -0.2612222479 0.0227009733 #> 2838       b_cyl -0.2575465078 0.0222081116 #> 2839       b_cyl -0.2538707677 0.0217274484 #> 2840       b_cyl -0.2501950276 0.0212569303 #> 2841       b_cyl -0.2465192875 0.0207964242 #> 2842       b_cyl -0.2428435474 0.0203457849 #> 2843       b_cyl -0.2391678073 0.0199048555 #> 2844       b_cyl -0.2354920672 0.0194734682 #> 2845       b_cyl -0.2318163271 0.0190529227 #> 2846       b_cyl -0.2281405870 0.0186420156 #> 2847       b_cyl -0.2244648469 0.0182399667 #> 2848       b_cyl -0.2207891068 0.0178465681 #> 2849       b_cyl -0.2171133667 0.0174616075 #> 2850       b_cyl -0.2134376266 0.0170848695 #> 2851       b_cyl -0.2097618865 0.0167166117 #> 2852       b_cyl -0.2060861464 0.0163573133 #> 2853       b_cyl -0.2024104063 0.0160054539 #> 2854       b_cyl -0.1987346662 0.0156608174 #> 2855       b_cyl -0.1950589261 0.0153231906 #> 2856       b_cyl -0.1913831860 0.0149923639 #> 2857       b_cyl -0.1877074459 0.0146681322 #> 2858       b_cyl -0.1840317058 0.0143514280 #> 2859       b_cyl -0.1803559657 0.0140410687 #> 2860       b_cyl -0.1766802256 0.0137366259 #> 2861       b_cyl -0.1730044855 0.0134379278 #> 2862       b_cyl -0.1693287454 0.0131448106 #> 2863       b_cyl -0.1656530053 0.0128571179 #> 2864       b_cyl -0.1619772652 0.0125751315 #> 2865       b_cyl -0.1583015250 0.0122989457 #> 2866       b_cyl -0.1546257849 0.0120277005 #> 2867       b_cyl -0.1509500448 0.0117612817 #> 2868       b_cyl -0.1472743047 0.0114995829 #> 2869       b_cyl -0.1435985646 0.0112425049 #> 2870       b_cyl -0.1399228245 0.0109899557 #> 2871       b_cyl -0.1362470844 0.0107427511 #> 2872       b_cyl -0.1325713443 0.0104999456 #> 2873       b_cyl -0.1288956042 0.0102613995 #> 2874       b_cyl -0.1252198641 0.0100270508 #> 2875       b_cyl -0.1215441240 0.0097968418 #> 2876       b_cyl -0.1178683839 0.0095707189 #> 2877       b_cyl -0.1141926438 0.0093490573 #> 2878       b_cyl -0.1105169037 0.0091318324 #> 2879       b_cyl -0.1068411636 0.0089185278 #> 2880       b_cyl -0.1031654235 0.0087091021 #> 2881       b_cyl -0.0994896834 0.0085035150 #> 2882       b_cyl -0.0958139433 0.0083017266 #> 2883       b_cyl -0.0921382032 0.0081037207 #> 2884       b_cyl -0.0884624631 0.0079102336 #> 2885       b_cyl -0.0847867230 0.0077204037 #> 2886       b_cyl -0.0811109829 0.0075341902 #> 2887       b_cyl -0.0774352428 0.0073515514 #> 2888       b_cyl -0.0737595027 0.0071724449 #> 2889       b_cyl -0.0700837626 0.0069968269 #> 2890       b_cyl -0.0664080225 0.0068250916 #> 2891       b_cyl -0.0627322824 0.0066570526 #> 2892       b_cyl -0.0590565423 0.0064923361 #> 2893       b_cyl -0.0553808022 0.0063308931 #> 2894       b_cyl -0.0517050621 0.0061726738 #> 2895       b_cyl -0.0480293220 0.0060176274 #> 2896       b_cyl -0.0443535819 0.0058657910 #> 2897       b_cyl -0.0406778418 0.0057176048 #> 2898       b_cyl -0.0370021017 0.0055724054 #> 2899       b_cyl -0.0333263616 0.0054301400 #> 2900       b_cyl -0.0296506215 0.0052907563 #> 2901       b_cyl -0.0259748814 0.0051542017 #> 2902       b_cyl -0.0222991413 0.0050204245 #> 2903       b_cyl -0.0186234012 0.0048897777 #> 2904       b_cyl -0.0149476611 0.0047619731 #> 2905       b_cyl -0.0112719210 0.0046367659 #> 2906       b_cyl -0.0075961809 0.0045141087 #> 2907       b_cyl -0.0039204408 0.0043939549 #> 2908       b_cyl -0.0002447007 0.0042762595 #> 2909       b_cyl  0.0034310394 0.0041611007 #> 2910       b_cyl  0.0071067795 0.0040487091 #> 2911       b_cyl  0.0107825196 0.0039386292 #> 2912       b_cyl  0.0144582597 0.0038308250 #> 2913       b_cyl  0.0181339998 0.0037252623 #> 2914       b_cyl  0.0218097399 0.0036219094 #> 2915       b_cyl  0.0254854800 0.0035207365 #> 2916       b_cyl  0.0291612201 0.0034220878 #> 2917       b_cyl  0.0328369602 0.0033256612 #> 2918       b_cyl  0.0365127003 0.0032313315 #> 2919       b_cyl  0.0401884405 0.0031390819 #> 2920       b_cyl  0.0438641806 0.0030488984 #> 2921       b_cyl  0.0475399207 0.0029607695 #> 2922       b_cyl  0.0512156608 0.0028748385 #> 2923       b_cyl  0.0548914009 0.0027912541 #> 2924       b_cyl  0.0585671410 0.0027097089 #> 2925       b_cyl  0.0622428811 0.0026302057 #> 2926       b_cyl  0.0659186212 0.0025527499 #> 2927       b_cyl  0.0695943613 0.0024773492 #> 2928       b_cyl  0.0732701014 0.0024040141 #> 2929       b_cyl  0.0769458415 0.0023331735 #> 2930       b_cyl  0.0806215816 0.0022644919 #> 2931       b_cyl  0.0842973217 0.0021979323 #> 2932       b_cyl  0.0879730618 0.0021335147 #> 2933       b_cyl  0.0916488019 0.0020712601 #> 2934       b_cyl  0.0953245420 0.0020111903 #> 2935       b_cyl  0.0990002821 0.0019535452 #> 2936       b_cyl  0.1026760222 0.0018984251 #> 2937       b_cyl  0.1063517623 0.0018455696 #> 2938       b_cyl  0.1100275024 0.0017949992 #> 2939       b_cyl  0.1137032425 0.0017467327 #> 2940       b_cyl  0.1173789826 0.0017007872 #> 2941       b_cyl  0.1210547227 0.0016571775 #> 2942       b_cyl  0.1247304628 0.0016164361 #> 2943       b_cyl  0.1284062029 0.0015780611 #> 2944       b_cyl  0.1320819430 0.0015420432 #> 2945       b_cyl  0.1357576831 0.0015083780 #> 2946       b_cyl  0.1394334232 0.0014770560 #> 2947       b_cyl  0.1431091633 0.0014480626 #> 2948       b_cyl  0.1467849034 0.0014216491 #> 2949       b_cyl  0.1504606435 0.0013977388 #> 2950       b_cyl  0.1541363836 0.0013760502 #> 2951       b_cyl  0.1578121237 0.0013565369 #> 2952       b_cyl  0.1614878638 0.0013391463 #> 2953       b_cyl  0.1651636039 0.0013238188 #> 2954       b_cyl  0.1688393440 0.0013105243 #> 2955       b_cyl  0.1725150841 0.0012995190 #> 2956       b_cyl  0.1761908242 0.0012903057 #> 2957       b_cyl  0.1798665643 0.0012827921 #> 2958       b_cyl  0.1835423044 0.0012768807 #> 2959       b_cyl  0.1872180445 0.0012724686 #> 2960       b_cyl  0.1908937846 0.0012694483 #> 2961       b_cyl  0.1945695247 0.0012678656 #> 2962       b_cyl  0.1982452648 0.0012674800 #> 2963       b_cyl  0.2019210049 0.0012680664 #> 2964       b_cyl  0.2055967450 0.0012695015 #> 2965       b_cyl  0.2092724851 0.0012716608 #> 2966       b_cyl  0.2129482252 0.0012744197 #> 2967       b_cyl  0.2166239653 0.0012776666 #> 2968       b_cyl  0.2202997054 0.0012812811 #> 2969       b_cyl  0.2239754455 0.0012850590 #> 2970       b_cyl  0.2276511856 0.0012888860 #> 2971       b_cyl  0.2313269257 0.0012926520 #> 2972       b_cyl  0.2350026658 0.0012962520 #> 2973       b_cyl  0.2386784059 0.0012995866 #> 2974       b_cyl  0.2423541461 0.0013024829 #> 2975       b_cyl  0.2460298862 0.0013048701 #> 2976       b_cyl  0.2497056263 0.0013067014 #> 2977       b_cyl  0.2533813664 0.0013079165 #> 2978       b_cyl  0.2570571065 0.0013084638 #> 2979       b_cyl  0.2607328466 0.0013083010 #> 2980       b_cyl  0.2644085867 0.0013073457 #> 2981       b_cyl  0.2680843268 0.0013054988 #> 2982       b_cyl  0.2717600669 0.0013028781 #> 2983       b_cyl  0.2754358070 0.0012994915 #> 2984       b_cyl  0.2791115471 0.0012953562 #> 2985       b_cyl  0.2827872872 0.0012904987 #> 2986       b_cyl  0.2864630273 0.0012849548 #> 2987       b_cyl  0.2901387674 0.0012786648 #> 2988       b_cyl  0.2938145075 0.0012718016 #> 2989       b_cyl  0.2974902476 0.0012644571 #> 2990       b_cyl  0.3011659877 0.0012567066 #> 2991       b_cyl  0.3048417278 0.0012486308 #> 2992       b_cyl  0.3085174679 0.0012403156 #> 2993       b_cyl  0.3121932080 0.0012318503 #> 2994       b_cyl  0.3158689481 0.0012233680 #> 2995       b_cyl  0.3195446882 0.0012149868 #> 2996       b_cyl  0.3232204283 0.0012068064 #> 2997       b_cyl  0.3268961684 0.0011989264 #> 2998       b_cyl  0.3305719085 0.0011914459 #> 2999       b_cyl  0.3342476486 0.0011844624 #> 3000       b_cyl  0.3379233887 0.0011782257 #> 3001       b_cyl  0.3415991288 0.0011727313 #> 3002       b_cyl  0.3452748689 0.0011680531 #> 3003       b_cyl  0.3489506090 0.0011642687 #> 3004       b_cyl  0.3526263491 0.0011614495 #> 3005       b_cyl  0.3563020892 0.0011596607 #> 3006       b_cyl  0.3599778293 0.0011590873 #> 3007       b_cyl  0.3636535694 0.0011598020 #> 3008       b_cyl  0.3673293095 0.0011617124 #> 3009       b_cyl  0.3710050496 0.0011648439 #> 3010       b_cyl  0.3746807897 0.0011692138 #> 3011       b_cyl  0.3783565298 0.0011748306 #> 3012       b_cyl  0.3820322699 0.0011817057 #> 3013       b_cyl  0.3857080100 0.0011900796 #> 3014       b_cyl  0.3893837501 0.0011996572 #> 3015       b_cyl  0.3930594902 0.0012104051 #> 3016       b_cyl  0.3967352303 0.0012222826 #> 3017       b_cyl  0.4004109704 0.0012352423 #> 3018       b_cyl  0.4040867105 0.0012492302 #> 3019       b_cyl  0.4077624506 0.0012643009 #> 3020       b_cyl  0.4114381907 0.0012803192 #> 3021       b_cyl  0.4151139308 0.0012971207 #> 3022       b_cyl  0.4187896709 0.0013146259 #> 3023       b_cyl  0.4224654110 0.0013327523 #> 3024       b_cyl  0.4261411511 0.0013514147 #> 3025       b_cyl  0.4298168912 0.0013705363 #> 3026       b_cyl  0.4334926313 0.0013900525 #> 3027       b_cyl  0.4371683714 0.0014097875 #> 3028       b_cyl  0.4408441116 0.0014296524 #> 3029       b_cyl  0.4445198517 0.0014495593 #> 3030       b_cyl  0.4481955918 0.0014694217 #> 3031       b_cyl  0.4518713319 0.0014891551 #> 3032       b_cyl  0.4555470720 0.0015086259 #> 3033       b_cyl  0.4592228121 0.0015277463 #> 3034       b_cyl  0.4628985522 0.0015464588 #> 3035       b_cyl  0.4665742923 0.0015646940 #> 3036       b_cyl  0.4702500324 0.0015823858 #> 3037       b_cyl  0.4739257725 0.0015994719 #> 3038       b_cyl  0.4776015126 0.0016158536 #> 3039       b_cyl  0.4812772527 0.0016313734 #> 3040       b_cyl  0.4849529928 0.0016460985 #> 3041       b_cyl  0.4886287329 0.0016599859 #> 3042       b_cyl  0.4923044730 0.0016729960 #> 3043       b_cyl  0.4959802131 0.0016850923 #> 3044       b_cyl  0.4996559532 0.0016962416 #> 3045       b_cyl  0.5033316933 0.0017062326 #> 3046       b_cyl  0.5070074334 0.0017151591 #> 3047       b_cyl  0.5106831735 0.0017230445 #> 3048       b_cyl  0.5143589136 0.0017298680 #> 3049       b_cyl  0.5180346537 0.0017356104 #> 3050       b_cyl  0.5217103938 0.0017402540 #> 3051       b_cyl  0.5253861339 0.0017436950 #> 3052       b_cyl  0.5290618740 0.0017458322 #> 3053       b_cyl  0.5327376141 0.0017468170 #> 3054       b_cyl  0.5364133542 0.0017466365 #> 3055       b_cyl  0.5400890943 0.0017452782 #> 3056       b_cyl  0.5437648344 0.0017427298 #> 3057       b_cyl  0.5474405745 0.0017389798 #> 3058       b_cyl  0.5511163146 0.0017337703 #> 3059       b_cyl  0.5547920547 0.0017273022 #> 3060       b_cyl  0.5584677948 0.0017195953 #> 3061       b_cyl  0.5621435349 0.0017106409 #> 3062       b_cyl  0.5658192750 0.0017004313 #> 3063       b_cyl  0.5694950151 0.0016889597 #> 3064       b_cyl  0.5731707552 0.0016760940 #> 3065       b_cyl  0.5768464953 0.0016617976 #> 3066       b_cyl  0.5805222354 0.0016462291 #> 3067       b_cyl  0.5841979755 0.0016293911 #> 3068       b_cyl  0.5878737156 0.0016112882 #> 3069       b_cyl  0.5915494557 0.0015919282 #> 3070       b_cyl  0.5952251958 0.0015713214 #> 3071       b_cyl  0.5989009359 0.0015492126 #> 3072       b_cyl  0.6025766760 0.0015258995 # }"},{"path":"https://easystats.github.io/bayestestR/reference/eti.html","id":null,"dir":"Reference","previous_headings":"","what":"Equal-Tailed Interval (ETI) — eti","title":"Equal-Tailed Interval (ETI) — eti","text":"Compute Equal-Tailed Interval (ETI) posterior distributions using quantiles method. probability interval equal probability . ETI can used context uncertainty characterisation posterior distributions Credible Interval (CI).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/eti.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Equal-Tailed Interval (ETI) — eti","text":"","code":"eti(x, ...)  # S3 method for numeric eti(x, ci = 0.95, verbose = TRUE, ...)  # S3 method for stanreg eti(   x,   ci = 0.95,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"location\", \"all\", \"conditional\", \"smooth_terms\", \"sigma\",     \"distributional\", \"auxiliary\"),   parameters = NULL,   verbose = TRUE,   ... )  # S3 method for brmsfit eti(   x,   ci = 0.95,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"conditional\", \"zi\", \"zero_inflated\", \"all\"),   parameters = NULL,   verbose = TRUE,   ... )"},{"path":"https://easystats.github.io/bayestestR/reference/eti.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Equal-Tailed Interval (ETI) — eti","text":"x Vector representing posterior distribution, data frame vectors. Can also Bayesian model. bayestestR supports wide range models (see, example, methods(\"hdi\")) documented 'Usage' section, methods classes mostly resemble arguments .numeric .data.framemethods. ... Currently used. ci Value vector probability (credible) interval - CI (0 1) estimated. Default .95 (95%). verbose Toggle warnings. effects results fixed effects, random effects returned? applies mixed models. May abbreviated. component results parameters, parameters conditional model zero-inflated part model returned? May abbreviated. applies brms-models. parameters Regular expression pattern describes parameters returned. Meta-parameters (like lp__ prior_) filtered default, parameters typically appear summary() returned. Use parameters select specific parameters output.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/eti.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Equal-Tailed Interval (ETI) — eti","text":"data frame following columns: Parameter model parameter(s), x model-object. x vector, column missing. CI probability credible interval. CI_low, CI_high lower upper credible interval limits parameters.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/eti.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Equal-Tailed Interval (ETI) — eti","text":"Unlike equal-tailed intervals (see eti()) typically exclude 2.5% tail distribution always include median, HDI equal-tailed therefore always includes mode(s) posterior distributions. can useful better represent credibility mass distribution, HDI also limitations. See spi() details.  95% 89% Credible Intervals (CI) two reasonable ranges characterize uncertainty related estimation (see discussion differences two values).  89% intervals (ci = 0.89) deemed stable , instance, 95% intervals (Kruschke, 2014). effective sample size least 10.000 recommended one wants estimate 95% intervals high precision (Kruschke, 2014, p. 183ff). Unfortunately, default number posterior samples Bayes packages (e.g., rstanarm brms) 4.000 (thus, might want increase fitting model). Moreover, 89 indicates arbitrariness interval limits - remarkable property highest prime number exceed already unstable 95% threshold (McElreath, 2015).  However, 95% advantages . instance, shares (case normal posterior distribution) intuitive relationship standard deviation conveys accurate image (artificial) bounds distribution. Also, wider, makes analyses conservative (.e., probability covering 0 larger 95% CI lower ranges 89%), good thing context reproducibility crisis.  95% equal-tailed interval (ETI) 2.5% distribution either side limits. indicates 2.5th percentile 97.5h percentile. symmetric distributions, two methods computing credible intervals, ETI HDI, return similar results.  case skewed distributions. Indeed, possible parameter values ETI lower credibility (less probable) parameter values outside ETI. property seems undesirable summary credible values distribution.  hand, ETI range change transformations applied distribution (instance, log odds scale probabilities): lower higher bounds transformed distribution correspond transformed lower higher bounds original distribution. contrary, applying transformations distribution change resulting HDI.","code":""},{"path":[]},{"path":"https://easystats.github.io/bayestestR/reference/eti.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Equal-Tailed Interval (ETI) — eti","text":"","code":"library(bayestestR)  posterior <- rnorm(1000) eti(posterior) #> 95% ETI: [-1.93, 1.84] eti(posterior, ci = c(0.80, 0.89, 0.95)) #> Equal-Tailed Interval #>  #> 80% ETI       |       89% ETI |       95% ETI #> --------------------------------------------- #> [-1.29, 1.25] | [-1.58, 1.53] | [-1.93, 1.84]  df <- data.frame(replicate(4, rnorm(100))) eti(df) #> Equal-Tailed Interval #>  #> Parameter |       95% ETI #> ------------------------- #> X1        | [-1.93, 2.19] #> X2        | [-1.70, 1.96] #> X3        | [-1.91, 1.63] #> X4        | [-1.87, 1.87] eti(df, ci = c(0.80, 0.89, 0.95)) #> Equal-Tailed Interval #>  #> Parameter |       80% ETI |       89% ETI |       95% ETI #> --------------------------------------------------------- #> X1        | [-1.16, 1.28] | [-1.74, 1.78] | [-1.93, 2.19] #> X2        | [-0.96, 1.62] | [-1.40, 1.70] | [-1.70, 1.96] #> X3        | [-1.07, 0.88] | [-1.52, 1.15] | [-1.91, 1.63] #> X4        | [-1.20, 1.18] | [-1.67, 1.47] | [-1.87, 1.87] # \\dontrun{ library(rstanarm) model <- stan_glm(mpg ~ wt + gear, data = mtcars, chains = 2, iter = 200, refresh = 0) #> Warning: The largest R-hat is 1.06, indicating chains have not mixed. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#r-hat #> Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#bulk-ess #> Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#tail-ess eti(model) #> Equal-Tailed Interval #>  #> Parameter   |        95% ETI | Effects |   Component #> ---------------------------------------------------- #> (Intercept) | [29.80, 50.38] |   fixed | conditional #> wt          | [-6.99, -3.94] |   fixed | conditional #> gear        | [-2.21,  1.24] |   fixed | conditional eti(model, ci = c(0.80, 0.89, 0.95)) #> Equal-Tailed Interval #>  #> Parameter   |        80% ETI |        89% ETI |        95% ETI | Effects |   Component #> -------------------------------------------------------------------------------------- #> (Intercept) | [32.43, 45.66] | [31.18, 47.98] | [29.80, 50.38] |   fixed | conditional #> wt          | [-6.44, -4.62] | [-6.66, -4.35] | [-6.99, -3.94] |   fixed | conditional #> gear        | [-1.52,  0.79] | [-1.88,  0.99] | [-2.21,  1.24] |   fixed | conditional  library(emmeans) eti(emtrends(model, ~1, \"wt\")) #> Equal-Tailed Interval #>  #> Parameter |        95% ETI #> -------------------------- #> overall   | [-6.99, -3.94]  library(brms) model <- brms::brm(mpg ~ wt + cyl, data = mtcars) #> Compiling Stan program... #> Start sampling #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 1e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.029065 seconds (Warm-up) #> Chain 1:                0.033814 seconds (Sampling) #> Chain 1:                0.062879 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 9e-06 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.030175 seconds (Warm-up) #> Chain 2:                0.033059 seconds (Sampling) #> Chain 2:                0.063234 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 7e-06 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.03207 seconds (Warm-up) #> Chain 3:                0.031392 seconds (Sampling) #> Chain 3:                0.063462 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 8e-06 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.030529 seconds (Warm-up) #> Chain 4:                0.024684 seconds (Sampling) #> Chain 4:                0.055213 seconds (Total) #> Chain 4:  eti(model) #> Equal-Tailed Interval #>  #> Parameter   |        95% ETI | Effects |   Component #> ---------------------------------------------------- #> b_Intercept | [36.22, 43.28] |   fixed | conditional #> b_wt        | [-4.81, -1.65] |   fixed | conditional #> b_cyl       | [-2.37, -0.62] |   fixed | conditional eti(model, ci = c(0.80, 0.89, 0.95)) #> Equal-Tailed Interval #>  #> Parameter   |        80% ETI |        89% ETI |        95% ETI | Effects |   Component #> -------------------------------------------------------------------------------------- #> b_Intercept | [37.48, 41.93] | [36.89, 42.57] | [36.22, 43.28] |   fixed | conditional #> b_wt        | [-4.21, -2.19] | [-4.48, -1.93] | [-4.81, -1.65] |   fixed | conditional #> b_cyl       | [-2.07, -0.95] | [-2.21, -0.80] | [-2.37, -0.62] |   fixed | conditional  library(BayesFactor) bf <- ttestBF(x = rnorm(100, 1, 1)) eti(bf) #> Equal-Tailed Interval #>  #> Parameter  |      95% ETI #> ------------------------- #> Difference | [0.85, 1.26] eti(bf, ci = c(0.80, 0.89, 0.95)) #> Equal-Tailed Interval #>  #> Parameter  |      80% ETI |      89% ETI |      95% ETI #> ------------------------------------------------------- #> Difference | [0.92, 1.19] | [0.89, 1.22] | [0.85, 1.26] # }"},{"path":"https://easystats.github.io/bayestestR/reference/hdi.html","id":null,"dir":"Reference","previous_headings":"","what":"Highest Density Interval (HDI) — hdi","title":"Highest Density Interval (HDI) — hdi","text":"Compute Highest Density Interval (HDI) posterior distributions. points within interval higher probability density points outside interval. HDI can used context uncertainty characterisation posterior distributions Credible Interval (CI).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/hdi.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Highest Density Interval (HDI) — hdi","text":"","code":"hdi(x, ...)  # S3 method for numeric hdi(x, ci = 0.95, verbose = TRUE, ...)  # S3 method for data.frame hdi(x, ci = 0.95, verbose = TRUE, ...)  # S3 method for stanreg hdi(   x,   ci = 0.95,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"location\", \"all\", \"conditional\", \"smooth_terms\", \"sigma\",     \"distributional\", \"auxiliary\"),   parameters = NULL,   verbose = TRUE,   ... )  # S3 method for brmsfit hdi(   x,   ci = 0.95,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"conditional\", \"zi\", \"zero_inflated\", \"all\"),   parameters = NULL,   verbose = TRUE,   ... )"},{"path":"https://easystats.github.io/bayestestR/reference/hdi.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Highest Density Interval (HDI) — hdi","text":"x Vector representing posterior distribution, data frame vectors. Can also Bayesian model. bayestestR supports wide range models (see, example, methods(\"hdi\")) documented 'Usage' section, methods classes mostly resemble arguments .numeric .data.framemethods. ... Currently used. ci Value vector probability (credible) interval - CI (0 1) estimated. Default .95 (95%). verbose Toggle warnings. effects results fixed effects, random effects returned? applies mixed models. May abbreviated. component results parameters, parameters conditional model zero-inflated part model returned? May abbreviated. applies brms-models. parameters Regular expression pattern describes parameters returned. Meta-parameters (like lp__ prior_) filtered default, parameters typically appear summary() returned. Use parameters select specific parameters output.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/hdi.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Highest Density Interval (HDI) — hdi","text":"data frame following columns: Parameter model parameter(s), x model-object. x vector, column missing. CI probability credible interval. CI_low, CI_high lower upper credible interval limits parameters.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/hdi.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Highest Density Interval (HDI) — hdi","text":"Unlike equal-tailed intervals (see eti()) typically exclude 2.5% tail distribution always include median, HDI equal-tailed therefore always includes mode(s) posterior distributions. can useful better represent credibility mass distribution, HDI also limitations. See spi() details.  95% 89% Credible Intervals (CI) two reasonable ranges characterize uncertainty related estimation (see discussion differences two values).  89% intervals (ci = 0.89) deemed stable , instance, 95% intervals (Kruschke, 2014). effective sample size least 10.000 recommended one wants estimate 95% intervals high precision (Kruschke, 2014, p. 183ff). Unfortunately, default number posterior samples Bayes packages (e.g., rstanarm brms) 4.000 (thus, might want increase fitting model). Moreover, 89 indicates arbitrariness interval limits - remarkable property highest prime number exceed already unstable 95% threshold (McElreath, 2015).  However, 95% advantages . instance, shares (case normal posterior distribution) intuitive relationship standard deviation conveys accurate image (artificial) bounds distribution. Also, wider, makes analyses conservative (.e., probability covering 0 larger 95% CI lower ranges 89%), good thing context reproducibility crisis.  95% equal-tailed interval (ETI) 2.5% distribution either side limits. indicates 2.5th percentile 97.5h percentile. symmetric distributions, two methods computing credible intervals, ETI HDI, return similar results.  case skewed distributions. Indeed, possible parameter values ETI lower credibility (less probable) parameter values outside ETI. property seems undesirable summary credible values distribution.  hand, ETI range change transformations applied distribution (instance, log odds scale probabilities): lower higher bounds transformed distribution correspond transformed lower higher bounds original distribution. contrary, applying transformations distribution change resulting HDI.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/hdi.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Highest Density Interval (HDI) — hdi","text":"also plot()-method implemented see-package.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/hdi.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Highest Density Interval (HDI) — hdi","text":"Kruschke, J. (2014). Bayesian data analysis: tutorial R, JAGS, Stan. Academic Press. McElreath, R. (2015). Statistical rethinking: Bayesian course examples R Stan. Chapman Hall/CRC.","code":""},{"path":[]},{"path":"https://easystats.github.io/bayestestR/reference/hdi.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Highest Density Interval (HDI) — hdi","text":"Credits go ggdistribute HDInterval.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/hdi.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Highest Density Interval (HDI) — hdi","text":"","code":"library(bayestestR)  posterior <- rnorm(1000) hdi(posterior, ci = 0.89) #>           [,1]     [,2] #> [1,] -2.096071 1.796661 hdi(posterior, ci = c(.80, .90, .95)) #>           [,1]     [,2] #> [1,] -2.096071 1.796661  df <- data.frame(replicate(4, rnorm(100))) hdi(df) #> Error in UseMethod(\"hdi_\"): no applicable method for 'hdi_' applied to an object of class \"data.frame\" hdi(df, ci = c(.80, .90, .95)) #> Error in UseMethod(\"hdi_\"): no applicable method for 'hdi_' applied to an object of class \"data.frame\" # \\dontrun{ library(rstanarm) model <- stan_glm(mpg ~ wt + gear, data = mtcars, chains = 2, iter = 200, refresh = 0) #> Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#bulk-ess #> Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#tail-ess hdi(model) #> Error in UseMethod(\"hdi_\"): no applicable method for 'hdi_' applied to an object of class \"c('stanreg', 'glm', 'lm')\" hdi(model, ci = c(.80, .90, .95)) #> Error in UseMethod(\"hdi_\"): no applicable method for 'hdi_' applied to an object of class \"c('stanreg', 'glm', 'lm')\"  library(emmeans) hdi(emtrends(model, ~1, \"wt\")) #> Error in UseMethod(\"hdi_\"): no applicable method for 'hdi_' applied to an object of class \"emmGrid\"  library(brms) model <- brms::brm(mpg ~ wt + cyl, data = mtcars) #> Compiling Stan program... #> Start sampling #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 1.1e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.033512 seconds (Warm-up) #> Chain 1:                0.030694 seconds (Sampling) #> Chain 1:                0.064206 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 8e-06 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.028687 seconds (Warm-up) #> Chain 2:                0.026231 seconds (Sampling) #> Chain 2:                0.054918 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 7e-06 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.033172 seconds (Warm-up) #> Chain 3:                0.0274 seconds (Sampling) #> Chain 3:                0.060572 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 8e-06 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.030848 seconds (Warm-up) #> Chain 4:                0.02788 seconds (Sampling) #> Chain 4:                0.058728 seconds (Total) #> Chain 4:  hdi(model) #> Error in UseMethod(\"hdi_\"): no applicable method for 'hdi_' applied to an object of class \"brmsfit\" hdi(model, ci = c(.80, .90, .95)) #> Error in UseMethod(\"hdi_\"): no applicable method for 'hdi_' applied to an object of class \"brmsfit\"  library(BayesFactor) bf <- ttestBF(x = rnorm(100, 1, 1)) hdi(bf) #> Error in UseMethod(\"hdi_\"): no applicable method for 'hdi_' applied to an object of class \"c('BFBayesFactor', 'BFOrNULL')\" hdi(bf, ci = c(.80, .90, .95)) #> Error in UseMethod(\"hdi_\"): no applicable method for 'hdi_' applied to an object of class \"c('BFBayesFactor', 'BFOrNULL')\" # }"},{"path":"https://easystats.github.io/bayestestR/reference/map_estimate.html","id":null,"dir":"Reference","previous_headings":"","what":"Maximum A Posteriori probability estimate (MAP) — map_estimate","title":"Maximum A Posteriori probability estimate (MAP) — map_estimate","text":"Find Highest Maximum Posteriori probability estimate (MAP) posterior, .e., value associated highest probability density (\"peak\" posterior distribution). words, estimation mode continuous parameters. Note function relies estimate_density, default uses different smoothing bandwidth (\"SJ\") compared legacy default implemented base R density function (\"nrd0\").","code":""},{"path":"https://easystats.github.io/bayestestR/reference/map_estimate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Maximum A Posteriori probability estimate (MAP) — map_estimate","text":"","code":"map_estimate(x, precision = 2^10, method = \"kernel\", ...)  # S3 method for numeric map_estimate(x, precision = 2^10, method = \"kernel\", ...)  # S3 method for stanreg map_estimate(   x,   precision = 2^10,   method = \"kernel\",   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"location\", \"all\", \"conditional\", \"smooth_terms\", \"sigma\",     \"distributional\", \"auxiliary\"),   parameters = NULL,   ... )  # S3 method for brmsfit map_estimate(   x,   precision = 2^10,   method = \"kernel\",   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"conditional\", \"zi\", \"zero_inflated\", \"all\"),   parameters = NULL,   ... )  # S3 method for data.frame map_estimate(x, precision = 2^10, method = \"kernel\", ...)"},{"path":"https://easystats.github.io/bayestestR/reference/map_estimate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Maximum A Posteriori probability estimate (MAP) — map_estimate","text":"x Vector representing posterior distribution, data frame vectors. Can also Bayesian model. bayestestR supports wide range models (see, example, methods(\"hdi\")) documented 'Usage' section, methods classes mostly resemble arguments .numeric .data.framemethods. precision Number points density data. See n parameter density. method Density estimation method. Can \"kernel\" (default), \"logspline\" \"KernSmooth\". ... Currently used. effects results fixed effects, random effects returned? applies mixed models. May abbreviated. component results parameters, parameters conditional model zero-inflated part model returned? May abbreviated. applies brms-models. parameters Regular expression pattern describes parameters returned. Meta-parameters (like lp__ prior_) filtered default, parameters typically appear summary() returned. Use parameters select specific parameters output.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/map_estimate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Maximum A Posteriori probability estimate (MAP) — map_estimate","text":"numeric value x vector. x model-object, returns data frame following columns: Parameter model parameter(s), x model-object. x vector, column missing. MAP_Estimate MAP estimate posterior model parameter.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/map_estimate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Maximum A Posteriori probability estimate (MAP) — map_estimate","text":"","code":"# \\dontrun{ library(bayestestR)  posterior <- rnorm(10000) map_estimate(posterior) #> MAP Estimate: 0.03  plot(density(posterior)) abline(v = map_estimate(posterior), col = \"red\")   library(rstanarm) model <- rstanarm::stan_glm(mpg ~ wt + cyl, data = mtcars) #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 2.4e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.24 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.055042 seconds (Warm-up) #> Chain 1:                0.05673 seconds (Sampling) #> Chain 1:                0.111772 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 1.3e-05 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.054936 seconds (Warm-up) #> Chain 2:                0.058846 seconds (Sampling) #> Chain 2:                0.113782 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 1.3e-05 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.066393 seconds (Warm-up) #> Chain 3:                0.056864 seconds (Sampling) #> Chain 3:                0.123257 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 1.2e-05 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.052555 seconds (Warm-up) #> Chain 4:                0.05665 seconds (Sampling) #> Chain 4:                0.109205 seconds (Total) #> Chain 4:  map_estimate(model) #> MAP Estimate #>  #> Parameter   | MAP_Estimate #> -------------------------- #> (Intercept) |        39.67 #> wt          |        -3.11 #> cyl         |        -1.52  library(brms) model <- brms::brm(mpg ~ wt + cyl, data = mtcars) #> Compiling Stan program... #> Start sampling #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 1.1e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.031537 seconds (Warm-up) #> Chain 1:                0.037843 seconds (Sampling) #> Chain 1:                0.06938 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 7e-06 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.034379 seconds (Warm-up) #> Chain 2:                0.034177 seconds (Sampling) #> Chain 2:                0.068556 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 8e-06 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.032244 seconds (Warm-up) #> Chain 3:                0.030302 seconds (Sampling) #> Chain 3:                0.062546 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 7e-06 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.029612 seconds (Warm-up) #> Chain 4:                0.025548 seconds (Sampling) #> Chain 4:                0.05516 seconds (Total) #> Chain 4:  map_estimate(model) #> MAP Estimate #>  #> Parameter   | MAP_Estimate #> -------------------------- #> b_Intercept |        39.58 #> b_wt        |        -3.24 #> b_cyl       |        -1.45 # }"},{"path":"https://easystats.github.io/bayestestR/reference/mcse.html","id":null,"dir":"Reference","previous_headings":"","what":"Monte-Carlo Standard Error (MCSE) — mcse","title":"Monte-Carlo Standard Error (MCSE) — mcse","text":"function returns Monte Carlo Standard Error (MCSE).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/mcse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Monte-Carlo Standard Error (MCSE) — mcse","text":"","code":"mcse(model, ...)  # S3 method for stanreg mcse(   model,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"location\", \"all\", \"conditional\", \"smooth_terms\", \"sigma\",     \"distributional\", \"auxiliary\"),   parameters = NULL,   ... )"},{"path":"https://easystats.github.io/bayestestR/reference/mcse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Monte-Carlo Standard Error (MCSE) — mcse","text":"model stanreg, stanfit, brmsfit, blavaan, MCMCglmm object. ... Currently used. effects results fixed effects, random effects returned? applies mixed models. May abbreviated. component results parameters, parameters conditional model zero-inflated part model returned? May abbreviated. applies brms-models. parameters Regular expression pattern describes parameters returned. Meta-parameters (like lp__ prior_) filtered default, parameters typically appear summary() returned. Use parameters select specific parameters output.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/mcse.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Monte-Carlo Standard Error (MCSE) — mcse","text":"Monte Carlo Standard Error (MCSE) another measure accuracy chains. defined standard deviation chains divided effective sample size (formula mcse() Kruschke 2015, p. 187). MCSE “provides quantitative suggestion big estimation noise ”.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/mcse.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Monte-Carlo Standard Error (MCSE) — mcse","text":"Kruschke, J. (2014). Bayesian data analysis: tutorial R, JAGS, Stan. Academic Press.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/mcse.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Monte-Carlo Standard Error (MCSE) — mcse","text":"","code":"# \\dontrun{ library(bayestestR) library(rstanarm)  model <- stan_glm(mpg ~ wt + am, data = mtcars, chains = 1, refresh = 0) mcse(model) #>     Parameter       MCSE #> 1 (Intercept) 0.14420893 #> 2          wt 0.03564543 #> 3          am 0.06916278 # }"},{"path":"https://easystats.github.io/bayestestR/reference/mediation.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary of Bayesian multivariate-response mediation-models — mediation","title":"Summary of Bayesian multivariate-response mediation-models — mediation","text":"mediation() short summary multivariate-response mediation-models, .e. function computes average direct average causal mediation effects multivariate response models.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/mediation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary of Bayesian multivariate-response mediation-models — mediation","text":"","code":"mediation(model, ...)  # S3 method for brmsfit mediation(   model,   treatment,   mediator,   response = NULL,   centrality = \"median\",   ci = 0.95,   method = \"ETI\",   ... )  # S3 method for stanmvreg mediation(   model,   treatment,   mediator,   response = NULL,   centrality = \"median\",   ci = 0.95,   method = \"ETI\",   ... )"},{"path":"https://easystats.github.io/bayestestR/reference/mediation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary of Bayesian multivariate-response mediation-models — mediation","text":"model brmsfit stanmvreg object. ... used. treatment Character, name treatment variable (direct effect) (multivariate response) mediator-model. missing, mediation() tries find treatment variable automatically, however, may fail. mediator Character, name mediator variable (multivariate response) mediator-model. missing, mediation() tries find treatment variable automatically, however, may fail. response named character vector, indicating names response variables used mediation analysis. Usually can NULL, case variables retrieved automatically. NULL, names match names model formulas, names(insight::find_response(model, combine = TRUE)). can useful , instance, mediator variable used predictor different name mediator variable used response. might occur mediator transformed one model, used \"\" response variable model. Example: mediator m used response variable, centered version m_center used mediator variable. second response variable (treatment model, mediator additional predictor), y, transformed. use response like : mediation(model, response = c(m = \"m_center\", y = \"y\")). centrality point-estimates (centrality indices) compute.  Character (vector) list one options: \"median\", \"mean\", \"MAP\" \"\". ci Value vector probability CI (0 1) estimated. Default .95 (95%). method Can 'ETI' (default), 'HDI', 'BCI', 'SPI' 'SI'.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/mediation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summary of Bayesian multivariate-response mediation-models — mediation","text":"data frame direct, indirect, mediator total effect multivariate-response mediation-model, well proportion mediated. effect sizes median values posterior samples (use centrality centrality indices).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/mediation.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Summary of Bayesian multivariate-response mediation-models — mediation","text":"mediation() returns data frame information direct effect (mean value posterior samples treatment outcome model), mediator effect (mean value posterior samples mediator outcome model), indirect effect (mean value multiplication posterior samples mediator outcome model posterior samples treatment mediation model) total effect (mean value sums posterior samples used direct indirect effect). proportion mediated indirect effect divided total effect.  values, 89% credible intervals calculated default. Use ci calculate different interval.  arguments treatment mediator necessarily need specified. missing, mediation() tries find treatment mediator variable automatically. work, specify variables.  direct effect also called average direct effect (ADE), indirect effect also called average causal mediation effects (ACME). See also Tingley et al. 2014 Imai et al. 2010.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/mediation.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Summary of Bayesian multivariate-response mediation-models — mediation","text":".data.frame() method returns posterior samples effects, can used processing different bayestestR package.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/mediation.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Summary of Bayesian multivariate-response mediation-models — mediation","text":"Imai, K., Keele, L. Tingley, D. (2010) General Approach Causal Mediation Analysis, Psychological Methods, Vol. 15, . 4 (December), pp. 309-334. Tingley, D., Yamamoto, T., Hirose, K., Imai, K. Keele, L. (2014). mediation: R package Causal Mediation Analysis, Journal Statistical Software, Vol. 59, . 5, pp. 1-38.","code":""},{"path":[]},{"path":"https://easystats.github.io/bayestestR/reference/mediation.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summary of Bayesian multivariate-response mediation-models — mediation","text":"","code":"# \\dontrun{ library(mediation) #> Loading required package: MASS #>  #> Attaching package: ‘MASS’ #> The following object is masked from ‘package:patchwork’: #>  #>     area #> Loading required package: mvtnorm #> Loading required package: sandwich #> mediation: Causal Mediation Analysis #> Version: 4.5.0 library(brms) library(rstanarm)  # load sample data data(jobs) set.seed(123)  # linear models, for mediation analysis b1 <- lm(job_seek ~ treat + econ_hard + sex + age, data = jobs) b2 <- lm(depress2 ~ treat + job_seek + econ_hard + sex + age, data = jobs) # mediation analysis, for comparison with Stan models m1 <- mediate(b1, b2, sims = 1000, treat = \"treat\", mediator = \"job_seek\")  # Fit Bayesian mediation model in brms f1 <- bf(job_seek ~ treat + econ_hard + sex + age) f2 <- bf(depress2 ~ treat + job_seek + econ_hard + sex + age) m2 <- brm(f1 + f2 + set_rescor(FALSE), data = jobs, cores = 4, refresh = 0) #> Compiling Stan program... #> Start sampling  # Fit Bayesian mediation model in rstanarm m3 <- stan_mvmer(   list(     job_seek ~ treat + econ_hard + sex + age + (1 | occp),     depress2 ~ treat + job_seek + econ_hard + sex + age + (1 | occp)   ),   data = jobs,   cores = 4,   refresh = 0 ) #> Fitting a multivariate glmer model. #>  #> Please note the warmup may be much slower than later iterations!  summary(m1) #>  #> Causal Mediation Analysis  #>  #> Quasi-Bayesian Confidence Intervals #>  #>                Estimate 95% CI Lower 95% CI Upper p-value #> ACME            -0.0157      -0.0387         0.01    0.19 #> ADE             -0.0438      -0.1315         0.04    0.35 #> Total Effect    -0.0595      -0.1530         0.02    0.21 #> Prop. Mediated   0.2137      -2.0277         2.70    0.32 #>  #> Sample Size Used: 899  #>  #>  #> Simulations: 1000  #>  mediation(m2, centrality = \"mean\", ci = 0.95) #> # Causal Mediation Analysis for Stan Model #>  #>   Treatment: treat #>   Mediator : job_seek #>   Response : depress2 #>  #> Effect                 | Estimate |          95% ETI #> ---------------------------------------------------- #> Direct Effect (ADE)    |   -0.039 | [-0.125,  0.048] #> Indirect Effect (ACME) |   -0.016 | [-0.042,  0.009] #> Mediator Effect        |   -0.240 | [-0.294, -0.184] #> Total Effect           |   -0.055 | [-0.146,  0.035] #>  #> Proportion mediated: 28.62% [-219.38%, 276.62%] #>  mediation(m3, centrality = \"mean\", ci = 0.95) #> # Causal Mediation Analysis for Stan Model #>  #>   Treatment: treat #>   Mediator : job_seek #>   Response : depress2 #>  #> Effect                 | Estimate |          95% ETI #> ---------------------------------------------------- #> Direct Effect (ADE)    |   -0.041 | [-0.128,  0.043] #> Indirect Effect (ACME) |   -0.018 | [-0.044,  0.007] #> Mediator Effect        |   -0.241 | [-0.298, -0.184] #> Total Effect           |   -0.059 | [-0.149,  0.030] #>  #> Proportion mediated: 30.34% [-203.49%, 264.17%] #>  # }"},{"path":"https://easystats.github.io/bayestestR/reference/model_to_priors.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert model's posteriors to priors (EXPERIMENTAL) — model_to_priors","title":"Convert model's posteriors to priors (EXPERIMENTAL) — model_to_priors","text":"Convert model's posteriors (normal) priors.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/model_to_priors.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert model's posteriors to priors (EXPERIMENTAL) — model_to_priors","text":"","code":"model_to_priors(model, scale_multiply = 3, ...)"},{"path":"https://easystats.github.io/bayestestR/reference/model_to_priors.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert model's posteriors to priors (EXPERIMENTAL) — model_to_priors","text":"model Bayesian model. scale_multiply SD posterior multiplied amount set prior avoid overly narrow priors. ... arguments insight::get_prior() describe_posterior.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/model_to_priors.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert model's posteriors to priors (EXPERIMENTAL) — model_to_priors","text":"","code":"# \\dontrun{ # brms models # ----------------------------------------------- if (require(\"brms\")) {   formula <- brms::brmsformula(mpg ~ wt + cyl, center = FALSE)    model <- brms::brm(formula, data = mtcars, refresh = 0)   priors <- model_to_priors(model)   priors <- brms::validate_prior(priors, formula, data = mtcars)   priors    model2 <- brms::brm(formula, data = mtcars, prior = priors, refresh = 0) } #> Compiling Stan program... #> Start sampling #> Compiling Stan program... #> Start sampling # }"},{"path":"https://easystats.github.io/bayestestR/reference/overlap.html","id":null,"dir":"Reference","previous_headings":"","what":"Overlap Coefficient — overlap","title":"Overlap Coefficient — overlap","text":"method calculate overlap coefficient two empirical distributions (can used measure similarity two samples).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/overlap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Overlap Coefficient — overlap","text":"","code":"overlap(   x,   y,   method_density = \"kernel\",   method_auc = \"trapezoid\",   precision = 2^10,   extend = TRUE,   extend_scale = 0.1,   ... )"},{"path":"https://easystats.github.io/bayestestR/reference/overlap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Overlap Coefficient — overlap","text":"x Vector x values. y Vector x values. method_density Density estimation method. See estimate_density(). method_auc Area Curve (AUC) estimation method. See area_under_curve(). precision Number points density data. See n parameter density. extend Extend range x axis factor extend_scale. extend_scale Ratio range extend x axis. value 0.1 means x axis extended 1/10 range data. ... Currently used.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/overlap.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Overlap Coefficient — overlap","text":"","code":"library(bayestestR)  x <- distribution_normal(1000, 2, 0.5) y <- distribution_normal(1000, 0, 1)  overlap(x, y) #> # Overlap #>  #> 0.19 plot(overlap(x, y))"},{"path":"https://easystats.github.io/bayestestR/reference/p_direction.html","id":null,"dir":"Reference","previous_headings":"","what":"Probability of Direction (pd) — p_direction","title":"Probability of Direction (pd) — p_direction","text":"Compute Probability Direction (pd, also known Maximum Probability Effect - MPE). varies 50% 100% (.e., 0.5 1) can interpreted probability (expressed percentage) parameter (described posterior distribution) strictly positive negative (whichever probable). mathematically defined proportion posterior distribution median's sign. Although differently expressed, index fairly similar (.e., strongly correlated) frequentist p-value.  Note (rare) cases, especially used model averaged posteriors (see weighted_posteriors() brms::posterior_average), pd can smaller 0.5, reflecting high credibility 0.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/p_direction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Probability of Direction (pd) — p_direction","text":"","code":"p_direction(x, ...)  pd(x, ...)  # S3 method for numeric p_direction(x, method = \"direct\", null = 0, ...)  # S3 method for data.frame p_direction(x, method = \"direct\", null = 0, ...)  # S3 method for MCMCglmm p_direction(x, method = \"direct\", null = 0, ...)  # S3 method for emmGrid p_direction(x, method = \"direct\", null = 0, ...)  # S3 method for stanreg p_direction(   x,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"location\", \"all\", \"conditional\", \"smooth_terms\", \"sigma\",     \"distributional\", \"auxiliary\"),   parameters = NULL,   method = \"direct\",   null = 0,   ... )  # S3 method for brmsfit p_direction(   x,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"conditional\", \"zi\", \"zero_inflated\", \"all\"),   parameters = NULL,   method = \"direct\",   null = 0,   ... )  # S3 method for BFBayesFactor p_direction(x, method = \"direct\", null = 0, ...)"},{"path":"https://easystats.github.io/bayestestR/reference/p_direction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Probability of Direction (pd) — p_direction","text":"x Vector representing posterior distribution. Can also Bayesian model (stanreg, brmsfit BayesFactor). ... Currently used. method Can \"direct\" one methods density estimation, \"kernel\", \"logspline\" \"KernSmooth\". \"direct\" (default), computation based raw ratio samples superior inferior 0. Else, result based Area Curve (AUC) estimated density function. null value considered \"null\" effect. Traditionally 0, also 1 case ratios. effects results fixed effects, random effects returned? applies mixed models. May abbreviated. component results parameters, parameters conditional model zero-inflated part model returned? May abbreviated. applies brms-models. parameters Regular expression pattern describes parameters returned. Meta-parameters (like lp__ prior_) filtered default, parameters typically appear summary() returned. Use parameters select specific parameters output.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/p_direction.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Probability of Direction (pd) — p_direction","text":"Values 0.5 1 corresponding probability direction (pd). Note (rare) cases, especially used model averaged posteriors (see weighted_posteriors() brms::posterior_average), pd can smaller 0.5, reflecting high credibility 0. detect cases, method = \"direct\" must used.","code":""},{"path":[]},{"path":"https://easystats.github.io/bayestestR/reference/p_direction.html","id":"what-is-the-pd-","dir":"Reference","previous_headings":"","what":"What is the pd?","title":"Probability of Direction (pd) — p_direction","text":"Probability Direction (pd) index effect existence, ranging 50% 100%, representing certainty effect goes particular direction (.e., positive negative). Beyond simplicity interpretation, understanding computation, index also presents interesting properties: independent model: solely based posterior distributions require additional information data model. robust scale response variable predictors. strongly correlated frequentist p-value, can thus used draw parallels give reference readers non-familiar Bayesian statistics.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/p_direction.html","id":"relationship-with-the-p-value","dir":"Reference","previous_headings":"","what":"Relationship with the p-value","title":"Probability of Direction (pd) — p_direction","text":"cases, seems pd direct correspondence frequentist one-sided p-value formula pone sided = 1 - p(d)/100 two-sided p-value (commonly reported one) formula ptwo sided = 2 * (1 - p(d)/100). Thus, two-sided p-value respectively .1, .05, .01 .001 correspond approximately pd 95%, 97.5%, 99.5% 99.95%. See also pd_to_p().","code":""},{"path":"https://easystats.github.io/bayestestR/reference/p_direction.html","id":"methods-of-computation","dir":"Reference","previous_headings":"","what":"Methods of computation","title":"Probability of Direction (pd) — p_direction","text":"simple direct way compute pd 1) look median's sign, 2) select portion posterior sign 3) compute percentage portion represents. \"simple\" method straightforward, precision directly tied number posterior draws. second approach relies density estimation. starts estimating density function (many methods available), computing area curve (AUC) density curve side 0.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/p_direction.html","id":"strengths-and-limitations","dir":"Reference","previous_headings":"","what":"Strengths and Limitations","title":"Probability of Direction (pd) — p_direction","text":"Strengths: Straightforward computation interpretation. Objective property posterior distribution. 1:1 correspondence frequentist p-value. Limitations: Limited information favoring null hypothesis.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/p_direction.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Probability of Direction (pd) — p_direction","text":"also plot()-method implemented see-package.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/p_direction.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Probability of Direction (pd) — p_direction","text":"Makowski D, Ben-Shachar MS, Chen SHA, Lüdecke D (2019) Indices Effect Existence Significance Bayesian Framework. Frontiers Psychology 2019;10:2767. doi:10.3389/fpsyg.2019.02767","code":""},{"path":[]},{"path":"https://easystats.github.io/bayestestR/reference/p_direction.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Probability of Direction (pd) — p_direction","text":"","code":"library(bayestestR)  # Simulate a posterior distribution of mean 1 and SD 1 # ---------------------------------------------------- posterior <- rnorm(1000, mean = 1, sd = 1) p_direction(posterior) #> Probability of Direction: 0.87 p_direction(posterior, method = \"kernel\") #> Probability of Direction: 0.85  # Simulate a dataframe of posterior distributions # ----------------------------------------------- df <- data.frame(replicate(4, rnorm(100))) p_direction(df) #> Probability of Direction #>  #> Parameter |     pd #> ------------------ #> X1        | 52.00% #> X2        | 56.00% #> X3        | 61.00% #> X4        | 59.00% p_direction(df, method = \"kernel\") #> Probability of Direction #>  #> Parameter |     pd #> ------------------ #> X1        | 50.98% #> X2        | 51.95% #> X3        | 58.20% #> X4        | 56.33% # \\dontrun{ # rstanarm models # ----------------------------------------------- if (require(\"rstanarm\")) {   model <- rstanarm::stan_glm(mpg ~ wt + cyl,     data = mtcars,     chains = 2, refresh = 0   )   p_direction(model)   p_direction(model, method = \"kernel\") } #> Probability of Direction  #>  #> Parameter   |   pd #> ------------------ #> (Intercept) | 100% #> wt          | 100% #> cyl         | 100%  # emmeans # ----------------------------------------------- if (require(\"emmeans\")) {   p_direction(emtrends(model, ~1, \"wt\")) } #> Probability of Direction #>  #> Parameter |   pd #> ---------------- #> overall   | 100%  # brms models # ----------------------------------------------- if (require(\"brms\")) {   model <- brms::brm(mpg ~ wt + cyl, data = mtcars)   p_direction(model)   p_direction(model, method = \"kernel\") } #> Compiling Stan program... #> Start sampling #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 1.1e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.028647 seconds (Warm-up) #> Chain 1:                0.028816 seconds (Sampling) #> Chain 1:                0.057463 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 7e-06 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.030673 seconds (Warm-up) #> Chain 2:                0.03236 seconds (Sampling) #> Chain 2:                0.063033 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 7e-06 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.028591 seconds (Warm-up) #> Chain 3:                0.02869 seconds (Sampling) #> Chain 3:                0.057281 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 7e-06 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.030041 seconds (Warm-up) #> Chain 4:                0.02733 seconds (Sampling) #> Chain 4:                0.057371 seconds (Total) #> Chain 4:  #> Probability of Direction  #>  #> Parameter   |   pd #> ------------------ #> (Intercept) | 100% #> wt          | 100% #> cyl         | 100%  # BayesFactor objects # ----------------------------------------------- if (require(\"BayesFactor\")) {   bf <- ttestBF(x = rnorm(100, 1, 1))   p_direction(bf)   p_direction(bf, method = \"kernel\") } #> Probability of Direction #>  #> Parameter  |   pd #> ----------------- #> Difference | 100% # }"},{"path":"https://easystats.github.io/bayestestR/reference/p_map.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayesian p-value based on the density at the Maximum A Posteriori (MAP) — p_map","title":"Bayesian p-value based on the density at the Maximum A Posteriori (MAP) — p_map","text":"Compute Bayesian equivalent p-value, related odds parameter (described posterior distribution) null hypothesis (h0) using Mills' (2014, 2017) Objective Bayesian Hypothesis Testing framework. corresponds density value null (e.g., 0) divided density Maximum Posteriori (MAP).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/p_map.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayesian p-value based on the density at the Maximum A Posteriori (MAP) — p_map","text":"","code":"p_map(x, null = 0, precision = 2^10, method = \"kernel\", ...)  p_pointnull(x, null = 0, precision = 2^10, method = \"kernel\", ...)  # S3 method for stanreg p_map(   x,   null = 0,   precision = 2^10,   method = \"kernel\",   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"location\", \"all\", \"conditional\", \"smooth_terms\", \"sigma\",     \"distributional\", \"auxiliary\"),   parameters = NULL,   ... )  # S3 method for brmsfit p_map(   x,   null = 0,   precision = 2^10,   method = \"kernel\",   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"conditional\", \"zi\", \"zero_inflated\", \"all\"),   parameters = NULL,   ... )"},{"path":"https://easystats.github.io/bayestestR/reference/p_map.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayesian p-value based on the density at the Maximum A Posteriori (MAP) — p_map","text":"x Vector representing posterior distribution, data frame vectors. Can also Bayesian model. bayestestR supports wide range models (see, example, methods(\"hdi\")) documented 'Usage' section, methods classes mostly resemble arguments .numeric .data.framemethods. null value considered \"null\" effect. Traditionally 0, also 1 case ratios. precision Number points density data. See n parameter density. method Density estimation method. Can \"kernel\" (default), \"logspline\" \"KernSmooth\". ... Currently used. effects results fixed effects, random effects returned? applies mixed models. May abbreviated. component results parameters, parameters conditional model zero-inflated part model returned? May abbreviated. applies brms-models. parameters Regular expression pattern describes parameters returned. Meta-parameters (like lp__ prior_) filtered default, parameters typically appear summary() returned. Use parameters select specific parameters output.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/p_map.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bayesian p-value based on the density at the Maximum A Posteriori (MAP) — p_map","text":"Note method sensitive density estimation method (see section examples ).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/p_map.html","id":"strengths-and-limitations","dir":"Reference","previous_headings":"","what":"Strengths and Limitations","title":"Bayesian p-value based on the density at the Maximum A Posteriori (MAP) — p_map","text":"Strengths: Straightforward computation. Objective property posterior distribution. Limitations: Limited information favoring null hypothesis. Relates density approximation. Indirect relationship mathematical definition interpretation. suitable weak / diffused priors.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/p_map.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Bayesian p-value based on the density at the Maximum A Posteriori (MAP) — p_map","text":"Makowski D, Ben-Shachar MS, Chen SHA, Lüdecke D (2019) Indices Effect Existence Significance Bayesian Framework. Frontiers Psychology 2019;10:2767. doi:10.3389/fpsyg.2019.02767 Mills, J. . (2018). Objective Bayesian Precise Hypothesis Testing. University Cincinnati.","code":""},{"path":[]},{"path":"https://easystats.github.io/bayestestR/reference/p_map.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bayesian p-value based on the density at the Maximum A Posteriori (MAP) — p_map","text":"","code":"library(bayestestR)  p_map(rnorm(1000, 0, 1)) #> MAP-based p-value: 0.99 p_map(rnorm(1000, 10, 1)) #> MAP-based p-value: 0.00 # \\dontrun{ library(rstanarm) model <- stan_glm(mpg ~ wt + gear, data = mtcars, chains = 2, iter = 200, refresh = 0) #> Warning: The largest R-hat is 1.09, indicating chains have not mixed. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#r-hat #> Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#bulk-ess #> Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#tail-ess p_map(model) #> MAP-based p-value  #>  #> Parameter   | p (MAP) #> --------------------- #> (Intercept) |  < .001 #> wt          |  < .001 #> gear        |  0.979   library(emmeans) p_map(emtrends(model, ~1, \"wt\")) #> MAP-based p-value #>  #> Parameter | p (MAP) #> ------------------- #> overall   |  < .001  library(brms) model <- brms::brm(mpg ~ wt + cyl, data = mtcars) #> Compiling Stan program... #> Start sampling #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 1.4e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.030447 seconds (Warm-up) #> Chain 1:                0.027203 seconds (Sampling) #> Chain 1:                0.05765 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 7e-06 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.028982 seconds (Warm-up) #> Chain 2:                0.030767 seconds (Sampling) #> Chain 2:                0.059749 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 6e-06 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.03082 seconds (Warm-up) #> Chain 3:                0.032409 seconds (Sampling) #> Chain 3:                0.063229 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 6e-06 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.02934 seconds (Warm-up) #> Chain 4:                0.027352 seconds (Sampling) #> Chain 4:                0.056692 seconds (Total) #> Chain 4:  p_map(model) #> MAP-based p-value  #>  #> Parameter   | p (MAP) #> --------------------- #> (Intercept) |  < .001 #> wt          |  < .001 #> cyl         |  0.008   library(BayesFactor) bf <- ttestBF(x = rnorm(100, 1, 1)) p_map(bf) #> MAP-based p-value #>  #> Parameter  | p (MAP) #> -------------------- #> Difference |  < .001  # --------------------------------------- # Robustness to density estimation method set.seed(333) data <- data.frame() for (iteration in 1:250) {   x <- rnorm(1000, 1, 1)   result <- data.frame(     \"Kernel\" = p_map(x, method = \"kernel\"),     \"KernSmooth\" = p_map(x, method = \"KernSmooth\"),     \"logspline\" = p_map(x, method = \"logspline\")   )   data <- rbind(data, result) } data$KernSmooth <- data$Kernel - data$KernSmooth data$logspline <- data$Kernel - data$logspline  summary(data$KernSmooth) #>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max.  #> -0.039649 -0.007867 -0.003854 -0.005315 -0.001114  0.056255  summary(data$logspline) #>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max.  #> -0.092188 -0.008992  0.022235  0.026989  0.066329  0.166891  boxplot(data[c(\"KernSmooth\", \"logspline\")])  # }"},{"path":"https://easystats.github.io/bayestestR/reference/p_rope.html","id":null,"dir":"Reference","previous_headings":"","what":"Probability of being in the ROPE — p_rope","title":"Probability of being in the ROPE — p_rope","text":"Compute proportion whole posterior distribution lie within region practical equivalence (ROPE). equivalent running rope(..., ci = 1).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/p_rope.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Probability of being in the ROPE — p_rope","text":"","code":"p_rope(x, ...)  # S3 method for numeric p_rope(x, range = \"default\", ...)  # S3 method for stanreg p_rope(   x,   range = \"default\",   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"location\", \"all\", \"conditional\", \"smooth_terms\", \"sigma\",     \"distributional\", \"auxiliary\"),   parameters = NULL,   ... )  # S3 method for brmsfit p_rope(   x,   range = \"default\",   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"conditional\", \"zi\", \"zero_inflated\", \"all\"),   parameters = NULL,   ... )"},{"path":"https://easystats.github.io/bayestestR/reference/p_rope.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Probability of being in the ROPE — p_rope","text":"x Vector representing posterior distribution. Can also stanreg brmsfit model. ... Currently used. range ROPE's lower higher bounds. \"default\" depending number outcome variables vector list. models one response, range vector length two (e.g., c(-0.1, 0.1)). multivariate models, range list numeric vectors response variable. Vector names correspond name response variables. \"default\" input vector, range set c(-0.1, 0.1). \"default\" input Bayesian model, rope_range() used. effects results fixed effects, random effects returned? applies mixed models. May abbreviated. component results parameters, parameters conditional model zero-inflated part model returned? May abbreviated. applies brms-models. parameters Regular expression pattern describes parameters returned. Meta-parameters (like lp__ prior_) filtered default, parameters typically appear summary() returned. Use parameters select specific parameters output.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/p_rope.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Probability of being in the ROPE — p_rope","text":"","code":"library(bayestestR)  p_rope(x = rnorm(1000, 0, 0.01), range = c(-0.1, 0.1)) #> Proportion of samples inside the ROPE [-0.10, 0.10]: > .999 p_rope(x = mtcars, range = c(-0.1, 0.1)) #> Proportion of samples inside the ROPE [-0.10, 0.10] #>  #> Parameter | p (ROPE) #> -------------------- #> mpg       |   < .001 #> cyl       |   < .001 #> disp      |   < .001 #> hp        |   < .001 #> drat      |   < .001 #> wt        |   < .001 #> qsec      |   < .001 #> vs        |   0.562  #> am        |   0.594  #> gear      |   < .001 #> carb      |   < .001"},{"path":"https://easystats.github.io/bayestestR/reference/p_significance.html","id":null,"dir":"Reference","previous_headings":"","what":"Practical Significance (ps) — p_significance","title":"Practical Significance (ps) — p_significance","text":"Compute probability Practical Significance (ps), can conceptualized unidirectional equivalence test. returns probability effect given threshold corresponding negligible effect median's direction. Mathematically, defined proportion posterior distribution median sign threshold.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/p_significance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Practical Significance (ps) — p_significance","text":"","code":"p_significance(x, ...)  # S3 method for numeric p_significance(x, threshold = \"default\", ...)  # S3 method for stanreg p_significance(   x,   threshold = \"default\",   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"location\", \"all\", \"conditional\", \"smooth_terms\", \"sigma\",     \"distributional\", \"auxiliary\"),   parameters = NULL,   verbose = TRUE,   ... )  # S3 method for brmsfit p_significance(   x,   threshold = \"default\",   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"conditional\", \"zi\", \"zero_inflated\", \"all\"),   parameters = NULL,   verbose = TRUE,   ... )"},{"path":"https://easystats.github.io/bayestestR/reference/p_significance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Practical Significance (ps) — p_significance","text":"x Vector representing posterior distribution. Can also stanreg brmsfit model. ... Currently used. threshold threshold value separates significant negligible effect. \"default\", range set 0.1 input vector, based rope_range() Bayesian model provided. effects results fixed effects, random effects returned? applies mixed models. May abbreviated. component results parameters, parameters conditional model zero-inflated part model returned? May abbreviated. applies brms-models. parameters Regular expression pattern describes parameters returned. Meta-parameters (like lp__ prior_) filtered default, parameters typically appear summary() returned. Use parameters select specific parameters output. verbose Toggle warnings.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/p_significance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Practical Significance (ps) — p_significance","text":"Values 0 1 corresponding probability practical significance (ps).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/p_significance.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Practical Significance (ps) — p_significance","text":"p_significance() returns proportion probability distribution (x) outside certain range (negligible effect, ROPE, see argument threshold). values distribution ROPE, p_significance() returns higher probability value outside ROPE. Typically, value larger 0.5 indicate practical significance. However, range negligible effect rather large compared range probability distribution x, p_significance() less 0.5, indicates clear practical significance.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/p_significance.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Practical Significance (ps) — p_significance","text":"also plot()-method implemented see-package.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/p_significance.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Practical Significance (ps) — p_significance","text":"","code":"library(bayestestR)  # Simulate a posterior distribution of mean 1 and SD 1 # ---------------------------------------------------- posterior <- rnorm(1000, mean = 1, sd = 1) p_significance(posterior) #> Practical Significance (threshold: 0.10): 0.80  # Simulate a dataframe of posterior distributions # ----------------------------------------------- df <- data.frame(replicate(4, rnorm(100))) p_significance(df) #> Practical Significance (threshold: 0.10) #>  #> Parameter |   ps #> ---------------- #> X1        | 0.51 #> X2        | 0.55 #> X3        | 0.47 #> X4        | 0.47 # \\dontrun{ # rstanarm models # ----------------------------------------------- if (require(\"rstanarm\")) {   model <- rstanarm::stan_glm(mpg ~ wt + cyl,     data = mtcars,     chains = 2, refresh = 0   )   p_significance(model) } #> Practical Significance (threshold: 0.60)  #>  #> Parameter   |   ps #> ------------------ #> (Intercept) | 1.00 #> wt          | 1.00 #> cyl         | 0.98 # }"},{"path":"https://easystats.github.io/bayestestR/reference/p_to_bf.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert p-values to (pseudo) Bayes Factors — p_to_bf","title":"Convert p-values to (pseudo) Bayes Factors — p_to_bf","text":"Convert p-values (pseudo) Bayes Factors. transformation suggested Wagenmakers (2022), based vast amount assumptions. might therefore reliable. Use risks. accurate approximate Bayes factors, use bic_to_bf() instead.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/p_to_bf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert p-values to (pseudo) Bayes Factors — p_to_bf","text":"","code":"p_to_bf(x, log = FALSE, ...)  # S3 method for numeric p_to_bf(x, log = FALSE, n_obs = NULL, ...)  # S3 method for default p_to_bf(x, log = FALSE, ...)"},{"path":"https://easystats.github.io/bayestestR/reference/p_to_bf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert p-values to (pseudo) Bayes Factors — p_to_bf","text":"x (frequentist) model object, (numeric) vector p-values. log Wether return log Bayes Factors. Note: print() method always shows BF - \"log_BF\" column accessible returned data frame. ... arguments passed (used now). n_obs Number observations. Either length 1, length p.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/p_to_bf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert p-values to (pseudo) Bayes Factors — p_to_bf","text":"data frame p-values pseudo-Bayes factors (null).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/p_to_bf.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Convert p-values to (pseudo) Bayes Factors — p_to_bf","text":"Wagenmakers, E.J. (2022). Approximate objective Bayes factors p-values sample size: 3p(sqrt(n)) rule. Preprint available ArXiv: https://psyarxiv.com/egydq","code":""},{"path":[]},{"path":"https://easystats.github.io/bayestestR/reference/p_to_bf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert p-values to (pseudo) Bayes Factors — p_to_bf","text":"","code":"if (requireNamespace(\"parameters\", quietly = TRUE)) {   data(iris)   model <- lm(Petal.Length ~ Sepal.Length + Species, data = iris)   p_to_bf(model)    # Examples that demonstrate comparison between   # BIC-approximated and pseudo BF   # --------------------------------------------   m0 <- lm(mpg ~ 1, mtcars)   m1 <- lm(mpg ~ am, mtcars)   m2 <- lm(mpg ~ factor(cyl), mtcars)    # In this first example, BIC-approximated BF and   # pseudo-BF based on p-values are close...    # BIC-approximated BF, m1 against null model   bic_to_bf(BIC(m1), denominator = BIC(m0))    # pseudo-BF based on p-values - dropping intercept   p_to_bf(m1)[-1, ]    # The second example shows that results from pseudo-BF are less accurate   # and should be handled wit caution!   bic_to_bf(BIC(m2), denominator = BIC(m0))   p_to_bf(anova(m2), n_obs = nrow(mtcars)) } #> Pseudo-BF (against NULL) #>  #> Parameter   |      p |       BF #> ------------------------------- #> factor(cyl) | < .001 | 1.18e+07"},{"path":"https://easystats.github.io/bayestestR/reference/pd_to_p.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert between Probability of Direction (pd) and p-value. — pd_to_p","title":"Convert between Probability of Direction (pd) and p-value. — pd_to_p","text":"Enables conversion Probability Direction (pd) p-value.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/pd_to_p.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert between Probability of Direction (pd) and p-value. — pd_to_p","text":"","code":"pd_to_p(pd, direction = \"two-sided\", ...)  p_to_pd(p, direction = \"two-sided\", ...)  convert_p_to_pd(p, direction = \"two-sided\", ...)  convert_pd_to_p(pd, direction = \"two-sided\", ...)"},{"path":"https://easystats.github.io/bayestestR/reference/pd_to_p.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert between Probability of Direction (pd) and p-value. — pd_to_p","text":"pd Probability Direction (pd) value (0 1). direction type p-value requested provided. Can \"two-sided\" (default, two tailed) \"one-sided\" (one tailed). ... Arguments passed methods. p p-value.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/pd_to_p.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert between Probability of Direction (pd) and p-value. — pd_to_p","text":"","code":"pd_to_p(pd = 0.95) #> [1] 0.1 pd_to_p(pd = 0.95, direction = \"one-sided\") #> [1] 0.05"},{"path":"https://easystats.github.io/bayestestR/reference/point_estimate.html","id":null,"dir":"Reference","previous_headings":"","what":"Point-estimates of posterior distributions — point_estimate","title":"Point-estimates of posterior distributions — point_estimate","text":"Compute various point-estimates, mean, median MAP, describe posterior distributions.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/point_estimate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Point-estimates of posterior distributions — point_estimate","text":"","code":"point_estimate(x, ...)  # S3 method for numeric point_estimate(x, centrality = \"all\", dispersion = FALSE, threshold = 0.1, ...)  # S3 method for stanreg point_estimate(   x,   centrality = \"all\",   dispersion = FALSE,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"location\", \"all\", \"conditional\", \"smooth_terms\", \"sigma\",     \"distributional\", \"auxiliary\"),   parameters = NULL,   ... )  # S3 method for brmsfit point_estimate(   x,   centrality = \"all\",   dispersion = FALSE,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"conditional\", \"zi\", \"zero_inflated\", \"all\"),   parameters = NULL,   ... )  # S3 method for BFBayesFactor point_estimate(x, centrality = \"all\", dispersion = FALSE, ...)"},{"path":"https://easystats.github.io/bayestestR/reference/point_estimate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Point-estimates of posterior distributions — point_estimate","text":"x Vector representing posterior distribution, data frame vectors. Can also Bayesian model. bayestestR supports wide range models (see, example, methods(\"hdi\")) documented 'Usage' section, methods classes mostly resemble arguments .numeric .data.framemethods. ... Additional arguments passed methods. centrality point-estimates (centrality indices) compute.  Character (vector) list one options: \"median\", \"mean\", \"MAP\" \"\". dispersion Logical, TRUE, computes indices dispersion related estimate(s) (SD MAD mean median, respectively). threshold centrality = \"trimmed\" (.e. trimmed mean), indicates fraction (0 0.5) observations trimmed end vector mean computed. effects results fixed effects, random effects returned? applies mixed models. May abbreviated. component results parameters, parameters conditional model zero-inflated part model returned? May abbreviated. applies brms-models. parameters Regular expression pattern describes parameters returned. Meta-parameters (like lp__ prior_) filtered default, parameters typically appear summary() returned. Use parameters select specific parameters output.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/point_estimate.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Point-estimates of posterior distributions — point_estimate","text":"also plot()-method implemented see-package.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/point_estimate.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Point-estimates of posterior distributions — point_estimate","text":"Makowski, D., Ben-Shachar, M. S., Chen, S. H. ., Lüdecke, D. (2019). Indices Effect Existence Significance Bayesian Framework. Frontiers Psychology 2019;10:2767. doi:10.3389/fpsyg.2019.02767","code":""},{"path":"https://easystats.github.io/bayestestR/reference/point_estimate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Point-estimates of posterior distributions — point_estimate","text":"","code":"library(bayestestR)  point_estimate(rnorm(1000)) #> Point Estimate #>  #> Median   | Mean |  MAP #> ---------------------- #> 5.58e-03 | 0.01 | 0.03 point_estimate(rnorm(1000), centrality = \"all\", dispersion = TRUE) #> Point Estimate #>  #> Median |  MAD |      Mean |   SD |  MAP #> --------------------------------------- #> -0.02  | 0.97 | -5.33e-03 | 1.00 | 0.05 point_estimate(rnorm(1000), centrality = c(\"median\", \"MAP\")) #> Point Estimate #>  #> Median |   MAP #> -------------- #> 0.03   | -0.07  df <- data.frame(replicate(4, rnorm(100))) point_estimate(df, centrality = \"all\", dispersion = TRUE) #> Point Estimate #>  #> Parameter | Median |  MAD |  Mean |   SD |   MAP #> ------------------------------------------------ #> X1        |  -0.02 | 1.21 |  0.02 | 1.12 |  0.85 #> X2        |  -0.07 | 1.07 | -0.10 | 1.00 | -0.18 #> X3        |  -0.29 | 1.02 | -0.33 | 0.89 | -0.10 #> X4        |  -0.08 | 0.90 | -0.14 | 0.89 |  0.11 point_estimate(df, centrality = c(\"median\", \"MAP\")) #> Point Estimate #>  #> Parameter | Median |   MAP #> -------------------------- #> X1        |  -0.02 |  0.85 #> X2        |  -0.07 | -0.18 #> X3        |  -0.29 | -0.10 #> X4        |  -0.08 |  0.11 # \\dontrun{ # rstanarm models # ----------------------------------------------- library(rstanarm) model <- rstanarm::stan_glm(mpg ~ wt + cyl, data = mtcars) #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 2.2e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.22 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.052316 seconds (Warm-up) #> Chain 1:                0.051486 seconds (Sampling) #> Chain 1:                0.103802 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 1.3e-05 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.054613 seconds (Warm-up) #> Chain 2:                0.049259 seconds (Sampling) #> Chain 2:                0.103872 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 1.2e-05 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.052986 seconds (Warm-up) #> Chain 3:                0.061248 seconds (Sampling) #> Chain 3:                0.114234 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 1.2e-05 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.054986 seconds (Warm-up) #> Chain 4:                0.05157 seconds (Sampling) #> Chain 4:                0.106556 seconds (Total) #> Chain 4:  point_estimate(model, centrality = \"all\", dispersion = TRUE) #> Point Estimate  #>  #> Parameter   | Median |  MAD |  Mean |   SD |   MAP #> -------------------------------------------------- #> (Intercept) |  39.65 | 1.86 | 39.69 | 1.84 | 39.57 #> wt          |  -3.19 | 0.81 | -3.18 | 0.82 | -3.19 #> cyl         |  -1.51 | 0.44 | -1.52 | 0.44 | -1.54 point_estimate(model, centrality = c(\"median\", \"MAP\")) #> Point Estimate  #>  #> Parameter   | Median |   MAP #> ---------------------------- #> (Intercept) |  39.65 | 39.57 #> wt          |  -3.19 | -3.19 #> cyl         |  -1.51 | -1.54   # emmeans estimates # ----------------------------------------------- library(emmeans) point_estimate(emtrends(model, ~1, \"wt\"), centrality = c(\"median\", \"MAP\")) #> Point Estimate #>  #> Parameter | Median |   MAP #> -------------------------- #> overall   |  -3.19 | -3.19  # brms models # ----------------------------------------------- library(brms) model <- brms::brm(mpg ~ wt + cyl, data = mtcars) #> Compiling Stan program... #> Start sampling #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 1e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.032867 seconds (Warm-up) #> Chain 1:                0.026224 seconds (Sampling) #> Chain 1:                0.059091 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 8e-06 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.030032 seconds (Warm-up) #> Chain 2:                0.022794 seconds (Sampling) #> Chain 2:                0.052826 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 6e-06 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.028068 seconds (Warm-up) #> Chain 3:                0.025234 seconds (Sampling) #> Chain 3:                0.053302 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 7e-06 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.035304 seconds (Warm-up) #> Chain 4:                0.025181 seconds (Sampling) #> Chain 4:                0.060485 seconds (Total) #> Chain 4:  point_estimate(model, centrality = \"all\", dispersion = TRUE) #> Point Estimate  #>  #> Parameter   | Median |  MAD |  Mean |   SD |   MAP #> -------------------------------------------------- #> (Intercept) |  39.69 | 1.79 | 39.72 | 1.82 | 39.66 #> wt          |  -3.19 | 0.77 | -3.17 | 0.80 | -3.26 #> cyl         |  -1.51 | 0.43 | -1.52 | 0.44 | -1.46 point_estimate(model, centrality = c(\"median\", \"MAP\")) #> Point Estimate  #>  #> Parameter   | Median |   MAP #> ---------------------------- #> (Intercept) |  39.69 | 39.66 #> wt          |  -3.19 | -3.26 #> cyl         |  -1.51 | -1.46  # BayesFactor objects # ----------------------------------------------- library(BayesFactor) bf <- ttestBF(x = rnorm(100, 1, 1)) point_estimate(bf, centrality = \"all\", dispersion = TRUE) #> Point Estimate #>  #> Parameter  | Median |  MAD | Mean |   SD |  MAP #> ----------------------------------------------- #> Difference |   1.03 | 0.11 | 1.03 | 0.11 | 1.02 point_estimate(bf, centrality = c(\"median\", \"MAP\")) #> Point Estimate #>  #> Parameter  | Median |  MAP #> -------------------------- #> Difference |   1.03 | 1.04 # }"},{"path":"https://easystats.github.io/bayestestR/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. datawizard reshape_ci insight print_html, print_md","code":""},{"path":"https://easystats.github.io/bayestestR/reference/reshape_iterations.html","id":null,"dir":"Reference","previous_headings":"","what":"Reshape estimations with multiple iterations (draws) to long format — reshape_iterations","title":"Reshape estimations with multiple iterations (draws) to long format — reshape_iterations","text":"Reshape wide data.frame iterations (posterior draws bootsrapped samples) columns long format. Instead iterations columns (e.g., iter_1, iter_2, ...), return 3 columns \\*_index (previous index row), \\*_group (iteration number) \\*_value (value said iteration).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/reshape_iterations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reshape estimations with multiple iterations (draws) to long format — reshape_iterations","text":"","code":"reshape_iterations(x, prefix = c(\"draw\", \"iter\", \"iteration\", \"sim\"))  reshape_draws(x, prefix = c(\"draw\", \"iter\", \"iteration\", \"sim\"))"},{"path":"https://easystats.github.io/bayestestR/reference/reshape_iterations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reshape estimations with multiple iterations (draws) to long format — reshape_iterations","text":"x data.frame containing posterior draws obtained estimate_response estimate_link. prefix prefix draws (instance, \"iter_\" columns named iter_1, iter_2, iter_3). one provided, search first one matches.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/reshape_iterations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reshape estimations with multiple iterations (draws) to long format — reshape_iterations","text":"Data frame reshaped draws long format.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/reshape_iterations.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Reshape estimations with multiple iterations (draws) to long format — reshape_iterations","text":"","code":"# \\donttest{ if (require(\"rstanarm\")) {   model <- stan_glm(mpg ~ am, data = mtcars, refresh = 0)   draws <- insight::get_predicted(model)   long_format <- reshape_iterations(draws)   head(long_format) } #>   Predicted iter_index iter_group iter_value #> 1  24.38890          1          1   24.05244 #> 2  24.38890          2          1   24.05244 #> 3  24.38890          3          1   24.05244 #> 4  17.14047          4          1   17.27725 #> 5  17.14047          5          1   17.27725 #> 6  17.14047          6          1   17.27725 # }"},{"path":"https://easystats.github.io/bayestestR/reference/rope.html","id":null,"dir":"Reference","previous_headings":"","what":"Region of Practical Equivalence (ROPE) — rope","title":"Region of Practical Equivalence (ROPE) — rope","text":"Compute proportion HDI (default 89% HDI) posterior distribution lies within region practical equivalence.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/rope.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Region of Practical Equivalence (ROPE) — rope","text":"","code":"rope(x, ...)  # S3 method for numeric rope(x, range = \"default\", ci = 0.95, ci_method = \"ETI\", verbose = TRUE, ...)  # S3 method for stanreg rope(   x,   range = \"default\",   ci = 0.95,   ci_method = \"ETI\",   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"location\", \"all\", \"conditional\", \"smooth_terms\", \"sigma\",     \"distributional\", \"auxiliary\"),   parameters = NULL,   verbose = TRUE,   ... )  # S3 method for brmsfit rope(   x,   range = \"default\",   ci = 0.95,   ci_method = \"ETI\",   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"conditional\", \"zi\", \"zero_inflated\", \"all\"),   parameters = NULL,   verbose = TRUE,   ... )"},{"path":"https://easystats.github.io/bayestestR/reference/rope.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Region of Practical Equivalence (ROPE) — rope","text":"x Vector representing posterior distribution. Can also stanreg brmsfit model. ... Currently used. range ROPE's lower higher bounds. \"default\" depending number outcome variables vector list. models one response, range vector length two (e.g., c(-0.1, 0.1)). multivariate models, range list numeric vectors response variable. Vector names correspond name response variables. \"default\" input vector, range set c(-0.1, 0.1). \"default\" input Bayesian model, rope_range() used. ci Credible Interval (CI) probability, corresponding proportion HDI, use percentage ROPE. ci_method type interval use quantify percentage ROPE. Can 'HDI' (default) 'ETI'. See ci(). verbose Toggle warnings. effects results fixed effects, random effects returned? applies mixed models. May abbreviated. component results parameters, parameters conditional model zero-inflated part model returned? May abbreviated. applies brms-models. parameters Regular expression pattern describes parameters returned. Meta-parameters (like lp__ prior_) filtered default, parameters typically appear summary() returned. Use parameters select specific parameters output.","code":""},{"path":[]},{"path":"https://easystats.github.io/bayestestR/reference/rope.html","id":"rope","dir":"Reference","previous_headings":"","what":"ROPE","title":"Region of Practical Equivalence (ROPE) — rope","text":"Statistically, probability posterior distribution different 0 make much sense (probability single value null hypothesis continuous distribution 0). Therefore, idea underlining ROPE let user define area around null value enclosing values equivalent null value practical purposes (Kruschke 2010, 2011, 2014).  Kruschke (2018) suggests null value set, default, -0.1 0.1 range standardized parameter (negligible effect size according Cohen, 1988). generalized: instance, linear models, ROPE set 0 +/- .1 * sd(y). ROPE range can automatically computed models using rope_range function.  Kruschke (2010, 2011, 2014) suggests using proportion  95% (89%, considered stable) HDI falls within ROPE index \"null-hypothesis\" testing (understood Bayesian framework, see equivalence_test()).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/rope.html","id":"sensitivity-to-parameter-s-scale","dir":"Reference","previous_headings":"","what":"Sensitivity to parameter's scale","title":"Region of Practical Equivalence (ROPE) — rope","text":"important consider unit (.e., scale) predictors using index based ROPE, correct interpretation ROPE representing region practical equivalence zero dependent scale predictors. Indeed, percentage ROPE depend unit parameter. words, ROPE represents fixed portion response's scale, proximity coefficient depends scale coefficient .","code":""},{"path":"https://easystats.github.io/bayestestR/reference/rope.html","id":"multicollinearity-non-independent-covariates","dir":"Reference","previous_headings":"","what":"Multicollinearity: Non-independent covariates","title":"Region of Practical Equivalence (ROPE) — rope","text":"parameters show strong correlations, .e. covariates independent, joint parameter distributions may shift towards away ROPE. Collinearity invalidates ROPE hypothesis testing based univariate marginals, probabilities conditional independence. problematic parameters partial overlap ROPE region. case collinearity, (joint) distributions parameters may either get increased decreased ROPE, means inferences based rope() inappropriate (Kruschke 2014, 340f). rope() performs simple check pairwise correlations parameters, can collinearity two variables, first step check assumptions hypothesis testing look different pair plots. even sophisticated check projection predictive variable selection (Piironen Vehtari 2017).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/rope.html","id":"strengths-and-limitations","dir":"Reference","previous_headings":"","what":"Strengths and Limitations","title":"Region of Practical Equivalence (ROPE) — rope","text":"Strengths: Provides information related practical relevance effects. Limitations: ROPE range needs arbitrarily defined. Sensitive scale (unit) predictors. sensitive highly significant effects.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/rope.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Region of Practical Equivalence (ROPE) — rope","text":"also plot()-method implemented see-package.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/rope.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Region of Practical Equivalence (ROPE) — rope","text":"Cohen, J. (1988). Statistical power analysis behavioural sciences. Kruschke, J. K. (2010). believe: Bayesian methods data analysis. Trends cognitive sciences, 14(7), 293-300. doi:10.1016/j.tics.2010.05.001 . Kruschke, J. K. (2011). Bayesian assessment null values via parameter estimation model comparison. Perspectives Psychological Science, 6(3), 299-312. doi:10.1177/1745691611406925 . Kruschke, J. K. (2014). Bayesian data analysis: tutorial R, JAGS, Stan. Academic Press. doi:10.1177/2515245918771304 . Kruschke, J. K. (2018). Rejecting accepting parameter values Bayesian estimation. Advances Methods Practices Psychological Science, 1(2), 270-280. doi:10.1177/2515245918771304 . Makowski D, Ben-Shachar MS, Chen SHA, Lüdecke D (2019) Indices Effect Existence Significance Bayesian Framework. Frontiers Psychology 2019;10:2767. doi:10.3389/fpsyg.2019.02767 Piironen, J., & Vehtari, . (2017). Comparison Bayesian predictive methods model selection. Statistics Computing, 27(3), 711–735. doi:10.1007/s11222-016-9649-y","code":""},{"path":"https://easystats.github.io/bayestestR/reference/rope.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Region of Practical Equivalence (ROPE) — rope","text":"","code":"library(bayestestR)  rope(x = rnorm(1000, 0, 0.01), range = c(-0.1, 0.1)) #> # Proportion of samples inside the ROPE [-0.10, 0.10]: #>  #> inside ROPE #> ----------- #> 100.00 %    #>  rope(x = rnorm(1000, 0, 1), range = c(-0.1, 0.1)) #> # Proportion of samples inside the ROPE [-0.10, 0.10]: #>  #> inside ROPE #> ----------- #> 8.32 %      #>  rope(x = rnorm(1000, 1, 0.01), range = c(-0.1, 0.1)) #> # Proportion of samples inside the ROPE [-0.10, 0.10]: #>  #> inside ROPE #> ----------- #> 0.00 %      #>  rope(x = rnorm(1000, 1, 1), ci = c(.90, .95)) #> # Proportions of samples inside the ROPE [-0.10, 0.10]: #>  #> ROPE for the 90% HDI: #>  #> inside ROPE #> ----------- #> 4.89 %      #>  #>  #> ROPE for the 95% HDI: #>  #> inside ROPE #> ----------- #> 4.63 %      #>  #>  # \\dontrun{ library(rstanarm) model <- stan_glm(mpg ~ wt + gear, data = mtcars, chains = 2, iter = 200, refresh = 0) #> Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#bulk-ess #> Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#tail-ess rope(model) #> # Proportion of samples inside the ROPE [-0.60, 0.60]: #>  #> Parameter   | inside ROPE #> ------------------------- #> (Intercept) |      0.00 % #> wt          |      0.00 % #> gear        |     43.68 % #>  rope(model, ci = c(.90, .95)) #> # Proportions of samples inside the ROPE [-0.60, 0.60]: #>  #> ROPE for the 90% HDI: #>  #> Parameter   | inside ROPE #> ------------------------- #> (Intercept) |      0.00 % #> wt          |      0.00 % #> gear        |     46.11 % #>  #>  #> ROPE for the 95% HDI: #>  #> Parameter   | inside ROPE #> ------------------------- #> (Intercept) |      0.00 % #> wt          |      0.00 % #> gear        |     43.68 % #>  #>   library(emmeans) rope(emtrends(model, ~1, \"wt\"), ci = c(.90, .95)) #> # Proportions of samples inside the ROPE [-0.10, 0.10]: #>  #> ROPE for the 90% HDI: #>  #> Parameter | inside ROPE #> ----------------------- #> overall   |      0.00 % #>  #>  #> ROPE for the 95% HDI: #>  #> Parameter | inside ROPE #> ----------------------- #> overall   |      0.00 % #>  #>   library(brms) model <- brms::brm(mpg ~ wt + cyl, data = mtcars) #> Compiling Stan program... #> Start sampling #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 1e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.029444 seconds (Warm-up) #> Chain 1:                0.030556 seconds (Sampling) #> Chain 1:                0.06 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 7e-06 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.030335 seconds (Warm-up) #> Chain 2:                0.030901 seconds (Sampling) #> Chain 2:                0.061236 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 8e-06 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.033047 seconds (Warm-up) #> Chain 3:                0.031583 seconds (Sampling) #> Chain 3:                0.06463 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 7e-06 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.028634 seconds (Warm-up) #> Chain 4:                0.024962 seconds (Sampling) #> Chain 4:                0.053596 seconds (Total) #> Chain 4:  rope(model) #> Possible multicollinearity between b_cyl and b_wt (r = 0.77). This might lead to inappropriate results. See 'Details' in '?rope'. #> # Proportion of samples inside the ROPE [-0.60, 0.60]: #>  #> Parameter | inside ROPE #> ----------------------- #> Intercept |      0.00 % #> wt        |      0.00 % #> cyl       |      0.00 % #>  rope(model, ci = c(.90, .95)) #> Possible multicollinearity between b_cyl and b_wt (r = 0.77). This might lead to inappropriate results. See 'Details' in '?rope'. #> # Proportions of samples inside the ROPE [-0.60, 0.60]: #>  #> ROPE for the 90% HDI: #>  #> Parameter | inside ROPE #> ----------------------- #> Intercept |      0.00 % #> wt        |      0.00 % #> cyl       |      0.00 % #>  #>  #> ROPE for the 95% HDI: #>  #> Parameter | inside ROPE #> ----------------------- #> Intercept |      0.00 % #> wt        |      0.00 % #> cyl       |      0.00 % #>  #>   library(brms) model <- brms::brm(brms::mvbind(mpg, disp) ~ wt + cyl, data = mtcars) #> Setting 'rescor' to TRUE by default for this model #> Warning: In the future, 'rescor' will be set to FALSE by default for all models. It is thus recommended to explicitely set 'rescor' via 'set_rescor' instead of using the default. #> Compiling Stan program... #> Start sampling #>  #> SAMPLING FOR MODEL 'd63df2ad8d51e86b32c14269cc8da8db' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 7.6e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.76 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 1.1575 seconds (Warm-up) #> Chain 1:                0.498151 seconds (Sampling) #> Chain 1:                1.65565 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL 'd63df2ad8d51e86b32c14269cc8da8db' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 5.6e-05 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.56 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 1.11982 seconds (Warm-up) #> Chain 2:                0.498177 seconds (Sampling) #> Chain 2:                1.61799 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL 'd63df2ad8d51e86b32c14269cc8da8db' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 5.6e-05 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.56 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.983383 seconds (Warm-up) #> Chain 3:                0.524985 seconds (Sampling) #> Chain 3:                1.50837 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL 'd63df2ad8d51e86b32c14269cc8da8db' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 5.8e-05 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.58 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.972748 seconds (Warm-up) #> Chain 4:                0.55432 seconds (Sampling) #> Chain 4:                1.52707 seconds (Total) #> Chain 4:  rope(model) #> Possible multicollinearity between b_mpg_cyl and b_mpg_wt (r = 0.78), b_disp_cyl and b_disp_wt (r = 0.78). This might lead to inappropriate results. See 'Details' in '?rope'. #> # Proportion of samples inside the ROPE. #> ROPE with depends on outcome variable. #>  #> Parameter      | inside ROPE |      ROPE width #> ---------------------------------------------- #> mpg_Intercept  |      0.00 % |   [-0.60, 0.60] #> mpg_wt         |      0.00 % |   [-0.60, 0.60] #> mpg_cyl        |      0.00 % |   [-0.60, 0.60] #> disp_Intercept |      0.00 % | [-12.39, 12.39] #> disp_wt        |      0.00 % | [-12.39, 12.39] #> disp_cyl       |      0.00 % | [-12.39, 12.39] #>  rope(model, ci = c(.90, .95)) #> Possible multicollinearity between b_mpg_cyl and b_mpg_wt (r = 0.78), b_disp_cyl and b_disp_wt (r = 0.78). This might lead to inappropriate results. See 'Details' in '?rope'. #> # Proportions of samples inside the ROPE. #> ROPE with depends on outcome variable. #>  #> ROPE for the 90% HDI: #>  #> Parameter      | inside ROPE |      ROPE width #> ---------------------------------------------- #> mpg_Intercept  |      0.00 % |   [-0.60, 0.60] #> mpg_wt         |      0.00 % |   [-0.60, 0.60] #> mpg_cyl        |      0.00 % |   [-0.60, 0.60] #> disp_Intercept |      0.00 % | [-12.39, 12.39] #> disp_wt        |      0.00 % | [-12.39, 12.39] #> disp_cyl       |      0.00 % | [-12.39, 12.39] #>  #>  #> ROPE for the 95% HDI: #>  #> Parameter      | inside ROPE |      ROPE width #> ---------------------------------------------- #> mpg_Intercept  |      0.00 % |   [-0.60, 0.60] #> mpg_wt         |      0.00 % |   [-0.60, 0.60] #> mpg_cyl        |      0.00 % |   [-0.60, 0.60] #> disp_Intercept |      0.00 % | [-12.39, 12.39] #> disp_wt        |      0.00 % | [-12.39, 12.39] #> disp_cyl       |      0.00 % | [-12.39, 12.39] #>  #>   library(BayesFactor) bf <- ttestBF(x = rnorm(100, 1, 1)) rope(bf) #> # Proportion of samples inside the ROPE [-0.10, 0.10]: #>  #> Parameter  | inside ROPE #> ------------------------ #> Difference |      0.00 % #>  rope(bf, ci = c(.90, .95)) #> # Proportions of samples inside the ROPE [-0.10, 0.10]: #>  #> ROPE for the 90% HDI: #>  #> Parameter  | inside ROPE #> ------------------------ #> Difference |      0.00 % #>  #>  #> ROPE for the 95% HDI: #>  #> Parameter  | inside ROPE #> ------------------------ #> Difference |      0.00 % #>  #>  # }"},{"path":"https://easystats.github.io/bayestestR/reference/rope_range.html","id":null,"dir":"Reference","previous_headings":"","what":"Find Default Equivalence (ROPE) Region Bounds — rope_range","title":"Find Default Equivalence (ROPE) Region Bounds — rope_range","text":"function attempts automatically finding suitable \"default\" values Region Practical Equivalence (ROPE).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/rope_range.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find Default Equivalence (ROPE) Region Bounds — rope_range","text":"","code":"rope_range(x, ...)  # S3 method for default rope_range(x, verbose = TRUE, ...)"},{"path":"https://easystats.github.io/bayestestR/reference/rope_range.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find Default Equivalence (ROPE) Region Bounds — rope_range","text":"x stanreg, brmsfit BFBayesFactor object. ... Currently used. verbose Toggle warnings.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/rope_range.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Find Default Equivalence (ROPE) Region Bounds — rope_range","text":"Kruschke (2018) suggests region practical equivalence set, default, range -0.1 0.1 standardized parameter (negligible effect size according Cohen, 1988). linear models (lm), can generalised -0.1 * SDy, 0.1 *     SDy.","code":"\\item For **logistic models**, the parameters expressed in log odds ratio can be converted to standardized difference through the formula \\ifelse{html}{\\out{&pi;/&radic;(3)}}{\\eqn{\\pi/\\sqrt{3}}}, resulting in a range of `-0.18` to `0.18`.  \\item For other models with **binary outcome**, it is strongly recommended to manually specify the rope argument. Currently, the same default is applied that for logistic models.  \\item For models from **count data**, the residual variance is used. This is a rather experimental threshold and is probably often similar to `-0.1, 0.1`, but should be used with care!  \\item For **t-tests**, the standard deviation of the response is used, similarly to linear models (see above).  \\item For **correlations**, `-0.05, 0.05` is used, i.e., half the value of a negligible correlation as suggested by Cohen's (1988) rules of thumb.  \\item For all other models, `-0.1, 0.1` is used to determine the ROPE limits, but it is strongly advised to specify it manually."},{"path":"https://easystats.github.io/bayestestR/reference/rope_range.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Find Default Equivalence (ROPE) Region Bounds — rope_range","text":"Kruschke, J. K. (2018). Rejecting accepting parameter values Bayesian estimation. Advances Methods Practices Psychological Science, 1(2), 270-280. doi:10.1177/2515245918771304 .","code":""},{"path":"https://easystats.github.io/bayestestR/reference/rope_range.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find Default Equivalence (ROPE) Region Bounds — rope_range","text":"","code":"# \\dontrun{ if (require(\"rstanarm\")) {   model <- stan_glm(     mpg ~ wt + gear,     data = mtcars,     chains = 2,     iter = 200,     refresh = 0   )   rope_range(model)    model <- stan_glm(vs ~ mpg, data = mtcars, family = \"binomial\", refresh = 0)   rope_range(model) } #> Warning: The largest R-hat is 1.1, indicating chains have not mixed. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#r-hat #> Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#bulk-ess #> Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#tail-ess #> Warning: Markov chains did not converge! Do not analyze results! #> [1] -0.1813799  0.1813799  if (require(\"brms\")) {   model <- brm(mpg ~ wt + cyl, data = mtcars)   rope_range(model) } #> Compiling Stan program... #> Start sampling #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 1.3e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.032162 seconds (Warm-up) #> Chain 1:                0.030525 seconds (Sampling) #> Chain 1:                0.062687 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 7e-06 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.03252 seconds (Warm-up) #> Chain 2:                0.031677 seconds (Sampling) #> Chain 2:                0.064197 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 8e-06 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.034712 seconds (Warm-up) #> Chain 3:                0.031572 seconds (Sampling) #> Chain 3:                0.066284 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 8e-06 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.033967 seconds (Warm-up) #> Chain 4:                0.025248 seconds (Sampling) #> Chain 4:                0.059215 seconds (Total) #> Chain 4:  #> [1] -0.6026948  0.6026948  if (require(\"BayesFactor\")) {   model <- ttestBF(mtcars[mtcars$vs == 1, \"mpg\"], mtcars[mtcars$vs == 0, \"mpg\"])   rope_range(model)    model <- lmBF(mpg ~ vs, data = mtcars)   rope_range(model) } #> [1] -0.6026948  0.6026948 # }"},{"path":"https://easystats.github.io/bayestestR/reference/sensitivity_to_prior.html","id":null,"dir":"Reference","previous_headings":"","what":"Sensitivity to Prior — sensitivity_to_prior","title":"Sensitivity to Prior — sensitivity_to_prior","text":"Computes sensitivity priors specification. represents proportion change indices model fitted antagonistic prior (prior shape located opposite effect).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/sensitivity_to_prior.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sensitivity to Prior — sensitivity_to_prior","text":"","code":"sensitivity_to_prior(model, index = \"Median\", magnitude = 10, ...)"},{"path":"https://easystats.github.io/bayestestR/reference/sensitivity_to_prior.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sensitivity to Prior — sensitivity_to_prior","text":"model Bayesian model (stanreg brmsfit). index indices compute sensitivity. Can one multiple names columns returned describe_posterior. case important (e.g., write 'Median' instead 'median'). magnitude represent magnitude shift antagonistic prior (test sensitivity). instance, magnitude 10 (default) means mode wil updated prior located 10 standard deviations original location. ... Arguments passed methods.","code":""},{"path":[]},{"path":"https://easystats.github.io/bayestestR/reference/sensitivity_to_prior.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sensitivity to Prior — sensitivity_to_prior","text":"","code":"# \\dontrun{ library(bayestestR)  # rstanarm models # ----------------------------------------------- if (require(\"rstanarm\")) {   model <- rstanarm::stan_glm(mpg ~ wt, data = mtcars)   sensitivity_to_prior(model)    model <- rstanarm::stan_glm(mpg ~ wt + cyl, data = mtcars)   sensitivity_to_prior(model, index = c(\"Median\", \"MAP\")) } #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 2.1e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.040754 seconds (Warm-up) #> Chain 1:                0.036042 seconds (Sampling) #> Chain 1:                0.076796 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 1.3e-05 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.048171 seconds (Warm-up) #> Chain 2:                0.036572 seconds (Sampling) #> Chain 2:                0.084743 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 1.3e-05 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.035627 seconds (Warm-up) #> Chain 3:                0.037309 seconds (Sampling) #> Chain 3:                0.072936 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 1.3e-05 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.037517 seconds (Warm-up) #> Chain 4:                0.041655 seconds (Sampling) #> Chain 4:                0.079172 seconds (Total) #> Chain 4:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 2.2e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.22 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.05696 seconds (Warm-up) #> Chain 1:                0.063754 seconds (Sampling) #> Chain 1:                0.120714 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 4.2e-05 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.42 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.069711 seconds (Warm-up) #> Chain 2:                0.0738 seconds (Sampling) #> Chain 2:                0.143511 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 1.5e-05 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.071322 seconds (Warm-up) #> Chain 3:                0.066913 seconds (Sampling) #> Chain 3:                0.138235 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 1.4e-05 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.067849 seconds (Warm-up) #> Chain 4:                0.052254 seconds (Sampling) #> Chain 4:                0.120103 seconds (Total) #> Chain 4:  #>   Parameter Sensitivity_Median Sensitivity_MAP #> 1        wt         0.03868003      0.05031580 #> 2       cyl         0.01861603      0.02613953  # brms models # ----------------------------------------------- if (require(\"brms\")) {   model <- brms::brm(mpg ~ wt + cyl, data = mtcars)   # sensitivity_to_prior(model) } #> Compiling Stan program... #> Start sampling #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 1.3e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.03336 seconds (Warm-up) #> Chain 1:                0.025329 seconds (Sampling) #> Chain 1:                0.058689 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 7e-06 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.032108 seconds (Warm-up) #> Chain 2:                0.044797 seconds (Sampling) #> Chain 2:                0.076905 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 9e-06 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.032709 seconds (Warm-up) #> Chain 3:                0.031037 seconds (Sampling) #> Chain 3:                0.063746 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 8e-06 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.033883 seconds (Warm-up) #> Chain 4:                0.025523 seconds (Sampling) #> Chain 4:                0.059406 seconds (Total) #> Chain 4:  # }"},{"path":"https://easystats.github.io/bayestestR/reference/sexit.html","id":null,"dir":"Reference","previous_headings":"","what":"Sequential Effect eXistence and sIgnificance Testing (SEXIT) — sexit","title":"Sequential Effect eXistence and sIgnificance Testing (SEXIT) — sexit","text":"SEXIT new framework describe Bayesian effects, guiding indices use. Accordingly, sexit() function returns minimal (optimal) required information describe models' parameters Bayesian framework. includes following indices: Centrality: median posterior distribution. probabilistic terms, 50% probability effect higher lower. See point_estimate(). Uncertainty: 95% Highest Density Interval (HDI). probabilistic terms, 95% probability effect within confidence interval. See ci(). Existence: probability direction allows quantify certainty effect positive negative. critical index show effect manipulation harmful (instance clinical studies) assess direction link. See p_direction(). Significance: existence demonstrated high certainty, can assess whether effect sufficient size considered significant (.e., negligible). useful index determine effects actually important worthy discussion given process. See p_significance(). Size: Finally, index gives idea strength effect. However, beware, studies shown big effect size can also suggestive low statistical power (see details section).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/sexit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sequential Effect eXistence and sIgnificance Testing (SEXIT) — sexit","text":"","code":"sexit(x, significant = \"default\", large = \"default\", ci = 0.95, ...)"},{"path":"https://easystats.github.io/bayestestR/reference/sexit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sequential Effect eXistence and sIgnificance Testing (SEXIT) — sexit","text":"x Vector representing posterior distribution. Can also Bayesian model (stanreg, brmsfit BayesFactor). significant, large threshold values use significant large probabilities. left 'default', selected sexit_thresholds(). See details section . ci Value vector probability (credible) interval - CI (0 1) estimated. Default .95 (95%). ... Currently used.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/sexit.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sequential Effect eXistence and sIgnificance Testing (SEXIT) — sexit","text":"dataframe text attribute.","code":""},{"path":[]},{"path":"https://easystats.github.io/bayestestR/reference/sexit.html","id":"rationale","dir":"Reference","previous_headings":"","what":"Rationale","title":"Sequential Effect eXistence and sIgnificance Testing (SEXIT) — sexit","text":"assessment \"significance\" (broadest meaning) pervasive issue science, historical index, p-value, strongly criticized deemed played important role replicability crisis. reaction, scientists tuned Bayesian methods, offering alternative set tools answer questions. However, Bayesian framework offers wide variety possible indices related \"significance\", debate raging index best, one report. situation can lead mindless reporting possible indices (hopes reader satisfied), often without writer understanding interpreting . indeed complicated juggle many indices complicated definitions subtle differences. SEXIT aims offering practical framework Bayesian effects reporting, focus put intuitiveness, explicitness usefulness indices' interpretation. end, suggest system description parameters intuitive, easy learn apply, mathematically accurate useful taking decision. thresholds significance (.e., ROPE) one \"large\" effect explicitly defined, SEXIT framework make interpretation, .e., label effects, just sequentially gives 3 probabilities (direction, significance large, respectively) -top characteristics posterior (using median HDI centrality uncertainty description). Thus, provides lot information posterior distribution (mass different 'sections' posterior) clear meaningful way.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/sexit.html","id":"threshold-selection","dir":"Reference","previous_headings":"","what":"Threshold selection","title":"Sequential Effect eXistence and sIgnificance Testing (SEXIT) — sexit","text":"One important thing SEXIT framework relies two \"arbitrary\" thresholds (.e., absolute meaning). ones related effect size (inherently subjective notion), namely thresholds significant large effects. set, default, 0.05 0.3 standard deviation outcome variable (tiny large effect sizes correlations according Funder Ozer, 2019). However, defaults chosen lack better option, might adapted case. Thus, handled care, chosen thresholds always explicitly reported justified. linear models (lm), can generalised 0.05 * SDy 0.3 * SDy significant large effects, respectively. logistic models, parameters expressed log odds ratio can converted standardized difference formula π/√(3), resulting threshold 0.09 0.54. models binary outcome, strongly recommended manually specify rope argument. Currently, default applied logistic models. models count data, residual variance used. rather experimental threshold probably often similar 0.05 0.3, used care! t-tests, standard deviation response used, similarly linear models (see ). correlations,0.05 0.3 used. models, 0.05 0.3 used, strongly advised specify manually.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/sexit.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sequential Effect eXistence and sIgnificance Testing (SEXIT) — sexit","text":"three values existence, significance size provide useful description posterior distribution effects. possible scenarios include: probability existence low, probability large high: suggests posterior wide (covering large territories side 0). statistical power might low, warrant confident conclusion. probability existence significance high, probability large small: suggests effect , high confidence, large (posterior mostly contained significance large thresholds). 3 indices low: suggests effect null high confidence (posterior closely centred around 0).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/sexit.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Sequential Effect eXistence and sIgnificance Testing (SEXIT) — sexit","text":"Makowski, D., Ben-Shachar, M. S., & Lüdecke, D. (2019). bayestestR: Describing Effects Uncertainty, Existence Significance within Bayesian Framework. Journal Open Source Software, 4(40), 1541. doi:10.21105/joss.01541 Makowski D, Ben-Shachar MS, Chen SHA, Lüdecke D (2019) Indices Effect Existence Significance Bayesian Framework. Frontiers Psychology 2019;10:2767. doi:10.3389/fpsyg.2019.02767","code":""},{"path":"https://easystats.github.io/bayestestR/reference/sexit.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sequential Effect eXistence and sIgnificance Testing (SEXIT) — sexit","text":"","code":"# \\dontrun{ library(bayestestR)  s <- sexit(rnorm(1000, -1, 1)) s #> # Following the Sequential Effect eXistence and sIgnificance Testing (SEXIT) framework, we report the median of the posterior distribution and its 95% CI (Highest Density Interval), along the probability of direction (pd), the probability of significance and the probability of being large. The thresholds beyond which the effect is considered as significant (i.e., non-negligible) and large are |0.05| and |0.30|. #>  #> The effect (Median = -0.96, 95% CI [-3.06, 0.91]) has a 83.20% probability of being negative (< 0), 82.30% of being significant (< -0.05), and 74.50% of being large (< -0.30) #>  #> Median |        95% CI | Direction | Significance (> |0.05|) | Large (> |0.30|) #> ------------------------------------------------------------------------------- #>  -0.96 | [-3.06, 0.91] |      0.83 |                    0.82 |             0.74 #>  print(s, summary = TRUE) #> # The thresholds beyond which the effect is considered as significant (i.e., non-negligible) and large are |0.05| and |0.30|. #>  #> The effect (Median = -0.96, 95% CI [-3.06, 0.91]) has 83.20%, 82.30% and 74.50% probability of being negative (< 0), significant (< -0.05) and large (< -0.30)  s <- sexit(iris) s #> # Following the Sequential Effect eXistence and sIgnificance Testing (SEXIT) framework, we report the median of the posterior distribution and its 95% CI (Highest Density Interval), along the probability of direction (pd), the probability of significance and the probability of being large. The thresholds beyond which the effect is considered as significant (i.e., non-negligible) and large are |0.05| and |0.30|. #>  #> - Sepal.Length (Median = 5.80, 95% CI [4.47, 7.70]) has a 100.00% probability of being positive (> 0), 100.00% of being significant (> 0.05), and 100.00% of being large (> 0.30) #> - Sepal.Width (Median = 3.00, 95% CI [2.27, 3.93]) has a 100.00% probability of being positive (> 0), 100.00% of being significant (> 0.05), and 100.00% of being large (> 0.30) #> - Petal.Length (Median = 4.35, 95% CI [1.27, 6.46]) has a 100.00% probability of being positive (> 0), 100.00% of being significant (> 0.05), and 100.00% of being large (> 0.30) #> - Petal.Width (Median = 1.30, 95% CI [0.10, 2.40]) has a 100.00% probability of being positive (> 0), 100.00% of being significant (> 0.05), and 72.67% of being large (> 0.30) #>  #> Parameter    | Median |       95% CI | Direction | Significance (> |0.05|) | Large (> |0.30|) #> --------------------------------------------------------------------------------------------- #> Sepal.Length |   5.80 | [4.47, 7.70] |         1 |                       1 |             1.00 #> Sepal.Width  |   3.00 | [2.27, 3.93] |         1 |                       1 |             1.00 #> Petal.Length |   4.35 | [1.27, 6.46] |         1 |                       1 |             1.00 #> Petal.Width  |   1.30 | [0.10, 2.40] |         1 |                       1 |             0.73 #>  print(s, summary = TRUE) #> # The thresholds beyond which the effect is considered as significant (i.e., non-negligible) and large are |0.05| and |0.30|. #>  #> - Sepal.Length (Median = 5.80, 95% CI [4.47, 7.70]) has 100.00%, 100.00% and 100.00% probability of being positive (> 0), significant (> 0.05) and large (> 0.30) #> - Sepal.Width (Median = 3.00, 95% CI [2.27, 3.93]) has 100.00%, 100.00% and 100.00% probability of being positive (> 0), significant (> 0.05) and large (> 0.30) #> - Petal.Length (Median = 4.35, 95% CI [1.27, 6.46]) has 100.00%, 100.00% and 100.00% probability of being positive (> 0), significant (> 0.05) and large (> 0.30) #> - Petal.Width (Median = 1.30, 95% CI [0.10, 2.40]) has 100.00%, 100.00% and 72.67% probability of being positive (> 0), significant (> 0.05) and large (> 0.30)  if (require(\"rstanarm\")) {   model <- rstanarm::stan_glm(mpg ~ wt * cyl,     data = mtcars,     iter = 400, refresh = 0   )   s <- sexit(model)   s   print(s, summary = TRUE) } #> Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#bulk-ess #> Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#tail-ess #> # The thresholds beyond which the effect is considered as significant (i.e., non-negligible) and large are |0.30| and |1.81|. #>  #> - (Intercept) (Median = 53.10, 95% CI [40.35, 66.36]) has 100.00%, 100.00% and 100.00% probability of being positive (> 0), significant (> 0.30) and large (> 1.81) #> - wt (Median = -8.23, 95% CI [-13.27, -3.42]) has 100.00%, 100.00% and 99.62% probability of being negative (< 0), significant (< -0.30) and large (< -1.81) #> - cyl (Median = -3.64, 95% CI [-5.82, -1.66]) has 100.00%, 100.00% and 95.88% probability of being negative (< 0), significant (< -0.30) and large (< -1.81) #> - wt:cyl (Median = 0.74, 95% CI [0.08, 1.46]) has 98.38%, 91.00% and 0.00% probability of being positive (> 0), significant (> 0.30) and large (> 1.81) # }"},{"path":"https://easystats.github.io/bayestestR/reference/sexit_thresholds.html","id":null,"dir":"Reference","previous_headings":"","what":"Find Effect Size Thresholds — sexit_thresholds","title":"Find Effect Size Thresholds — sexit_thresholds","text":"function attempts automatically finding suitable default values \"significant\" (.e., non-negligible) \"large\" effect. used care, chosen threshold always explicitly reported justified. See detail section sexit() information.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/sexit_thresholds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find Effect Size Thresholds — sexit_thresholds","text":"","code":"sexit_thresholds(x, ...)"},{"path":"https://easystats.github.io/bayestestR/reference/sexit_thresholds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Find Effect Size Thresholds — sexit_thresholds","text":"x Vector representing posterior distribution. Can also stanreg brmsfit model. ... Currently used.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/sexit_thresholds.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Find Effect Size Thresholds — sexit_thresholds","text":"Kruschke, J. K. (2018). Rejecting accepting parameter values Bayesian estimation. Advances Methods Practices Psychological Science, 1(2), 270-280. doi:10.1177/2515245918771304 .","code":""},{"path":"https://easystats.github.io/bayestestR/reference/sexit_thresholds.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Find Effect Size Thresholds — sexit_thresholds","text":"","code":"sexit_thresholds(rnorm(1000)) #> [1] 0.05 0.30 # \\dontrun{ if (require(\"rstanarm\")) {   model <- stan_glm(     mpg ~ wt + gear,     data = mtcars,     chains = 2,     iter = 200,     refresh = 0   )   sexit_thresholds(model)    model <- stan_glm(vs ~ mpg, data = mtcars, family = \"binomial\", refresh = 0)   sexit_thresholds(model) } #> Warning: The largest R-hat is 1.08, indicating chains have not mixed. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#r-hat #> Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#bulk-ess #> Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#tail-ess #> [1] 0.09068997 0.54413981  if (require(\"brms\")) {   model <- brm(mpg ~ wt + cyl, data = mtcars)   sexit_thresholds(model) } #> Compiling Stan program... #> Start sampling #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 1.3e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.032429 seconds (Warm-up) #> Chain 1:                0.031083 seconds (Sampling) #> Chain 1:                0.063512 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 8e-06 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.033704 seconds (Warm-up) #> Chain 2:                0.035082 seconds (Sampling) #> Chain 2:                0.068786 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 8e-06 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.033507 seconds (Warm-up) #> Chain 3:                0.029135 seconds (Sampling) #> Chain 3:                0.062642 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL '667cda6873b3c0bde53ba0b20ec47308' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 9e-06 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.033062 seconds (Warm-up) #> Chain 4:                0.034386 seconds (Sampling) #> Chain 4:                0.067448 seconds (Total) #> Chain 4:  #> [1] 0.3013474 1.8080844  if (require(\"BayesFactor\")) {   bf <- ttestBF(x = rnorm(100, 1, 1))   sexit_thresholds(bf) } #> [1] 0.04872446 0.29234676 # }"},{"path":"https://easystats.github.io/bayestestR/reference/si.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Support Intervals — si","title":"Compute Support Intervals — si","text":"support interval contains values parameter predict observed data better average, degree k; values parameter associated updating factor greater equal k. perspective Savage-Dickey Bayes factor, testing point null hypothesis value within support interval yield Bayes factor smaller 1/k. info, particular specifying correct priors factors 2 levels, see Bayes factors vignette.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/si.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Support Intervals — si","text":"","code":"si(posterior, prior = NULL, BF = 1, verbose = TRUE, ...)  # S3 method for numeric si(posterior, prior = NULL, BF = 1, verbose = TRUE, ...)  # S3 method for stanreg si(   posterior,   prior = NULL,   BF = 1,   verbose = TRUE,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"location\", \"all\", \"conditional\", \"smooth_terms\", \"sigma\",     \"distributional\", \"auxiliary\"),   parameters = NULL,   ... )  # S3 method for brmsfit si(   posterior,   prior = NULL,   BF = 1,   verbose = TRUE,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"location\", \"all\", \"conditional\", \"smooth_terms\", \"sigma\",     \"distributional\", \"auxiliary\"),   parameters = NULL,   ... )  # S3 method for blavaan si(   posterior,   prior = NULL,   BF = 1,   verbose = TRUE,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"location\", \"all\", \"conditional\", \"smooth_terms\", \"sigma\",     \"distributional\", \"auxiliary\"),   parameters = NULL,   ... )  # S3 method for emmGrid si(posterior, prior = NULL, BF = 1, verbose = TRUE, ...)  # S3 method for data.frame si(posterior, prior = NULL, BF = 1, verbose = TRUE, ...)"},{"path":"https://easystats.github.io/bayestestR/reference/si.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Support Intervals — si","text":"posterior numerical vector, stanreg / brmsfit object, emmGrid data frame - representing posterior distribution(s) (see 'Details'). prior object representing prior distribution (see 'Details'). BF amount support required included support interval. verbose Toggle warnings. ... Arguments passed methods. (Can used pass arguments internal logspline::logspline().) effects results fixed effects, random effects returned? applies mixed models. May abbreviated. component results parameters, parameters conditional model zero-inflated part model returned? May abbreviated. applies brms-models. parameters Regular expression pattern describes parameters returned. Meta-parameters (like lp__ prior_) filtered default, parameters typically appear summary() returned. Use parameters select specific parameters output.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/si.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Support Intervals — si","text":"data frame containing lower upper bounds SI. Note level requested support higher observed data, interval [NA,NA].","code":""},{"path":"https://easystats.github.io/bayestestR/reference/si.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute Support Intervals — si","text":"method used compute support intervals based prior posterior distributions. computation support intervals, model priors must proper priors (least flat, preferable informative - note default, brms::brm() uses flat priors fixed-effects; see example ).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/si.html","id":"choosing-a-value-of-bf","dir":"Reference","previous_headings":"","what":"Choosing a value of BF","title":"Compute Support Intervals — si","text":"choice BF (level support) depends want interval represent: BF = 1 contains values whose credibility decreased observing data. BF > 1 contains values received impressive support data. BF < 1 contains values whose credibility impressively decreased observing data. Testing values outside interval produce Bayes factor larger 1/BF support alternative. E.g., SI (BF = 1/3) excludes 0, Bayes factor point-null larger 3.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/si.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Compute Support Intervals — si","text":"also plot()-method implemented see-package.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/si.html","id":"setting-the-correct-prior","dir":"Reference","previous_headings":"","what":"Setting the correct prior","title":"Compute Support Intervals — si","text":"computation Bayes factors, model priors must proper priors (least flat, preferable informative); priors alternative get wider, likelihood null value(s) increases, extreme completely flat priors null infinitely favorable alternative (called Jeffreys-Lindley-Bartlett paradox). Thus, ever try (want) compute Bayes factor informed prior.  (Note default, brms::brm() uses flat priors fixed-effects; See example .)  important provide correct prior meaningful results. posterior numerical vector, prior also numerical vector. posterior data.frame, prior also data.frame, matching column order. posterior stanreg, brmsfit supported Bayesian model: prior can set NULL, case prior samples drawn internally. prior can also model equivalent posterior samples priors . See unupdate(). Note: posterior brmsfit_multiple model, prior must provided. posterior emmGrid / emm_list object: prior also emmGrid / emm_list object equivalent posterior created model priors samples . See unupdate(). prior can also original (posterior) model. , function try update emmGrid / emm_list use unupdate()d prior-model. (done brmsfit models.) Note: emmGrid undergone transformations (\"log\", \"response\", etc.), regriding, prior must emmGrid object, stated .","code":""},{"path":"https://easystats.github.io/bayestestR/reference/si.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute Support Intervals — si","text":"Wagenmakers, E., Gronau, Q. F., Dablander, F., & Etz, . (2018, November 22). Support Interval. doi:10.31234/osf.io/zwnxb","code":""},{"path":[]},{"path":"https://easystats.github.io/bayestestR/reference/si.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute Support Intervals — si","text":"","code":"library(bayestestR)  prior <- distribution_normal(1000, mean = 0, sd = 1) posterior <- distribution_normal(1000, mean = .5, sd = .3)  si(posterior, prior) #> Warning: Support intervals might not be precise. #>   For precise support intervals, sampling at least 40,000 posterior #>   samples is recommended. #> BF = 1 SI: [0.04, 1.04] # \\dontrun{ # rstanarm models # --------------- library(rstanarm) contrasts(sleep$group) <- contr.equalprior_pairs # see vignette stan_model <- stan_lmer(extra ~ group + (1 | ID), data = sleep) #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 5.3e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.53 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.297577 seconds (Warm-up) #> Chain 1:                0.181997 seconds (Sampling) #> Chain 1:                0.479574 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 2.1e-05 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.305566 seconds (Warm-up) #> Chain 2:                0.220722 seconds (Sampling) #> Chain 2:                0.526288 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 2e-05 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.306953 seconds (Warm-up) #> Chain 3:                0.233888 seconds (Sampling) #> Chain 3:                0.540841 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL 'continuous' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 1.9e-05 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.303673 seconds (Warm-up) #> Chain 4:                0.281269 seconds (Sampling) #> Chain 4:                0.584942 seconds (Total) #> Chain 4:  si(stan_model) #> Sampling priors, please wait... #> Warning: Support intervals might not be precise. #>   For precise support intervals, sampling at least 40,000 posterior #>   samples is recommended. #> Support Interval #>  #> Parameter   |    BF = 1 SI | Effects |   Component #> -------------------------------------------------- #> (Intercept) | [0.44, 2.71] |   fixed | conditional #> group1      | [0.41, 2.76] |   fixed | conditional si(stan_model, BF = 3) #> Sampling priors, please wait... #> Warning: Support intervals might not be precise. #>   For precise support intervals, sampling at least 40,000 posterior #>   samples is recommended. #> Support Interval #>  #> Parameter   |    BF = 3 SI | Effects |   Component #> -------------------------------------------------- #> (Intercept) | [0.74, 2.32] |   fixed | conditional #> group1      | [0.66, 2.45] |   fixed | conditional  # emmGrid objects # --------------- library(emmeans) group_diff <- pairs(emmeans(stan_model, ~group)) si(group_diff, prior = stan_model) #> Sampling priors, please wait... #> Warning: Support intervals might not be precise. #>   For precise support intervals, sampling at least 40,000 posterior #>   samples is recommended. #> Support Interval #>  #> Parameter       |      BF = 1 SI #> -------------------------------- #> group1 - group2 | [-2.75, -0.51]  # brms models # ----------- library(brms) contrasts(sleep$group) <- contr.equalprior_pairs # see vingette my_custom_priors <-   set_prior(\"student_t(3, 0, 1)\", class = \"b\") +   set_prior(\"student_t(3, 0, 1)\", class = \"sd\", group = \"ID\")  brms_model <- brm(extra ~ group + (1 | ID),   data = sleep,   prior = my_custom_priors ) #> Compiling Stan program... #> Start sampling #>  #> SAMPLING FOR MODEL 'adf329cdde529e84ce328f199529dd87' NOW (CHAIN 1). #> Chain 1:  #> Chain 1: Gradient evaluation took 1.9e-05 seconds #> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds. #> Chain 1: Adjust your expectations accordingly! #> Chain 1:  #> Chain 1:  #> Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 1:  #> Chain 1:  Elapsed Time: 0.128083 seconds (Warm-up) #> Chain 1:                0.133013 seconds (Sampling) #> Chain 1:                0.261096 seconds (Total) #> Chain 1:  #>  #> SAMPLING FOR MODEL 'adf329cdde529e84ce328f199529dd87' NOW (CHAIN 2). #> Chain 2:  #> Chain 2: Gradient evaluation took 1.4e-05 seconds #> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. #> Chain 2: Adjust your expectations accordingly! #> Chain 2:  #> Chain 2:  #> Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 2:  #> Chain 2:  Elapsed Time: 0.158313 seconds (Warm-up) #> Chain 2:                0.118129 seconds (Sampling) #> Chain 2:                0.276442 seconds (Total) #> Chain 2:  #>  #> SAMPLING FOR MODEL 'adf329cdde529e84ce328f199529dd87' NOW (CHAIN 3). #> Chain 3:  #> Chain 3: Gradient evaluation took 1.4e-05 seconds #> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. #> Chain 3: Adjust your expectations accordingly! #> Chain 3:  #> Chain 3:  #> Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 3:  #> Chain 3:  Elapsed Time: 0.145196 seconds (Warm-up) #> Chain 3:                0.140048 seconds (Sampling) #> Chain 3:                0.285244 seconds (Total) #> Chain 3:  #>  #> SAMPLING FOR MODEL 'adf329cdde529e84ce328f199529dd87' NOW (CHAIN 4). #> Chain 4:  #> Chain 4: Gradient evaluation took 1.4e-05 seconds #> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. #> Chain 4: Adjust your expectations accordingly! #> Chain 4:  #> Chain 4:  #> Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup) #> Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup) #> Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup) #> Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup) #> Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup) #> Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup) #> Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling) #> Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling) #> Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling) #> Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling) #> Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling) #> Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling) #> Chain 4:  #> Chain 4:  Elapsed Time: 0.11711 seconds (Warm-up) #> Chain 4:                0.118326 seconds (Sampling) #> Chain 4:                0.235436 seconds (Total) #> Chain 4:  #> Warning: There were 1 divergent transitions after warmup. See #> https://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup #> to find out why this is a problem and how to eliminate them. #> Warning: Examine the pairs() plot to diagnose sampling problems si(brms_model) #> Sampling priors, please wait... #> Error in match.arg(component, choices = c(\"all\", .all_elements())): 'arg' should be one of “all”, “conditional”, “conditional1”, “conditional2”, “conditional3”, “precision”, “nonlinear”, “random”, “zi”, “zero_inflated”, “zero_inflated_random”, “shape”, “dispersion”, “instruments”, “interactions”, “simplex”, “smooth_terms”, “sigma”, “nu”, “tau”, “correlation”, “slopes”, “cluster”, “extra”, “scale”, “marginal”, “alpha”, “beta”, “survival”, “infrequent_purchase”, “auxiliary”, “mix”, “shiftprop”, “phi”, “ndt”, “hu”, “xi”, “coi”, “zoi”, “aux”, “dist”, “selection”, “outcome”, “time_dummies”, “sigma_random”, “beta_random”, “car”, “nominal” # }"},{"path":"https://easystats.github.io/bayestestR/reference/simulate_correlation.html","id":null,"dir":"Reference","previous_headings":"","what":"Data Simulation — simulate_correlation","title":"Data Simulation — simulate_correlation","text":"Simulate data specific characteristics.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/simulate_correlation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Data Simulation — simulate_correlation","text":"","code":"simulate_correlation(n = 100, r = 0.5, mean = 0, sd = 1, names = NULL, ...)  simulate_ttest(n = 100, d = 0.5, names = NULL, ...)  simulate_difference(n = 100, d = 0.5, names = NULL, ...)"},{"path":"https://easystats.github.io/bayestestR/reference/simulate_correlation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Data Simulation — simulate_correlation","text":"n number observations generated. r value vector corresponding desired correlation coefficients. mean value vector corresponding mean variables. sd value vector corresponding SD variables. names character vector desired variable names. ... Arguments passed methods. d value vector corresponding desired difference groups.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/simulate_correlation.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Data Simulation — simulate_correlation","text":"","code":"# Correlation -------------------------------- data <- simulate_correlation(r = 0.5) plot(data$V1, data$V2)  cor.test(data$V1, data$V2) #>  #> \tPearson's product-moment correlation #>  #> data:  data$V1 and data$V2 #> t = 5.7155, df = 98, p-value = 1.18e-07 #> alternative hypothesis: true correlation is not equal to 0 #> 95 percent confidence interval: #>  0.3366433 0.6341398 #> sample estimates: #> cor  #> 0.5  #>  summary(lm(V2 ~ V1, data = data)) #>  #> Call: #> lm(formula = V2 ~ V1, data = data) #>  #> Residuals: #>      Min       1Q   Median       3Q      Max  #> -2.16918 -0.49767 -0.06732  0.45637  2.15963  #>  #> Coefficients: #>               Estimate Std. Error t value Pr(>|t|)     #> (Intercept) -2.566e-17  8.704e-02   0.000        1     #> V1           5.000e-01  8.748e-02   5.715 1.18e-07 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Residual standard error: 0.8704 on 98 degrees of freedom #> Multiple R-squared:   0.25,\tAdjusted R-squared:  0.2423  #> F-statistic: 32.67 on 1 and 98 DF,  p-value: 1.18e-07 #>   # Specify mean and SD data <- simulate_correlation(r = 0.5, n = 50, mean = c(0, 1), sd = c(0.7, 1.7)) cor.test(data$V1, data$V2) #>  #> \tPearson's product-moment correlation #>  #> data:  data$V1 and data$V2 #> t = 4, df = 48, p-value = 0.000218 #> alternative hypothesis: true correlation is not equal to 0 #> 95 percent confidence interval: #>  0.2574879 0.6832563 #> sample estimates: #> cor  #> 0.5  #>  round(c(mean(data$V1), sd(data$V1)), 1) #> [1] 0.0 0.7 round(c(mean(data$V2), sd(data$V2)), 1) #> [1] 1.0 1.7 summary(lm(V2 ~ V1, data = data)) #>  #> Call: #> lm(formula = V2 ~ V1, data = data) #>  #> Residuals: #>     Min      1Q  Median      3Q     Max  #> -2.9078 -0.9438 -0.1638  0.8995  3.9841  #>  #> Coefficients: #>             Estimate Std. Error t value Pr(>|t|)     #> (Intercept)   1.0000     0.2104   4.754 1.86e-05 *** #> V1            1.2143     0.3036   4.000 0.000218 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Residual standard error: 1.487 on 48 degrees of freedom #> Multiple R-squared:   0.25,\tAdjusted R-squared:  0.2344  #> F-statistic:    16 on 1 and 48 DF,  p-value: 0.000218 #>   # Generate multiple variables cor_matrix <- matrix(   c(     1.0, 0.2, 0.4,     0.2, 1.0, 0.3,     0.4, 0.3, 1.0   ),   nrow = 3 )  data <- simulate_correlation(r = cor_matrix, names = c(\"y\", \"x1\", \"x2\")) cor(data) #>      y  x1  x2 #> y  1.0 0.2 0.4 #> x1 0.2 1.0 0.3 #> x2 0.4 0.3 1.0 summary(lm(y ~ x1, data = data)) #>  #> Call: #> lm(formula = y ~ x1, data = data) #>  #> Residuals: #>      Min       1Q   Median       3Q      Max  #> -3.00408 -0.57695 -0.04957  0.69355  2.61495  #>  #> Coefficients: #>               Estimate Std. Error t value Pr(>|t|)   #> (Intercept) -2.853e-17  9.848e-02   0.000    1.000   #> x1           2.000e-01  9.897e-02   2.021    0.046 * #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> Residual standard error: 0.9848 on 98 degrees of freedom #> Multiple R-squared:   0.04,\tAdjusted R-squared:  0.0302  #> F-statistic: 4.083 on 1 and 98 DF,  p-value: 0.04604 #>   # t-test -------------------------------- data <- simulate_ttest(n = 30, d = 0.3) plot(data$V1, data$V0)  round(c(mean(data$V1), sd(data$V1)), 1) #> [1] 0 1 diff(t.test(data$V1 ~ data$V0)$estimate) #> mean in group 1  #>      0.04575124  summary(lm(V1 ~ V0, data = data)) #>  #> Call: #> lm(formula = V1 ~ V0, data = data) #>  #> Residuals: #>      Min       1Q   Median       3Q      Max  #> -2.10517 -0.66055 -0.02288  0.63767  2.15092  #>  #> Coefficients: #>             Estimate Std. Error t value Pr(>|t|) #> (Intercept) -0.02288    0.26160  -0.087    0.931 #> V01          0.04575    0.36996   0.124    0.902 #>  #> Residual standard error: 1.013 on 28 degrees of freedom #> Multiple R-squared:  0.0005459,\tAdjusted R-squared:  -0.03515  #> F-statistic: 0.01529 on 1 and 28 DF,  p-value: 0.9025 #>  summary(glm(V0 ~ V1, data = data, family = \"binomial\")) #>  #> Call: #> glm(formula = V0 ~ V1, family = \"binomial\", data = data) #>  #> Deviance Residuals:  #>      Min        1Q    Median        3Q       Max   #> -1.22085  -1.17869   0.00483   1.17783   1.21094   #>  #> Coefficients: #>              Estimate Std. Error z value Pr(>|z|) #> (Intercept) 2.870e-17  3.652e-01   0.000    1.000 #> V1          4.775e-02  3.732e-01   0.128    0.898 #>  #> (Dispersion parameter for binomial family taken to be 1) #>  #>     Null deviance: 41.589  on 29  degrees of freedom #> Residual deviance: 41.572  on 28  degrees of freedom #> AIC: 45.572 #>  #> Number of Fisher Scoring iterations: 3 #>   # Difference -------------------------------- data <- simulate_difference(n = 30, d = 0.3) plot(data$V1, data$V0)  round(c(mean(data$V1), sd(data$V1)), 1) #> [1] 0 1 diff(t.test(data$V1 ~ data$V0)$estimate) #> mean in group 1  #>             0.3  summary(lm(V1 ~ V0, data = data)) #>  #> Call: #> lm(formula = V1 ~ V0, data = data) #>  #> Residuals: #>    Min     1Q Median     3Q    Max  #> -1.834 -0.677  0.000  0.677  1.834  #>  #> Coefficients: #>             Estimate Std. Error t value Pr(>|t|) #> (Intercept)  -0.1500     0.2562  -0.586    0.563 #> V01           0.3000     0.3623   0.828    0.415 #>  #> Residual standard error: 0.9922 on 28 degrees of freedom #> Multiple R-squared:  0.0239,\tAdjusted R-squared:  -0.01096  #> F-statistic: 0.6857 on 1 and 28 DF,  p-value: 0.4146 #>  summary(glm(V0 ~ V1, data = data, family = \"binomial\")) #>  #> Call: #> glm(formula = V0 ~ V1, family = \"binomial\", data = data) #>  #> Deviance Residuals:  #>    Min      1Q  Median      3Q     Max   #> -1.417  -1.151   0.000   1.151   1.417   #>  #> Coefficients: #>               Estimate Std. Error z value Pr(>|z|) #> (Intercept) -4.569e-17  3.696e-01   0.000    1.000 #> V1           3.251e-01  3.877e-01   0.839    0.402 #>  #> (Dispersion parameter for binomial family taken to be 1) #>  #>     Null deviance: 41.589  on 29  degrees of freedom #> Residual deviance: 40.865  on 28  degrees of freedom #> AIC: 44.865 #>  #> Number of Fisher Scoring iterations: 4 #>"},{"path":"https://easystats.github.io/bayestestR/reference/simulate_prior.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns Priors of a Model as Empirical Distributions — simulate_prior","title":"Returns Priors of a Model as Empirical Distributions — simulate_prior","text":"Transforms priors information actual distributions.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/simulate_prior.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns Priors of a Model as Empirical Distributions — simulate_prior","text":"","code":"simulate_prior(model, n = 1000, ...)"},{"path":"https://easystats.github.io/bayestestR/reference/simulate_prior.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Returns Priors of a Model as Empirical Distributions — simulate_prior","text":"model stanreg, stanfit, brmsfit, blavaan, MCMCglmm object. n Size simulated prior distributions. ... Currently used.","code":""},{"path":[]},{"path":"https://easystats.github.io/bayestestR/reference/simulate_prior.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Returns Priors of a Model as Empirical Distributions — simulate_prior","text":"","code":"# \\dontrun{ library(bayestestR) if (require(\"rstanarm\")) {   model <- stan_glm(mpg ~ wt + am, data = mtcars, chains = 1, refresh = 0)   simulate_prior(model) } #>       (Intercept)           wt           am #> 1    -29.48895919 -50.67117077 -99.35969268 #> 2    -24.62538077 -45.70051165 -89.61286514 #> 3    -22.20399176 -43.22581126 -84.76029381 #> 4    -20.54372566 -41.52899133 -81.43304669 #> 5    -19.26616155 -40.22329926 -78.87275136 #> 6    -18.22115924 -39.15528929 -76.77852025 #> 7    -17.33324626 -38.24782726 -74.99910312 #> 8    -16.55893057 -37.45646357 -73.44734004 #> 9    -15.87078383 -36.75316600 -72.06826334 #> 10   -15.25035829 -36.11908064 -70.82490296 #> 11   -14.68463134 -35.54089816 -69.69116098 #> 12   -14.16405635 -35.00886175 -68.64790554 #> 13   -13.68141951 -34.51559876 -67.68067983 #> 14   -13.23113238 -34.05539773 -66.77828439 #> 15   -12.80877469 -33.62374105 -65.93186079 #> 16   -12.41078784 -33.21699178 -65.13427743 #> 17   -12.03426309 -32.83217714 -64.37970508 #> 18   -11.67679137 -32.46683501 -63.66331584 #> 19   -11.33635417 -32.11890246 -62.98106456 #> 20   -11.01124290 -31.78663325 -62.32952709 #> 21   -10.69999799 -31.46853572 -61.70577846 #> 22   -10.40136247 -31.16332518 -61.10729958 #> 23   -10.11424576 -30.86988706 -60.53190492 #> 24    -9.83769537 -30.58724790 -59.97768563 #> 25    -9.57087431 -30.31455229 -59.44296437 #> 26    -9.31304305 -30.05104439 -58.92625904 #> 27    -9.06354475 -29.79605292 -58.42625335 #> 28    -8.82179333 -29.54897890 -57.94177276 #> 29    -8.58726358 -29.30928553 -57.47176469 #> 30    -8.35948289 -29.07648983 -57.01528205 #> 31    -8.13802443 -28.85015555 -56.57146946 #> 32    -7.92250139 -28.62988737 -56.13955169 #> 33    -7.71256203 -28.41532581 -55.71882387 #> 34    -7.50788557 -28.20614302 -55.30864314 #> 35    -7.30817862 -28.00203915 -54.90842153 #> 36    -7.11317211 -27.80273920 -54.51761980 #> 37    -6.92261864 -27.60799035 -54.13574218 #> 38    -6.73629024 -27.41755958 -53.76233176 #> 39    -6.55397631 -27.23123167 -53.39696653 #> 40    -6.37548191 -27.04880738 -53.03925581 #> 41    -6.20062624 -26.87010194 -52.68883727 #> 42    -6.02924127 -26.69494362 -52.34537417 #> 43    -5.86117058 -26.52317254 -52.00855303 #> 44    -5.69626827 -26.35463960 -51.67808147 #> 45    -5.53439806 -26.18920551 -51.35368636 #> 46    -5.37543243 -26.02673996 -51.03511216 #> 47    -5.21925189 -25.86712082 -50.72211942 #> 48    -5.06574433 -25.71023350 -50.41448343 #> 49    -4.91480437 -25.55597032 -50.11199304 #> 50    -4.76633288 -25.40422995 -49.81444956 #> 51    -4.62023645 -25.25491694 -49.52166582 #> 52    -4.47642699 -25.10794124 -49.23346526 #> 53    -4.33482129 -24.96321783 -48.94968113 #> 54    -4.19534069 -24.82066630 -48.67015580 #> 55    -4.05791074 -24.68021059 -48.39474008 #> 56    -3.92246092 -24.54177859 -48.12329261 #> 57    -3.78892433 -24.40530194 -47.85567935 #> 58    -3.65723748 -24.27071576 -47.59177303 #> 59    -3.52734003 -24.13795837 -47.33145276 #> 60    -3.39917460 -24.00697114 -47.07460353 #> 61    -3.27268657 -23.87769825 -46.82111591 #> 62    -3.14782393 -23.75008652 -46.57088562 #> 63    -3.02453707 -23.62408527 -46.32381326 #> 64    -2.90277865 -23.49964612 -46.07980397 #> 65    -2.78250349 -23.37672287 -45.83876719 #> 66    -2.66366839 -23.25527140 -45.60061638 #> 67    -2.54623207 -23.13524951 -45.36526877 #> 68    -2.43015501 -23.01661679 -45.13264517 #> 69    -2.31539937 -22.89933459 -44.90266975 #> 70    -2.20192888 -22.78336584 -44.67526985 #> 71    -2.08970880 -22.66867502 -44.45037581 #> 72    -1.97870577 -22.55522805 -44.22792080 #> 73    -1.86888777 -22.44299221 -44.00784065 #> 74    -1.76022407 -22.33193608 -43.79007376 #> 75    -1.65268511 -22.22202945 -43.57456090 #> 76    -1.54624248 -22.11324328 -43.36124513 #> 77    -1.44086885 -22.00554966 -43.15007168 #> 78    -1.33653790 -21.89892167 -42.94098782 #> 79    -1.23322430 -21.79333343 -42.73394277 #> 80    -1.13090363 -21.68875998 -42.52888760 #> 81    -1.02955236 -21.58517727 -42.32577514 #> 82    -0.92914779 -21.48256211 -42.12455992 #> 83    -0.82966803 -21.38089212 -41.92519805 #> 84    -0.73109193 -21.28014568 -41.72764716 #> 85    -0.63339908 -21.18030194 -41.53186634 #> 86    -0.53656976 -21.08134074 -41.33781607 #> 87    -0.44058490 -20.98324260 -41.14545815 #> 88    -0.34542608 -20.88598868 -40.95475563 #> 89    -0.25107546 -20.78956076 -40.76567280 #> 90    -0.15751581 -20.69394121 -40.57817509 #> 91    -0.06473041 -20.59911297 -40.39222901 #> 92     0.02729691 -20.50505950 -40.20780218 #> 93     0.11858180 -20.41176480 -40.02486319 #> 94     0.20913946 -20.31921335 -39.84338162 #> 95     0.29898459 -20.22739012 -39.66332799 #> 96     0.38813146 -20.13628051 -39.48467368 #> 97     0.47659393 -20.04587038 -39.30739097 #> 98     0.56438541 -19.95614600 -39.13145293 #> 99     0.65151895 -19.86709405 -38.95683343 #> 100    0.73800722 -19.77870159 -38.78350711 #> 101    0.82386249 -19.69095605 -38.61144932 #> 102    0.90909672 -19.60384523 -38.44063612 #> 103    0.99372152 -19.51735726 -38.27104425 #> 104    1.07774818 -19.43148060 -38.10265110 #> 105    1.16118766 -19.34620404 -37.93543467 #> 106    1.24405064 -19.26151668 -37.76937357 #> 107    1.32634750 -19.17740790 -37.60444700 #> 108    1.40808835 -19.09386737 -37.44063469 #> 109    1.48928303 -19.01088505 -37.27791695 #> 110    1.56994110 -18.92845114 -37.11627458 #> 111    1.65007191 -18.84655610 -36.95568888 #> 112    1.72968453 -18.76519066 -36.79614165 #> 113    1.80878781 -18.68434577 -36.63761514 #> 114    1.88739040 -18.60401260 -36.48009207 #> 115    1.96550069 -18.52418257 -36.32355557 #> 116    2.04312689 -18.44484728 -36.16798921 #> 117    2.12027700 -18.36599857 -36.01337696 #> 118    2.19695882 -18.28762846 -35.85970318 #> 119    2.27317996 -18.20972917 -35.70695262 #> 120    2.34894785 -18.13229310 -35.55511039 #> 121    2.42426975 -18.05531285 -35.40416195 #> 122    2.49915274 -17.97878118 -35.25409312 #> 123    2.57360372 -17.90269102 -35.10489004 #> 124    2.64762945 -17.82703547 -34.95653917 #> 125    2.72123654 -17.75180779 -34.80902730 #> 126    2.79443142 -17.67700139 -34.66234151 #> 127    2.86722039 -17.60260984 -34.51646917 #> 128    2.93960961 -17.52862683 -34.37139794 #> 129    3.01160511 -17.45504622 -34.22711577 #> 130    3.08321276 -17.38186199 -34.08361085 #> 131    3.15443832 -17.30906827 -33.94087165 #> 132    3.22528743 -17.23665928 -33.79888689 #> 133    3.29576559 -17.16462942 -33.65764552 #> 134    3.36587820 -17.09297315 -33.51713674 #> 135    3.43563052 -17.02168511 -33.37734998 #> 136    3.50502772 -16.95075999 -33.23827490 #> 137    3.57407486 -16.88019266 -33.09990136 #> 138    3.64277688 -16.80997803 -32.96221945 #> 139    3.71113864 -16.74011116 -32.82521945 #> 140    3.77916488 -16.67058720 -32.68889184 #> 141    3.84686025 -16.60140139 -32.55322731 #> 142    3.91422930 -16.53254908 -32.41821673 #> 143    3.98127651 -16.46402570 -32.28385114 #> 144    4.04800624 -16.39582679 -32.15012178 #> 145    4.11442279 -16.32794796 -32.01702006 #> 146    4.18053037 -16.26038490 -31.88453754 #> 147    4.24633308 -16.19313342 -31.75266597 #> 148    4.31183498 -16.12618937 -31.62139725 #> 149    4.37704004 -16.05954871 -31.49072342 #> 150    4.44195213 -15.99320746 -31.36063671 #> 151    4.50657507 -15.92716172 -31.23112945 #> 152    4.57091261 -15.86140767 -31.10219416 #> 153    4.63496842 -15.79594155 -30.97382346 #> 154    4.69874610 -15.73075968 -30.84601015 #> 155    4.76224920 -15.66585845 -30.71874712 #> 156    4.82548118 -15.60123430 -30.59202742 #> 157    4.88844545 -15.53688376 -30.46584422 #> 158    4.95114536 -15.47280339 -30.34019081 #> 159    5.01358420 -15.40898984 -30.21506060 #> 160    5.07576519 -15.34543982 -30.09044711 #> 161    5.13769152 -15.28215007 -29.96634400 #> 162    5.19936629 -15.21911742 -29.84274503 #> 163    5.26079256 -15.15633874 -29.71964405 #> 164    5.32197334 -15.09381095 -29.59703504 #> 165    5.38291158 -15.03153103 -29.47491209 #> 166    5.44361020 -14.96949602 -29.35326936 #> 167    5.50407204 -14.90770300 -29.23210114 #> 168    5.56429991 -14.84614910 -29.11140180 #> 169    5.62429657 -14.78483150 -28.99116583 #> 170    5.68406474 -14.72374743 -28.87138777 #> 171    5.74360707 -14.66289417 -28.75206229 #> 172    5.80292619 -14.60226902 -28.63318412 #> 173    5.86202469 -14.54186936 -28.51474811 #> 174    5.92090509 -14.48169260 -28.39674915 #> 175    5.97956990 -14.42173617 -28.27918226 #> 176    6.03802158 -14.36199757 -28.16204251 #> 177    6.09626253 -14.30247434 -28.04532505 #> 178    6.15429514 -14.24316404 -27.92902512 #> 179    6.21212174 -14.18406427 -27.81313804 #> 180    6.26974464 -14.12517269 -27.69765917 #> 181    6.32716611 -14.06648698 -27.58258399 #> 182    6.38438838 -14.00800486 -27.46790803 #> 183    6.44141364 -13.94972408 -27.35362686 #> 184    6.49824406 -13.89164244 -27.23973617 #> 185    6.55488177 -13.83375775 -27.12623169 #> 186    6.61132886 -13.77606787 -27.01310920 #> 187    6.66758741 -13.71857068 -26.90036458 #> 188    6.72365944 -13.66126413 -26.78799373 #> 189    6.77954695 -13.60414614 -26.67599266 #> 190    6.83525194 -13.54721471 -26.56435740 #> 191    6.89077633 -13.49046784 -26.45308404 #> 192    6.94612205 -13.43390359 -26.34216877 #> 193    7.00129098 -13.37752001 -26.23160778 #> 194    7.05628498 -13.32131521 -26.12139735 #> 195    7.11110589 -13.26528732 -26.01153380 #> 196    7.16575552 -13.20943448 -25.90201352 #> 197    7.22023564 -13.15375487 -25.79283292 #> 198    7.27454802 -13.09824671 -25.68398851 #> 199    7.32869438 -13.04290822 -25.57547679 #> 200    7.38267643 -12.98773765 -25.46729436 #> 201    7.43649585 -12.93273329 -25.35943784 #> 202    7.49015431 -12.87789344 -25.25190390 #> 203    7.54365344 -12.82321643 -25.14468927 #> 204    7.59699485 -12.76870061 -25.03779071 #> 205    7.65018014 -12.71434435 -24.93120503 #> 206    7.70321087 -12.66014605 -24.82492908 #> 207    7.75608859 -12.60610412 -24.71895976 #> 208    7.80881484 -12.55221701 -24.61329402 #> 209    7.86139112 -12.49848317 -24.50792882 #> 210    7.91381891 -12.44490108 -24.40286118 #> 211    7.96609969 -12.39146924 -24.29808818 #> 212    8.01823491 -12.33818617 -24.19360689 #> 213    8.07022598 -12.28505042 -24.08941447 #> 214    8.12207432 -12.23206054 -23.98550808 #> 215    8.17378133 -12.17921510 -23.88188493 #> 216    8.22534838 -12.12651271 -23.77854228 #> 217    8.27677682 -12.07395198 -23.67547739 #> 218    8.32806800 -12.02153153 -23.57268759 #> 219    8.37922323 -11.96925002 -23.47017023 #> 220    8.43024382 -11.91710612 -23.36792270 #> 221    8.48113106 -11.86509850 -23.26594240 #> 222    8.53188623 -11.81322587 -23.16422679 #> 223    8.58251057 -11.76148693 -23.06277335 #> 224    8.63300533 -11.70988043 -22.96157960 #> 225    8.68337174 -11.65840511 -22.86064307 #> 226    8.73361101 -11.60705973 -22.75996135 #> 227    8.78372433 -11.55584308 -22.65953202 #> 228    8.83371289 -11.50475393 -22.55935273 #> 229    8.88357785 -11.45379110 -22.45942114 #> 230    8.93332036 -11.40295342 -22.35973493 #> 231    8.98294156 -11.35223971 -22.26029182 #> 232    9.03244259 -11.30164882 -22.16108956 #> 233    9.08182455 -11.25117963 -22.06212592 #> 234    9.13108854 -11.20083100 -21.96339869 #> 235    9.18023565 -11.15060182 -21.86490569 #> 236    9.22926695 -11.10049101 -21.76664478 #> 237    9.27818351 -11.05049746 -21.66861383 #> 238    9.32698637 -11.00062012 -21.57081073 #> 239    9.37567657 -10.95085792 -21.47323341 #> 240    9.42425513 -10.90120981 -21.37587981 #> 241    9.47272307 -10.85167476 -21.27874790 #> 242    9.52108139 -10.80225174 -21.18183568 #> 243    9.56933108 -10.75293974 -21.08514115 #> 244    9.61747311 -10.70373777 -20.98866237 #> 245    9.66550847 -10.65464482 -20.89239737 #> 246    9.71343811 -10.60565993 -20.79634425 #> 247    9.76126296 -10.55678212 -20.70050111 #> 248    9.80898398 -10.50801043 -20.60486607 #> 249    9.85660209 -10.45934393 -20.50943726 #> 250    9.90411820 -10.41078166 -20.41421287 #> 251    9.95153321 -10.36232271 -20.31919106 #> 252    9.99884804 -10.31396617 -20.22437005 #> 253   10.04606356 -10.26571111 -20.12974805 #> 254   10.09318065 -10.21755665 -20.03532330 #> 255   10.14020018 -10.16950190 -19.94109407 #> 256   10.18712301 -10.12154597 -19.84705863 #> 257   10.23394999 -10.07368801 -19.75321528 #> 258   10.28068196 -10.02592715 -19.65956234 #> 259   10.32731975  -9.97826254 -19.56609813 #> 260   10.37386419  -9.93069334 -19.47282100 #> 261   10.42031609  -9.88321871 -19.37972932 #> 262   10.46667626  -9.83583784 -19.28682148 #> 263   10.51294550  -9.78854990 -19.19409587 #> 264   10.55912460  -9.74135408 -19.10155090 #> 265   10.60521433  -9.69424959 -19.00918501 #> 266   10.65121549  -9.64723563 -18.91699665 #> 267   10.69712883  -9.60031143 -18.82498428 #> 268   10.74295511  -9.55347620 -18.73314638 #> 269   10.78869508  -9.50672918 -18.64148144 #> 270   10.83434949  -9.46006960 -18.54998796 #> 271   10.87991908  -9.41349671 -18.45866448 #> 272   10.92540457  -9.36700977 -18.36750953 #> 273   10.97080670  -9.32060803 -18.27652165 #> 274   11.01612616  -9.27429077 -18.18569943 #> 275   11.06136368  -9.22805727 -18.09504142 #> 276   11.10651996  -9.18190679 -18.00454624 #> 277   11.15159568  -9.13583864 -17.91421248 #> 278   11.19659155  -9.08985211 -17.82403877 #> 279   11.24150823  -9.04394650 -17.73402373 #> 280   11.28634642  -8.99812112 -17.64416601 #> 281   11.33110677  -8.95237528 -17.55446427 #> 282   11.37578995  -8.90670832 -17.46491719 #> 283   11.42039663  -8.86111955 -17.37552343 #> 284   11.46492744  -8.81560831 -17.28628171 #> 285   11.50938303  -8.77017394 -17.19719072 #> 286   11.55376405  -8.72481579 -17.10824918 #> 287   11.59807112  -8.67953321 -17.01945583 #> 288   11.64230489  -8.63432557 -16.93080940 #> 289   11.68646595  -8.58919221 -16.84230866 #> 290   11.73055495  -8.54413251 -16.75395236 #> 291   11.77457247  -8.49914586 -16.66573927 #> 292   11.81851914  -8.45423162 -16.57766819 #> 293   11.86239555  -8.40938919 -16.48973791 #> 294   11.90620230  -8.36461796 -16.40194724 #> 295   11.94993998  -8.31991732 -16.31429500 #> 296   11.99360916  -8.27528667 -16.22678001 #> 297   12.03721044  -8.23072544 -16.13940111 #> 298   12.08074438  -8.18623301 -16.05215715 #> 299   12.12421156  -8.14180882 -15.96504699 #> 300   12.16761254  -8.09745229 -15.87806950 #> 301   12.21094788  -8.05316284 -15.79122354 #> 302   12.25421814  -8.00893991 -15.70450802 #> 303   12.29742386  -7.96478293 -15.61792182 #> 304   12.34056561  -7.92069134 -15.53146385 #> 305   12.38364390  -7.87666459 -15.44513303 #> 306   12.42665930  -7.83270214 -15.35892827 #> 307   12.46961232  -7.78880343 -15.27284851 #> 308   12.51250349  -7.74496792 -15.18689268 #> 309   12.55533334  -7.70119509 -15.10105975 #> 310   12.59810240  -7.65748440 -15.01534866 #> 311   12.64081117  -7.61383531 -14.92975839 #> 312   12.68346017  -7.57024732 -14.84428789 #> 313   12.72604990  -7.52671989 -14.75893617 #> 314   12.76858088  -7.48325252 -14.67370221 #> 315   12.81105359  -7.43984469 -14.58858500 #> 316   12.85346854  -7.39649589 -14.50358355 #> 317   12.89582622  -7.35320563 -14.41869688 #> 318   12.93812711  -7.30997341 -14.33392401 #> 319   12.98037170  -7.26679872 -14.24926396 #> 320   13.02256048  -7.22368108 -14.16471578 #> 321   13.06469391  -7.18062000 -14.08027851 #> 322   13.10677248  -7.13761500 -13.99595119 #> 323   13.14879665  -7.09466559 -13.91173289 #> 324   13.19076688  -7.05177130 -13.82762266 #> 325   13.23268365  -7.00893166 -13.74361960 #> 326   13.27454741  -6.96614619 -13.65972276 #> 327   13.31635862  -6.92341443 -13.57593124 #> 328   13.35811773  -6.88073592 -13.49224413 #> 329   13.39982519  -6.83811019 -13.40866052 #> 330   13.44148144  -6.79553680 -13.32517953 #> 331   13.48308694  -6.75301528 -13.24180027 #> 332   13.52464211  -6.71054519 -13.15852185 #> 333   13.56614741  -6.66812608 -13.07534340 #> 334   13.60760325  -6.62575751 -12.99226404 #> 335   13.64901007  -6.58343904 -12.90928293 #> 336   13.69036830  -6.54117023 -12.82639919 #> 337   13.73167837  -6.49895064 -12.74361197 #> 338   13.77294069  -6.45677985 -12.66092044 #> 339   13.81415569  -6.41465743 -12.57832375 #> 340   13.85532378  -6.37258295 -12.49582107 #> 341   13.89644537  -6.33055599 -12.41341157 #> 342   13.93752088  -6.28857613 -12.33109443 #> 343   13.97855071  -6.24664295 -12.24886882 #> 344   14.01953527  -6.20475604 -12.16673395 #> 345   14.06047496  -6.16291499 -12.08468899 #> 346   14.10137017  -6.12111939 -12.00273316 #> 347   14.14222132  -6.07936883 -11.92086565 #> 348   14.18302878  -6.03766292 -11.83908567 #> 349   14.22379296  -5.99600124 -11.75739245 #> 350   14.26451424  -5.95438340 -11.67578519 #> 351   14.30519301  -5.91280902 -11.59426312 #> 352   14.34582965  -5.87127768 -11.51282548 #> 353   14.38642456  -5.82978901 -11.43147149 #> 354   14.42697809  -5.78834261 -11.35020040 #> 355   14.46749064  -5.74693810 -11.26901144 #> 356   14.50796258  -5.70557509 -11.18790388 #> 357   14.54839428  -5.66425321 -11.10687695 #> 358   14.58878612  -5.62297208 -11.02592992 #> 359   14.62913845  -5.58173132 -10.94506205 #> 360   14.66945165  -5.54053055 -10.86427261 #> 361   14.70972608  -5.49936940 -10.78356086 #> 362   14.74996210  -5.45824751 -10.70292608 #> 363   14.79016007  -5.41716451 -10.62236756 #> 364   14.83032035  -5.37612002 -10.54188457 #> 365   14.87044329  -5.33511370 -10.46147640 #> 366   14.91052926  -5.29414517 -10.38114235 #> 367   14.95057859  -5.25321408 -10.30088171 #> 368   14.99059164  -5.21232007 -10.22069378 #> 369   15.03056875  -5.17146278 -10.14057786 #> 370   15.07051028  -5.13064187 -10.06053327 #> 371   15.11041656  -5.08985698  -9.98055931 #> 372   15.15028793  -5.04910776  -9.90065530 #> 373   15.19012474  -5.00839387  -9.82082056 #> 374   15.22992733  -4.96771495  -9.74105440 #> 375   15.26969602  -4.92707068  -9.66135617 #> 376   15.30943116  -4.88646070  -9.58172519 #> 377   15.34913308  -4.84588467  -9.50216078 #> 378   15.38880210  -4.80534226  -9.42266230 #> 379   15.42843855  -4.76483314  -9.34322907 #> 380   15.46804277  -4.72435695  -9.26386045 #> 381   15.50761508  -4.68391339  -9.18455579 #> 382   15.54715581  -4.64350210  -9.10531442 #> 383   15.58666526  -4.60312277  -9.02613571 #> 384   15.62614378  -4.56277507  -8.94701901 #> 385   15.66559166  -4.52245866  -8.86796369 #> 386   15.70500924  -4.48217323  -8.78896911 #> 387   15.74439683  -4.44191845  -8.71003463 #> 388   15.78375474  -4.40169400  -8.63115963 #> 389   15.82308328  -4.36149957  -8.55234348 #> 390   15.86238278  -4.32133483  -8.47358555 #> 391   15.90165352  -4.28119946  -8.39488522 #> 392   15.94089584  -4.24109315  -8.31624188 #> 393   15.98011002  -4.20101560  -8.23765491 #> 394   16.01929638  -4.16096648  -8.15912370 #> 395   16.05845522  -4.12094548  -8.08064764 #> 396   16.09758685  -4.08095230  -8.00222611 #> 397   16.13669156  -4.04098662  -7.92385853 #> 398   16.17576966  -4.00104815  -7.84554428 #> 399   16.21482144  -3.96113657  -7.76728277 #> 400   16.25384721  -3.92125158  -7.68907341 #> 401   16.29284725  -3.88139288  -7.61091559 #> 402   16.33182186  -3.84156017  -7.53280873 #> 403   16.37077134  -3.80175314  -7.45475224 #> 404   16.40969598  -3.76197150  -7.37674553 #> 405   16.44859607  -3.72221496  -7.29878802 #> 406   16.48747190  -3.68248321  -7.22087913 #> 407   16.52632376  -3.64277595  -7.14301828 #> 408   16.56515193  -3.60309291  -7.06520490 #> 409   16.60395670  -3.56343377  -6.98743840 #> 410   16.64273836  -3.52379826  -6.90971823 #> 411   16.68149720  -3.48418608  -6.83204380 #> 412   16.72023348  -3.44459694  -6.75441456 #> 413   16.75894751  -3.40503056  -6.67682993 #> 414   16.79763955  -3.36548664  -6.59928936 #> 415   16.83630989  -3.32596490  -6.52179228 #> 416   16.87495880  -3.28646506  -6.44433813 #> 417   16.91358657  -3.24698683  -6.36692637 #> 418   16.95219347  -3.20752992  -6.28955642 #> 419   16.99077978  -3.16809407  -6.21222775 #> 420   17.02934576  -3.12867898  -6.13493980 #> 421   17.06789170  -3.08928438  -6.05769202 #> 422   17.10641787  -3.04990999  -5.98048386 #> 423   17.14492454  -3.01055552  -5.90331478 #> 424   17.18341198  -2.97122071  -5.82618424 #> 425   17.22188046  -2.93190528  -5.74909170 #> 426   17.26033025  -2.89260895  -5.67203661 #> 427   17.29876161  -2.85333144  -5.59501844 #> 428   17.33717483  -2.81407249  -5.51803666 #> 429   17.37557015  -2.77483182  -5.44109072 #> 430   17.41394785  -2.73560916  -5.36418010 #> 431   17.45230820  -2.69640425  -5.28730426 #> 432   17.49065145  -2.65721680  -5.21046268 #> 433   17.52897787  -2.61804655  -5.13365483 #> 434   17.56728772  -2.57889323  -5.05688017 #> 435   17.60558127  -2.53975658  -4.98013820 #> 436   17.64385878  -2.50063633  -4.90342838 #> 437   17.68212049  -2.46153220  -4.82675019 #> 438   17.72036669  -2.42244395  -4.75010312 #> 439   17.75859762  -2.38337129  -4.67348663 #> 440   17.79681354  -2.34431398  -4.59690023 #> 441   17.83501471  -2.30527174  -4.52034338 #> 442   17.87320139  -2.26624431  -4.44381558 #> 443   17.91137383  -2.22723144  -4.36731632 #> 444   17.94953228  -2.18823285  -4.29084507 #> 445   17.98767701  -2.14924829  -4.21440134 #> 446   18.02580827  -2.11027751  -4.13798460 #> 447   18.06392630  -2.07132023  -4.06159436 #> 448   18.10203137  -2.03237621  -3.98523010 #> 449   18.14012373  -1.99344518  -3.90889133 #> 450   18.17820362  -1.95452688  -3.83257753 #> 451   18.21627131  -1.91562107  -3.75628820 #> 452   18.25432703  -1.87672748  -3.68002284 #> 453   18.29237104  -1.83784586  -3.60378095 #> 454   18.33040359  -1.79897595  -3.52756203 #> 455   18.36842493  -1.76011750  -3.45136557 #> 456   18.40643530  -1.72127026  -3.37519109 #> 457   18.44443496  -1.68243397  -3.29903808 #> 458   18.48242415  -1.64360837  -3.22290605 #> 459   18.52040313  -1.60479322  -3.14679450 #> 460   18.55837212  -1.56598827  -3.07070294 #> 461   18.59633139  -1.52719325  -2.99463087 #> 462   18.63428118  -1.48840793  -2.91857781 #> 463   18.67222174  -1.44963205  -2.84254325 #> 464   18.71015330  -1.41086535  -2.76652672 #> 465   18.74807611  -1.37210760  -2.69052771 #> 466   18.78599042  -1.33335853  -2.61454574 #> 467   18.82389647  -1.29461791  -2.53858033 #> 468   18.86179451  -1.25588547  -2.46263098 #> 469   18.89968477  -1.21716099  -2.38669720 #> 470   18.93756751  -1.17844419  -2.31077852 #> 471   18.97544296  -1.13973485  -2.23487444 #> 472   19.01331136  -1.10103270  -2.15898447 #> 473   19.05117296  -1.06233751  -2.08310815 #> 474   19.08902799  -1.02364903  -2.00724497 #> 475   19.12687670  -0.98496700  -1.93139446 #> 476   19.16471934  -0.94629119  -1.85555614 #> 477   19.20255613  -0.90762135  -1.77972952 #> 478   19.24038733  -0.86895723  -1.70391412 #> 479   19.27821316  -0.83029859  -1.62810946 #> 480   19.31603388  -0.79164518  -1.55231507 #> 481   19.35384972  -0.75299676  -1.47653045 #> 482   19.39166091  -0.71435308  -1.40075513 #> 483   19.42946771  -0.67571390  -1.32498863 #> 484   19.46727034  -0.63707897  -1.24923047 #> 485   19.50506905  -0.59844805  -1.17348017 #> 486   19.54286408  -0.55982090  -1.09773726 #> 487   19.58065566  -0.52119727  -1.02200126 #> 488   19.61844403  -0.48257691  -0.94627168 #> 489   19.65622943  -0.44395960  -0.87054806 #> 490   19.69401210  -0.40534507  -0.79482991 #> 491   19.73179227  -0.36673310  -0.71911675 #> 492   19.76957019  -0.32812343  -0.64340812 #> 493   19.80734609  -0.28951582  -0.56770353 #> 494   19.84512021  -0.25091003  -0.49200252 #> 495   19.88289279  -0.21230582  -0.41630459 #> 496   19.92066406  -0.17370294  -0.34060928 #> 497   19.95843427  -0.13510116  -0.26491611 #> 498   19.99620364  -0.09650022  -0.18922460 #> 499   20.03397242  -0.05789989  -0.11353429 #> 500   20.07174085  -0.01929992  -0.03784468 #> 501   20.10950915   0.01929992   0.03784468 #> 502   20.14727758   0.05789989   0.11353429 #> 503   20.18504636   0.09650022   0.18922460 #> 504   20.22281573   0.13510116   0.26491611 #> 505   20.26058594   0.17370294   0.34060928 #> 506   20.29835721   0.21230582   0.41630459 #> 507   20.33612979   0.25091003   0.49200252 #> 508   20.37390391   0.28951582   0.56770353 #> 509   20.41167981   0.32812343   0.64340812 #> 510   20.44945773   0.36673310   0.71911675 #> 511   20.48723790   0.40534507   0.79482991 #> 512   20.52502057   0.44395960   0.87054806 #> 513   20.56280597   0.48257691   0.94627168 #> 514   20.60059434   0.52119727   1.02200126 #> 515   20.63838592   0.55982090   1.09773726 #> 516   20.67618095   0.59844805   1.17348017 #> 517   20.71397966   0.63707897   1.24923047 #> 518   20.75178229   0.67571390   1.32498863 #> 519   20.78958909   0.71435308   1.40075513 #> 520   20.82740028   0.75299676   1.47653045 #> 521   20.86521612   0.79164518   1.55231507 #> 522   20.90303684   0.83029859   1.62810946 #> 523   20.94086267   0.86895723   1.70391412 #> 524   20.97869387   0.90762135   1.77972952 #> 525   21.01653066   0.94629119   1.85555614 #> 526   21.05437330   0.98496700   1.93139446 #> 527   21.09222201   1.02364903   2.00724497 #> 528   21.13007704   1.06233751   2.08310815 #> 529   21.16793864   1.10103270   2.15898447 #> 530   21.20580704   1.13973485   2.23487444 #> 531   21.24368249   1.17844419   2.31077852 #> 532   21.28156523   1.21716099   2.38669720 #> 533   21.31945549   1.25588547   2.46263098 #> 534   21.35735353   1.29461791   2.53858033 #> 535   21.39525958   1.33335853   2.61454574 #> 536   21.43317389   1.37210760   2.69052771 #> 537   21.47109670   1.41086535   2.76652672 #> 538   21.50902826   1.44963205   2.84254325 #> 539   21.54696882   1.48840793   2.91857781 #> 540   21.58491861   1.52719325   2.99463087 #> 541   21.62287788   1.56598827   3.07070294 #> 542   21.66084687   1.60479322   3.14679450 #> 543   21.69882585   1.64360837   3.22290605 #> 544   21.73681504   1.68243397   3.29903808 #> 545   21.77481470   1.72127026   3.37519109 #> 546   21.81282507   1.76011750   3.45136557 #> 547   21.85084641   1.79897595   3.52756203 #> 548   21.88887896   1.83784586   3.60378095 #> 549   21.92692297   1.87672748   3.68002284 #> 550   21.96497869   1.91562107   3.75628820 #> 551   22.00304638   1.95452688   3.83257753 #> 552   22.04112627   1.99344518   3.90889133 #> 553   22.07921863   2.03237621   3.98523010 #> 554   22.11732370   2.07132023   4.06159436 #> 555   22.15544173   2.11027751   4.13798460 #> 556   22.19357299   2.14924829   4.21440134 #> 557   22.23171772   2.18823285   4.29084507 #> 558   22.26987617   2.22723144   4.36731632 #> 559   22.30804861   2.26624431   4.44381558 #> 560   22.34623529   2.30527174   4.52034338 #> 561   22.38443646   2.34431398   4.59690023 #> 562   22.42265238   2.38337129   4.67348663 #> 563   22.46088331   2.42244395   4.75010312 #> 564   22.49912951   2.46153220   4.82675019 #> 565   22.53739122   2.50063633   4.90342838 #> 566   22.57566873   2.53975658   4.98013820 #> 567   22.61396228   2.57889323   5.05688017 #> 568   22.65227213   2.61804655   5.13365483 #> 569   22.69059855   2.65721680   5.21046268 #> 570   22.72894180   2.69640425   5.28730426 #> 571   22.76730215   2.73560916   5.36418010 #> 572   22.80567985   2.77483182   5.44109072 #> 573   22.84407517   2.81407249   5.51803666 #> 574   22.88248839   2.85333144   5.59501844 #> 575   22.92091975   2.89260895   5.67203661 #> 576   22.95936954   2.93190528   5.74909170 #> 577   22.99783802   2.97122071   5.82618424 #> 578   23.03632546   3.01055552   5.90331478 #> 579   23.07483213   3.04990999   5.98048386 #> 580   23.11335830   3.08928438   6.05769202 #> 581   23.15190424   3.12867898   6.13493980 #> 582   23.19047022   3.16809407   6.21222775 #> 583   23.22905653   3.20752992   6.28955642 #> 584   23.26766343   3.24698683   6.36692637 #> 585   23.30629120   3.28646506   6.44433813 #> 586   23.34494011   3.32596490   6.52179228 #> 587   23.38361045   3.36548664   6.59928936 #> 588   23.42230249   3.40503056   6.67682993 #> 589   23.46101652   3.44459694   6.75441456 #> 590   23.49975280   3.48418608   6.83204380 #> 591   23.53851164   3.52379826   6.90971823 #> 592   23.57729330   3.56343377   6.98743840 #> 593   23.61609807   3.60309291   7.06520490 #> 594   23.65492624   3.64277595   7.14301828 #> 595   23.69377810   3.68248321   7.22087913 #> 596   23.73265393   3.72221496   7.29878802 #> 597   23.77155402   3.76197150   7.37674553 #> 598   23.81047866   3.80175314   7.45475224 #> 599   23.84942814   3.84156017   7.53280873 #> 600   23.88840275   3.88139288   7.61091559 #> 601   23.92740279   3.92125158   7.68907341 #> 602   23.96642856   3.96113657   7.76728277 #> 603   24.00548034   4.00104815   7.84554428 #> 604   24.04455844   4.04098662   7.92385853 #> 605   24.08366315   4.08095230   8.00222611 #> 606   24.12279478   4.12094548   8.08064764 #> 607   24.16195362   4.16096648   8.15912370 #> 608   24.20113998   4.20101560   8.23765491 #> 609   24.24035416   4.24109315   8.31624188 #> 610   24.27959648   4.28119946   8.39488522 #> 611   24.31886722   4.32133483   8.47358555 #> 612   24.35816672   4.36149957   8.55234348 #> 613   24.39749526   4.40169400   8.63115963 #> 614   24.43685317   4.44191845   8.71003463 #> 615   24.47624076   4.48217323   8.78896911 #> 616   24.51565834   4.52245866   8.86796369 #> 617   24.55510622   4.56277507   8.94701901 #> 618   24.59458474   4.60312277   9.02613571 #> 619   24.63409419   4.64350210   9.10531442 #> 620   24.67363492   4.68391339   9.18455579 #> 621   24.71320723   4.72435695   9.26386045 #> 622   24.75281145   4.76483314   9.34322907 #> 623   24.79244790   4.80534226   9.42266230 #> 624   24.83211692   4.84588467   9.50216078 #> 625   24.87181884   4.88646070   9.58172519 #> 626   24.91155398   4.92707068   9.66135617 #> 627   24.95132267   4.96771495   9.74105440 #> 628   24.99112526   5.00839387   9.82082056 #> 629   25.03096207   5.04910776   9.90065530 #> 630   25.07083344   5.08985698   9.98055931 #> 631   25.11073972   5.13064187  10.06053327 #> 632   25.15068125   5.17146278  10.14057786 #> 633   25.19065836   5.21232007  10.22069378 #> 634   25.23067141   5.25321408  10.30088171 #> 635   25.27072074   5.29414517  10.38114235 #> 636   25.31080671   5.33511370  10.46147640 #> 637   25.35092965   5.37612002  10.54188457 #> 638   25.39108993   5.41716451  10.62236756 #> 639   25.43128790   5.45824751  10.70292608 #> 640   25.47152392   5.49936940  10.78356086 #> 641   25.51179835   5.54053055  10.86427261 #> 642   25.55211155   5.58173132  10.94506205 #> 643   25.59246388   5.62297208  11.02592992 #> 644   25.63285572   5.66425321  11.10687695 #> 645   25.67328742   5.70557509  11.18790388 #> 646   25.71375936   5.74693810  11.26901144 #> 647   25.75427191   5.78834261  11.35020040 #> 648   25.79482544   5.82978901  11.43147149 #> 649   25.83542035   5.87127768  11.51282548 #> 650   25.87605699   5.91280902  11.59426312 #> 651   25.91673576   5.95438340  11.67578519 #> 652   25.95745704   5.99600124  11.75739245 #> 653   25.99822122   6.03766292  11.83908567 #> 654   26.03902868   6.07936883  11.92086565 #> 655   26.07987983   6.12111939  12.00273316 #> 656   26.12077504   6.16291499  12.08468899 #> 657   26.16171473   6.20475604  12.16673395 #> 658   26.20269929   6.24664295  12.24886882 #> 659   26.24372912   6.28857613  12.33109443 #> 660   26.28480463   6.33055599  12.41341157 #> 661   26.32592622   6.37258295  12.49582107 #> 662   26.36709431   6.41465743  12.57832375 #> 663   26.40830931   6.45677985  12.66092044 #> 664   26.44957163   6.49895064  12.74361197 #> 665   26.49088170   6.54117023  12.82639919 #> 666   26.53223993   6.58343904  12.90928293 #> 667   26.57364675   6.62575751  12.99226404 #> 668   26.61510259   6.66812608  13.07534340 #> 669   26.65660789   6.71054519  13.15852185 #> 670   26.69816306   6.75301528  13.24180027 #> 671   26.73976856   6.79553680  13.32517953 #> 672   26.78142481   6.83811019  13.40866052 #> 673   26.82313227   6.88073592  13.49224413 #> 674   26.86489138   6.92341443  13.57593124 #> 675   26.90670259   6.96614619  13.65972276 #> 676   26.94856635   7.00893166  13.74361960 #> 677   26.99048312   7.05177130  13.82762266 #> 678   27.03245335   7.09466559  13.91173289 #> 679   27.07447752   7.13761500  13.99595119 #> 680   27.11655609   7.18062000  14.08027851 #> 681   27.15868952   7.22368108  14.16471578 #> 682   27.20087830   7.26679872  14.24926396 #> 683   27.24312289   7.30997341  14.33392401 #> 684   27.28542378   7.35320563  14.41869688 #> 685   27.32778146   7.39649589  14.50358355 #> 686   27.37019641   7.43984469  14.58858500 #> 687   27.41266912   7.48325252  14.67370221 #> 688   27.45520010   7.52671989  14.75893617 #> 689   27.49778983   7.57024732  14.84428789 #> 690   27.54043883   7.61383531  14.92975839 #> 691   27.58314760   7.65748440  15.01534866 #> 692   27.62591666   7.70119509  15.10105975 #> 693   27.66874651   7.74496792  15.18689268 #> 694   27.71163768   7.78880343  15.27284851 #> 695   27.75459070   7.83270214  15.35892827 #> 696   27.79760610   7.87666459  15.44513303 #> 697   27.84068439   7.92069134  15.53146385 #> 698   27.88382614   7.96478293  15.61792182 #> 699   27.92703186   8.00893991  15.70450802 #> 700   27.97030212   8.05316284  15.79122354 #> 701   28.01363746   8.09745229  15.87806950 #> 702   28.05703844   8.14180882  15.96504699 #> 703   28.10050562   8.18623301  16.05215715 #> 704   28.14403956   8.23072544  16.13940111 #> 705   28.18764084   8.27528667  16.22678001 #> 706   28.23131002   8.31991732  16.31429500 #> 707   28.27504770   8.36461796  16.40194724 #> 708   28.31885445   8.40938919  16.48973791 #> 709   28.36273086   8.45423162  16.57766819 #> 710   28.40667753   8.49914586  16.66573927 #> 711   28.45069505   8.54413251  16.75395236 #> 712   28.49478405   8.58919221  16.84230866 #> 713   28.53894511   8.63432557  16.93080940 #> 714   28.58317888   8.67953321  17.01945583 #> 715   28.62748595   8.72481579  17.10824918 #> 716   28.67186697   8.77017394  17.19719072 #> 717   28.71632256   8.81560831  17.28628171 #> 718   28.76085337   8.86111955  17.37552343 #> 719   28.80546005   8.90670832  17.46491719 #> 720   28.85014323   8.95237528  17.55446427 #> 721   28.89490358   8.99812112  17.64416601 #> 722   28.93974177   9.04394650  17.73402373 #> 723   28.98465845   9.08985211  17.82403877 #> 724   29.02965432   9.13583864  17.91421248 #> 725   29.07473004   9.18190679  18.00454624 #> 726   29.11988632   9.22805727  18.09504142 #> 727   29.16512384   9.27429077  18.18569943 #> 728   29.21044330   9.32060803  18.27652165 #> 729   29.25584543   9.36700977  18.36750953 #> 730   29.30133092   9.41349671  18.45866448 #> 731   29.34690051   9.46006960  18.54998796 #> 732   29.39255492   9.50672918  18.64148144 #> 733   29.43829489   9.55347620  18.73314638 #> 734   29.48412117   9.60031143  18.82498428 #> 735   29.53003451   9.64723563  18.91699665 #> 736   29.57603567   9.69424959  19.00918501 #> 737   29.62212540   9.74135408  19.10155090 #> 738   29.66830450   9.78854990  19.19409587 #> 739   29.71457374   9.83583784  19.28682148 #> 740   29.76093391   9.88321871  19.37972932 #> 741   29.80738581   9.93069334  19.47282100 #> 742   29.85393025   9.97826254  19.56609813 #> 743   29.90056804  10.02592715  19.65956234 #> 744   29.94730001  10.07368801  19.75321528 #> 745   29.99412699  10.12154597  19.84705863 #> 746   30.04104982  10.16950190  19.94109407 #> 747   30.08806935  10.21755665  20.03532330 #> 748   30.13518644  10.26571111  20.12974805 #> 749   30.18240196  10.31396617  20.22437005 #> 750   30.22971679  10.36232271  20.31919106 #> 751   30.27713180  10.41078166  20.41421287 #> 752   30.32464791  10.45934393  20.50943726 #> 753   30.37226602  10.50801043  20.60486607 #> 754   30.41998704  10.55678212  20.70050111 #> 755   30.46781189  10.60565993  20.79634425 #> 756   30.51574153  10.65464482  20.89239737 #> 757   30.56377689  10.70373777  20.98866237 #> 758   30.61191892  10.75293974  21.08514115 #> 759   30.66016861  10.80225174  21.18183568 #> 760   30.70852693  10.85167476  21.27874790 #> 761   30.75699487  10.90120981  21.37587981 #> 762   30.80557343  10.95085792  21.47323341 #> 763   30.85426363  11.00062012  21.57081073 #> 764   30.90306649  11.05049746  21.66861383 #> 765   30.95198305  11.10049101  21.76664478 #> 766   31.00101435  11.15060182  21.86490569 #> 767   31.05016146  11.20083100  21.96339869 #> 768   31.09942545  11.25117963  22.06212592 #> 769   31.14880741  11.30164882  22.16108956 #> 770   31.19830844  11.35223971  22.26029182 #> 771   31.24792964  11.40295342  22.35973493 #> 772   31.29767215  11.45379110  22.45942114 #> 773   31.34753711  11.50475393  22.55935273 #> 774   31.39752567  11.55584308  22.65953202 #> 775   31.44763899  11.60705973  22.75996135 #> 776   31.49787826  11.65840511  22.86064307 #> 777   31.54824467  11.70988043  22.96157960 #> 778   31.59873943  11.76148693  23.06277335 #> 779   31.64936377  11.81322587  23.16422679 #> 780   31.70011894  11.86509850  23.26594240 #> 781   31.75100618  11.91710612  23.36792270 #> 782   31.80202677  11.96925002  23.47017023 #> 783   31.85318200  12.02153153  23.57268759 #> 784   31.90447318  12.07395198  23.67547739 #> 785   31.95590162  12.12651271  23.77854228 #> 786   32.00746867  12.17921510  23.88188493 #> 787   32.05917568  12.23206054  23.98550808 #> 788   32.11102402  12.28505042  24.08941447 #> 789   32.16301509  12.33818617  24.19360689 #> 790   32.21515031  12.39146924  24.29808818 #> 791   32.26743109  12.44490108  24.40286118 #> 792   32.31985888  12.49848317  24.50792882 #> 793   32.37243516  12.55221701  24.61329402 #> 794   32.42516141  12.60610412  24.71895976 #> 795   32.47803913  12.66014605  24.82492908 #> 796   32.53106986  12.71434435  24.93120503 #> 797   32.58425515  12.76870061  25.03779071 #> 798   32.63759656  12.82321643  25.14468927 #> 799   32.69109569  12.87789344  25.25190390 #> 800   32.74475415  12.93273329  25.35943784 #> 801   32.79857357  12.98773765  25.46729436 #> 802   32.85255562  13.04290822  25.57547679 #> 803   32.90670198  13.09824671  25.68398851 #> 804   32.96101436  13.15375487  25.79283292 #> 805   33.01549448  13.20943448  25.90201352 #> 806   33.07014411  13.26528732  26.01153380 #> 807   33.12496502  13.32131521  26.12139735 #> 808   33.17995902  13.37752001  26.23160778 #> 809   33.23512795  13.43390359  26.34216877 #> 810   33.29047367  13.49046784  26.45308404 #> 811   33.34599806  13.54721471  26.56435740 #> 812   33.40170305  13.60414614  26.67599266 #> 813   33.45759056  13.66126413  26.78799373 #> 814   33.51366259  13.71857068  26.90036458 #> 815   33.56992114  13.77606787  27.01310920 #> 816   33.62636823  13.83375775  27.12623169 #> 817   33.68300594  13.89164244  27.23973617 #> 818   33.73983636  13.94972408  27.35362686 #> 819   33.79686162  14.00800486  27.46790803 #> 820   33.85408389  14.06648698  27.58258399 #> 821   33.91150536  14.12517269  27.69765917 #> 822   33.96912826  14.18406427  27.81313804 #> 823   34.02695486  14.24316404  27.92902512 #> 824   34.08498747  14.30247434  28.04532505 #> 825   34.14322842  14.36199757  28.16204251 #> 826   34.20168010  14.42173617  28.27918226 #> 827   34.26034491  14.48169260  28.39674915 #> 828   34.31922531  14.54186936  28.51474811 #> 829   34.37832381  14.60226902  28.63318412 #> 830   34.43764293  14.66289417  28.75206229 #> 831   34.49718526  14.72374743  28.87138777 #> 832   34.55695343  14.78483150  28.99116583 #> 833   34.61695009  14.84614910  29.11140180 #> 834   34.67717796  14.90770300  29.23210114 #> 835   34.73763980  14.96949602  29.35326936 #> 836   34.79833842  15.03153103  29.47491209 #> 837   34.85927666  15.09381095  29.59703504 #> 838   34.92045744  15.15633874  29.71964405 #> 839   34.98188371  15.21911742  29.84274503 #> 840   35.04355848  15.28215007  29.96634400 #> 841   35.10548481  15.34543982  30.09044711 #> 842   35.16766580  15.40898984  30.21506060 #> 843   35.23010464  15.47280339  30.34019081 #> 844   35.29280455  15.53688376  30.46584422 #> 845   35.35576882  15.60123430  30.59202742 #> 846   35.41900080  15.66585845  30.71874712 #> 847   35.48250390  15.73075968  30.84601015 #> 848   35.54628158  15.79594155  30.97382346 #> 849   35.61033739  15.86140767  31.10219416 #> 850   35.67467493  15.92716172  31.23112945 #> 851   35.73929787  15.99320746  31.36063671 #> 852   35.80420996  16.05954871  31.49072342 #> 853   35.86941502  16.12618937  31.62139725 #> 854   35.93491692  16.19313342  31.75266597 #> 855   36.00071963  16.26038490  31.88453754 #> 856   36.06682721  16.32794796  32.01702006 #> 857   36.13324376  16.39582679  32.15012178 #> 858   36.19997349  16.46402570  32.28385114 #> 859   36.26702070  16.53254908  32.41821673 #> 860   36.33438975  16.60140139  32.55322731 #> 861   36.40208512  16.67058720  32.68889184 #> 862   36.47011136  16.74011116  32.82521945 #> 863   36.53847312  16.80997803  32.96221945 #> 864   36.60717514  16.88019266  33.09990136 #> 865   36.67622228  16.95075999  33.23827490 #> 866   36.74561948  17.02168511  33.37734998 #> 867   36.81537180  17.09297315  33.51713674 #> 868   36.88548441  17.16462942  33.65764552 #> 869   36.95596257  17.23665928  33.79888689 #> 870   37.02681168  17.30906827  33.94087165 #> 871   37.09803724  17.38186199  34.08361085 #> 872   37.16964489  17.45504622  34.22711577 #> 873   37.24164039  17.52862683  34.37139794 #> 874   37.31402961  17.60260984  34.51646917 #> 875   37.38681858  17.67700139  34.66234151 #> 876   37.46001346  17.75180779  34.80902730 #> 877   37.53362055  17.82703547  34.95653917 #> 878   37.60764628  17.90269102  35.10489004 #> 879   37.68209726  17.97878118  35.25409312 #> 880   37.75698025  18.05531285  35.40416195 #> 881   37.83230215  18.13229310  35.55511039 #> 882   37.90807004  18.20972917  35.70695262 #> 883   37.98429118  18.28762846  35.85970318 #> 884   38.06097300  18.36599857  36.01337696 #> 885   38.13812311  18.44484728  36.16798921 #> 886   38.21574931  18.52418257  36.32355557 #> 887   38.29385960  18.60401260  36.48009207 #> 888   38.37246219  18.68434577  36.63761514 #> 889   38.45156547  18.76519066  36.79614165 #> 890   38.53117809  18.84655610  36.95568888 #> 891   38.61130890  18.92845114  37.11627458 #> 892   38.69196697  19.01088505  37.27791695 #> 893   38.77316165  19.09386737  37.44063469 #> 894   38.85490250  19.17740790  37.60444700 #> 895   38.93719936  19.26151668  37.76937357 #> 896   39.02006234  19.34620404  37.93543467 #> 897   39.10350182  19.43148060  38.10265110 #> 898   39.18752848  19.51735726  38.27104425 #> 899   39.27215328  19.60384523  38.44063612 #> 900   39.35738751  19.69095605  38.61144932 #> 901   39.44324278  19.77870159  38.78350711 #> 902   39.52973105  19.86709405  38.95683343 #> 903   39.61686459  19.95614600  39.13145293 #> 904   39.70465607  20.04587038  39.30739097 #> 905   39.79311854  20.13628051  39.48467368 #> 906   39.88226541  20.22739012  39.66332799 #> 907   39.97211054  20.31921335  39.84338162 #> 908   40.06266820  20.41176480  40.02486319 #> 909   40.15395309  20.50505950  40.20780218 #> 910   40.24598041  20.59911297  40.39222901 #> 911   40.33876581  20.69394121  40.57817509 #> 912   40.43232546  20.78956076  40.76567280 #> 913   40.52667608  20.88598868  40.95475563 #> 914   40.62183490  20.98324260  41.14545815 #> 915   40.71781976  21.08134074  41.33781607 #> 916   40.81464908  21.18030194  41.53186634 #> 917   40.91234193  21.28014568  41.72764716 #> 918   41.01091803  21.38089212  41.92519805 #> 919   41.11039779  21.48256211  42.12455992 #> 920   41.21080236  21.58517727  42.32577514 #> 921   41.31215363  21.68875998  42.52888760 #> 922   41.41447430  21.79333343  42.73394277 #> 923   41.51778790  21.89892167  42.94098782 #> 924   41.62211885  22.00554966  43.15007168 #> 925   41.72749248  22.11324328  43.36124513 #> 926   41.83393511  22.22202945  43.57456090 #> 927   41.94147407  22.33193608  43.79007376 #> 928   42.05013777  22.44299221  44.00784065 #> 929   42.15995577  22.55522805  44.22792080 #> 930   42.27095880  22.66867502  44.45037581 #> 931   42.38317888  22.78336584  44.67526985 #> 932   42.49664937  22.89933459  44.90266975 #> 933   42.61140501  23.01661679  45.13264517 #> 934   42.72748207  23.13524951  45.36526877 #> 935   42.84491839  23.25527140  45.60061638 #> 936   42.96375349  23.37672287  45.83876719 #> 937   43.08402865  23.49964612  46.07980397 #> 938   43.20578707  23.62408527  46.32381326 #> 939   43.32907393  23.75008652  46.57088562 #> 940   43.45393657  23.87769825  46.82111591 #> 941   43.58042460  24.00697114  47.07460353 #> 942   43.70859003  24.13795837  47.33145276 #> 943   43.83848748  24.27071576  47.59177303 #> 944   43.97017433  24.40530194  47.85567935 #> 945   44.10371092  24.54177859  48.12329261 #> 946   44.23916074  24.68021059  48.39474008 #> 947   44.37659069  24.82066630  48.67015580 #> 948   44.51607129  24.96321783  48.94968113 #> 949   44.65767699  25.10794124  49.23346526 #> 950   44.80148645  25.25491694  49.52166582 #> 951   44.94758288  25.40422995  49.81444956 #> 952   45.09605437  25.55597032  50.11199304 #> 953   45.24699433  25.71023350  50.41448343 #> 954   45.40050189  25.86712082  50.72211942 #> 955   45.55668243  26.02673996  51.03511216 #> 956   45.71564806  26.18920551  51.35368636 #> 957   45.87751827  26.35463960  51.67808147 #> 958   46.04242058  26.52317254  52.00855303 #> 959   46.21049127  26.69494362  52.34537417 #> 960   46.38187624  26.87010194  52.68883727 #> 961   46.55673191  27.04880738  53.03925581 #> 962   46.73522631  27.23123167  53.39696653 #> 963   46.91754024  27.41755958  53.76233176 #> 964   47.10386864  27.60799035  54.13574218 #> 965   47.29442211  27.80273920  54.51761980 #> 966   47.48942862  28.00203915  54.90842153 #> 967   47.68913557  28.20614302  55.30864314 #> 968   47.89381203  28.41532581  55.71882387 #> 969   48.10375139  28.62988737  56.13955169 #> 970   48.31927443  28.85015555  56.57146946 #> 971   48.54073289  29.07648983  57.01528205 #> 972   48.76851358  29.30928553  57.47176469 #> 973   49.00304333  29.54897890  57.94177276 #> 974   49.24479475  29.79605292  58.42625335 #> 975   49.49429305  30.05104439  58.92625904 #> 976   49.75212431  30.31455229  59.44296437 #> 977   50.01894537  30.58724790  59.97768563 #> 978   50.29549576  30.86988706  60.53190492 #> 979   50.58261247  31.16332518  61.10729958 #> 980   50.88124799  31.46853572  61.70577846 #> 981   51.19249290  31.78663325  62.32952709 #> 982   51.51760417  32.11890246  62.98106456 #> 983   51.85804137  32.46683501  63.66331584 #> 984   52.21551309  32.83217714  64.37970508 #> 985   52.59203784  33.21699178  65.13427743 #> 986   52.99002469  33.62374105  65.93186079 #> 987   53.41238238  34.05539773  66.77828439 #> 988   53.86266951  34.51559876  67.68067983 #> 989   54.34530635  35.00886175  68.64790554 #> 990   54.86588134  35.54089816  69.69116098 #> 991   55.43160829  36.11908064  70.82490296 #> 992   56.05203383  36.75316600  72.06826334 #> 993   56.74018057  37.45646357  73.44734004 #> 994   57.51449626  38.24782726  74.99910312 #> 995   58.40240924  39.15528929  76.77852025 #> 996   59.44741155  40.22329926  78.87275136 #> 997   60.72497566  41.52899133  81.43304669 #> 998   62.38524176  43.22581126  84.76029381 #> 999   64.80663077  45.70051165  89.61286514 #> 1000  69.67020919  50.67117077  99.35969268 # }"},{"path":"https://easystats.github.io/bayestestR/reference/simulate_simpson.html","id":null,"dir":"Reference","previous_headings":"","what":"Simpson's paradox dataset simulation — simulate_simpson","title":"Simpson's paradox dataset simulation — simulate_simpson","text":"Simpson's paradox, Yule-Simpson effect, phenomenon probability statistics, trend appears several different groups data disappears reverses groups combined.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/simulate_simpson.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simpson's paradox dataset simulation — simulate_simpson","text":"","code":"simulate_simpson(   n = 100,   r = 0.5,   groups = 3,   difference = 1,   group_prefix = \"G_\" )"},{"path":"https://easystats.github.io/bayestestR/reference/simulate_simpson.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simpson's paradox dataset simulation — simulate_simpson","text":"n number observations group generated (minimum 4). r value vector corresponding desired correlation coefficients. groups Number groups (groups can participants, clusters, anything). difference Difference groups. group_prefix prefix group name (e.g., \"G_1\", \"G_2\", \"G_3\", ...).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/simulate_simpson.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simpson's paradox dataset simulation — simulate_simpson","text":"dataset.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/simulate_simpson.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simpson's paradox dataset simulation — simulate_simpson","text":"","code":"data <- simulate_simpson(n = 10, groups = 5, r = 0.5)  if (require(\"ggplot2\")) {   ggplot(data, aes(x = V1, y = V2)) +     geom_point(aes(color = Group)) +     geom_smooth(aes(color = Group), method = \"lm\") +     geom_smooth(method = \"lm\") } #> `geom_smooth()` using formula = 'y ~ x' #> `geom_smooth()` using formula = 'y ~ x'"},{"path":"https://easystats.github.io/bayestestR/reference/spi.html","id":null,"dir":"Reference","previous_headings":"","what":"Shortest Probability Interval (SPI) — spi","title":"Shortest Probability Interval (SPI) — spi","text":"Compute Shortest Probability Interval (SPI) posterior distributions. SPI computationally stable HDI. implementation based algorithm SPIn package.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/spi.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Shortest Probability Interval (SPI) — spi","text":"","code":"spi(x, ...)  # S3 method for numeric spi(x, ci = 0.95, verbose = TRUE, ...)  # S3 method for stanreg spi(   x,   ci = 0.95,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"location\", \"all\", \"conditional\", \"smooth_terms\", \"sigma\",     \"distributional\", \"auxiliary\"),   parameters = NULL,   verbose = TRUE,   ... )  # S3 method for brmsfit spi(   x,   ci = 0.95,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"conditional\", \"zi\", \"zero_inflated\", \"all\"),   parameters = NULL,   verbose = TRUE,   ... )"},{"path":"https://easystats.github.io/bayestestR/reference/spi.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Shortest Probability Interval (SPI) — spi","text":"x Vector representing posterior distribution, data frame vectors. Can also Bayesian model. bayestestR supports wide range models (see, example, methods(\"hdi\")) documented 'Usage' section, methods classes mostly resemble arguments .numeric .data.framemethods. ... Currently used. ci Value vector probability (credible) interval - CI (0 1) estimated. Default .95 (95%). verbose Toggle warnings. effects results fixed effects, random effects returned? applies mixed models. May abbreviated. component results parameters, parameters conditional model zero-inflated part model returned? May abbreviated. applies brms-models. parameters Regular expression pattern describes parameters returned. Meta-parameters (like lp__ prior_) filtered default, parameters typically appear summary() returned. Use parameters select specific parameters output.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/spi.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Shortest Probability Interval (SPI) — spi","text":"data frame following columns: Parameter model parameter(s), x model-object. x vector, column missing. CI probability credible interval. CI_low, CI_high lower upper credible interval limits parameters.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/spi.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Shortest Probability Interval (SPI) — spi","text":"SPI alternative method HDI (hdi()) quantify uncertainty (posterior) distributions. SPI said stable HDI, , \"HDI can noisy (, high Monte Carlo error)\" (Liu et al. 2015). Furthermore, HDI sensitive additional assumptions, particular assumptions related different estimation methods, can make HDI less accurate reliable (see also discussion ).","code":""},{"path":"https://easystats.github.io/bayestestR/reference/spi.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Shortest Probability Interval (SPI) — spi","text":"code compute SPI adapted SPIn package, slightly modified robust Stan models. Thus, credits go Ying Liu original SPI algorithm R implementation.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/spi.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Shortest Probability Interval (SPI) — spi","text":"Liu, Y., Gelman, ., & Zheng, T. (2015). Simulation-efficient shortest probability intervals. Statistics Computing, 25(4), 809–819. https://doi.org/10.1007/s11222-015-9563-8","code":""},{"path":[]},{"path":"https://easystats.github.io/bayestestR/reference/spi.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Shortest Probability Interval (SPI) — spi","text":"","code":"library(bayestestR)  posterior <- rnorm(1000) spi(posterior) #> 95% SPI: [-1.69, 2.01] spi(posterior, ci = c(0.80, 0.89, 0.95)) #> Shortest Probability Interval #>  #> 80% SPI       |       89% SPI |       95% SPI #> --------------------------------------------- #> [-1.20, 1.18] | [-1.60, 1.40] | [-1.69, 2.01]  df <- data.frame(replicate(4, rnorm(100))) spi(df) #> Shortest Probability Interval #>  #> Parameter |       95% SPI #> ------------------------- #> X1        | [-1.68, 1.90] #> X2        | [-1.75, 1.95] #> X3        | [-2.39, 1.68] #> X4        | [-1.73, 2.01] spi(df, ci = c(0.80, 0.89, 0.95)) #> Shortest Probability Interval #>  #> Parameter |       80% SPI |       89% SPI |       95% SPI #> --------------------------------------------------------- #> X1        | [-1.45, 0.76] | [-1.52, 1.45] | [-1.68, 1.90] #> X2        | [-0.76, 1.61] | [-1.50, 1.61] | [-1.75, 1.95] #> X3        | [-1.22, 0.86] | [-1.24, 1.68] | [-2.39, 1.68] #> X4        | [-0.73, 1.35] | [-1.65, 1.35] | [-1.73, 2.01] # \\dontrun{ library(rstanarm) model <- stan_glm(mpg ~ wt + gear, data = mtcars, chains = 2, iter = 200, refresh = 0) #> Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#bulk-ess #> Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable. #> Running the chains for more iterations may help. See #> https://mc-stan.org/misc/warnings.html#tail-ess spi(model) #> Shortest Probability Interval  #>  #> Parameter   |        95% SPI #> ---------------------------- #> (Intercept) | [30.62, 47.80] #> wt          | [-6.54, -4.28] #> gear        | [-1.64,  1.61] # }"},{"path":"https://easystats.github.io/bayestestR/reference/unupdate.html","id":null,"dir":"Reference","previous_headings":"","what":"Un-update Bayesian models to their prior-to-data state — unupdate","title":"Un-update Bayesian models to their prior-to-data state — unupdate","text":"posteriors priors updated observing data, goal function un-update posteriors obtain models representing priors. models can used examine prior predictive distribution, compare priors posteriors.  function used internally compute Bayes factors.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/unupdate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Un-update Bayesian models to their prior-to-data state — unupdate","text":"","code":"unupdate(model, verbose = TRUE, ...)  # S3 method for stanreg unupdate(model, verbose = TRUE, ...)  # S3 method for brmsfit unupdate(model, verbose = TRUE, ...)  # S3 method for brmsfit_multiple unupdate(model, verbose = TRUE, newdata = NULL, ...)  # S3 method for blavaan unupdate(model, verbose = TRUE, ...)"},{"path":"https://easystats.github.io/bayestestR/reference/unupdate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Un-update Bayesian models to their prior-to-data state — unupdate","text":"model fitted Bayesian model. verbose Toggle warnings. ... used newdata List data.frames update model new data. Required even original data used.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/unupdate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Un-update Bayesian models to their prior-to-data state — unupdate","text":"model un-fitted data, representing prior model.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/weighted_posteriors.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate posterior distributions weighted across models — weighted_posteriors","title":"Generate posterior distributions weighted across models — weighted_posteriors","text":"Extract posterior samples parameters, weighted across models. Weighting done comparing posterior model probabilities, via bayesfactor_models().","code":""},{"path":"https://easystats.github.io/bayestestR/reference/weighted_posteriors.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate posterior distributions weighted across models — weighted_posteriors","text":"","code":"weighted_posteriors(..., prior_odds = NULL, missing = 0, verbose = TRUE)  # S3 method for data.frame weighted_posteriors(..., prior_odds = NULL, missing = 0, verbose = TRUE)  # S3 method for stanreg weighted_posteriors(   ...,   prior_odds = NULL,   missing = 0,   verbose = TRUE,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"conditional\", \"zi\", \"zero_inflated\", \"all\"),   parameters = NULL )  # S3 method for brmsfit weighted_posteriors(   ...,   prior_odds = NULL,   missing = 0,   verbose = TRUE,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"conditional\", \"zi\", \"zero_inflated\", \"all\"),   parameters = NULL )  # S3 method for blavaan weighted_posteriors(   ...,   prior_odds = NULL,   missing = 0,   verbose = TRUE,   effects = c(\"fixed\", \"random\", \"all\"),   component = c(\"conditional\", \"zi\", \"zero_inflated\", \"all\"),   parameters = NULL )  # S3 method for BFBayesFactor weighted_posteriors(   ...,   prior_odds = NULL,   missing = 0,   verbose = TRUE,   iterations = 4000 )"},{"path":"https://easystats.github.io/bayestestR/reference/weighted_posteriors.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate posterior distributions weighted across models — weighted_posteriors","text":"... Fitted models (see details), fit data, single BFBayesFactor object. prior_odds Optional vector prior odds models compared first model (denominator, BFBayesFactor objects). data.frames, used basis weighting. missing optional numeric value use model contain parameter appears models. Defaults 0. verbose Toggle warnings. effects results fixed effects, random effects returned? applies mixed models. May abbreviated. component results parameters, parameters conditional model zero-inflated part model returned? May abbreviated. applies brms-models. parameters Regular expression pattern describes parameters returned. Meta-parameters (like lp__ prior_) filtered default, parameters typically appear summary() returned. Use parameters select specific parameters output. iterations BayesFactor models, many posterior samples draw.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/weighted_posteriors.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate posterior distributions weighted across models — weighted_posteriors","text":"data frame posterior distributions (weighted across models) .","code":""},{"path":"https://easystats.github.io/bayestestR/reference/weighted_posteriors.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate posterior distributions weighted across models — weighted_posteriors","text":"Note across models parameters might play different roles. example, parameter plays different role model Y ~ + B (main effect) model Y ~ + B + :B (simple effect). many cases centering predictors (mean subtracting continuous variables, effects coding via contr.sum orthonormal coding via contr.equalprior_pairs factors) can reduce issue. case mindful issue.  See bayesfactor_models() details info passed models.  Note BayesFactor models, posterior samples generated intercept models.  function similar function brms::posterior_average.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/weighted_posteriors.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Generate posterior distributions weighted across models — weighted_posteriors","text":"BayesFactor < 0.9.12-4.3, instances might problems duplicate columns random effects resulting data frame.","code":""},{"path":"https://easystats.github.io/bayestestR/reference/weighted_posteriors.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Generate posterior distributions weighted across models — weighted_posteriors","text":"Clyde, M., Desimone, H., & Parmigiani, G. (1996). Prediction via orthogonalized model mixing. Journal American Statistical Association, 91(435), 1197-1208. Hinne, M., Gronau, Q. F., van den Bergh, D., Wagenmakers, E. (2019, March 25). conceptual introduction Bayesian Model Averaging. doi:10.31234/osf.io/wgb64 Rouder, J. N., Haaf, J. M., & Vandekerckhove, J. (2018). Bayesian inference psychology, part IV: Parameter estimation Bayes factors. Psychonomic bulletin & review, 25(1), 102-113. van den Bergh, D., Haaf, J. M., Ly, ., Rouder, J. N., & Wagenmakers, E. J. (2019). cautionary note estimating effect size.","code":""},{"path":[]},{"path":"https://easystats.github.io/bayestestR/reference/weighted_posteriors.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate posterior distributions weighted across models — weighted_posteriors","text":"","code":"# \\donttest{ if (require(\"rstanarm\") && require(\"see\")) {   stan_m0 <- stan_glm(extra ~ 1,     data = sleep,     family = gaussian(),     refresh = 0,     diagnostic_file = file.path(tempdir(), \"df0.csv\")   )    stan_m1 <- stan_glm(extra ~ group,     data = sleep,     family = gaussian(),     refresh = 0,     diagnostic_file = file.path(tempdir(), \"df1.csv\")   )    res <- weighted_posteriors(stan_m0, stan_m1)    plot(eti(res)) } #> Warning: Bayes factors might not be precise. #>   For precise Bayes factors, sampling at least 40,000 posterior samples is #>   recommended. #> Computation of Marginal Likelihood: estimating marginal likelihood, please wait... #> Computation of Marginal Likelihood: estimating marginal likelihood, please wait... #> Error: Failed at retrieving data :( Please provide original model or data through the `data` argument  ## With BayesFactor if (require(\"BayesFactor\")) {   extra_sleep <- ttestBF(formula = extra ~ group, data = sleep)    wp <- weighted_posteriors(extra_sleep)    describe_posterior(extra_sleep, test = NULL)   describe_posterior(wp$delta, test = NULL) # also considers the null } #> Warning: Could not estimate a good default ROPE range. Using 'c(-0.1, 0.1)'. #> Summary of Posterior Distribution #>  #> Parameter | Median |        95% CI #> ---------------------------------- #> Posterior |  -0.10 | [-1.41, 0.03]   ## weighted prediction distributions via data.frames if (require(\"rstanarm\")) {   m0 <- stan_glm(     mpg ~ 1,     data = mtcars,     family = gaussian(),     diagnostic_file = file.path(tempdir(), \"df0.csv\"),     refresh = 0   )    m1 <- stan_glm(     mpg ~ carb,     data = mtcars,     family = gaussian(),     diagnostic_file = file.path(tempdir(), \"df1.csv\"),     refresh = 0   )    # Predictions:   pred_m0 <- data.frame(posterior_predict(m0))   pred_m1 <- data.frame(posterior_predict(m1))    BFmods <- bayesfactor_models(m0, m1)    wp <- weighted_posteriors(pred_m0, pred_m1,     prior_odds = as.numeric(BFmods)[2]   )    # look at first 5 prediction intervals   hdi(pred_m0[1:5])   hdi(pred_m1[1:5])   hdi(wp[1:5]) # between, but closer to pred_m1 } #> Warning: Bayes factors might not be precise. #>   For precise Bayes factors, sampling at least 40,000 posterior samples is #>   recommended. #> Computation of Marginal Likelihood: estimating marginal likelihood, please wait... #> Computation of Marginal Likelihood: estimating marginal likelihood, please wait... #> Error in UseMethod(\"hdi_\"): no applicable method for 'hdi_' applied to an object of class \"data.frame\" # }"},{"path":[]},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"changes-0-13-1","dir":"Changelog","previous_headings":"","what":"Changes","title":"bayestestR 0.13.1","text":"Improved speed performance functions called using .call(). Improved speed performance bayesfactor_models() brmsfit objects already included marglik element model object.","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"new-functionality-0-13-1","dir":"Changelog","previous_headings":"","what":"New functionality","title":"bayestestR 0.13.1","text":".logical() bayesfactor_restricted() results, extracts boolean vector(s) mark draws part order restriction.","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bug-fixes-0-13-1","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"bayestestR 0.13.1","text":"p_map() gains new null argument specify non-0 nulls. Fixed non-working examples ci(method = \"SI\"). Fixed wrong calculation rope range model objects describe_posterior().","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bayestestr-0130","dir":"Changelog","previous_headings":"","what":"bayestestR 0.13.0","title":"bayestestR 0.13.0","text":"CRAN release: 2022-09-18","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"breaking-0-13-0","dir":"Changelog","previous_headings":"","what":"Breaking","title":"bayestestR 0.13.0","text":"minimum needed R version bumped 3.6. contr.equalprior(contrasts = FALSE) (previously contr.orthonorm) longer returns identity matrix, shifted diag(n) - 1/n, consistency.","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"new-functionality-0-13-0","dir":"Changelog","previous_headings":"","what":"New functionality","title":"bayestestR 0.13.0","text":"p_to_bf(), convert p-values Bayes factors. accurate approximate Bayes factors, use bic_to_bf(). bayestestR now supports objects class rvar package posterior. contr.equalprior (previously contr.orthonorm) gains two new functions: contr.equalprior_pairs contr.equalprior_deviations aide setting intuitive priors.","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"changes-0-13-0","dir":"Changelog","previous_headings":"","what":"Changes","title":"bayestestR 0.13.0","text":"renamed contr.equalprior explicit function. p_direction() now accepts objects class parameters_model() (parameters::model_parameters()), compute probability direction parameters frequentist models.","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bayestestr-0121","dir":"Changelog","previous_headings":"","what":"bayestestR 0.12.1","title":"bayestestR 0.12.1","text":"CRAN release: 2022-05-02","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"breaking-0-12-1","dir":"Changelog","previous_headings":"","what":"Breaking","title":"bayestestR 0.12.1","text":"Bayesfactor_models() frequentist models now relies updated insight::get_loglikelihood(). might change results REML based models. See documentation. estimate_density() argument group_by renamed . distribution_*(random = FALSE) functions now rely ppoints(), result slightly different results, especially small ns. Uncertainty estimation now defaults \"eti\" (formerly \"hdi\").","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"changes-0-12-1","dir":"Changelog","previous_headings":"","what":"Changes","title":"bayestestR 0.12.1","text":"bayestestR functions now support draws objects package posterior. rope_range() now handles log(normal)-families models log-transformed outcomes. New function spi(), compute shortest probability intervals. Furthermore, \"spi\" option added new method compute uncertainty intervals.","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bug-fixes-0-12-1","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"bayestestR 0.12.1","text":"bci() objects incorrectly returned equal-tailed intervals.","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bayestestr-0115","dir":"Changelog","previous_headings":"","what":"bayestestR 0.11.5","title":"bayestestR 0.11.5","text":"CRAN release: 2021-10-30 Fixes failing tests CRAN checks.","code":""},{"path":[]},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"new-functions-0-11-1","dir":"Changelog","previous_headings":"","what":"New functions","title":"bayestestR 0.11.1","text":"describe_posterior() gains plot() method, short cut plot(estimate_density(describe_posterior())).","code":""},{"path":[]},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bug-fixes-0-11","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"bayestestR 0.11","text":"Fixed issues related last brms update. Fixed bug describe_posterior.BFBayesFactor() Bayes factors missing put ( #442 ).","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bayestestr-0100","dir":"Changelog","previous_headings":"","what":"bayestestR 0.10.0","title":"bayestestR 0.10.0","text":"CRAN release: 2021-05-31","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"breaking-0-10-0","dir":"Changelog","previous_headings":"","what":"Breaking","title":"bayestestR 0.10.0","text":"Bayes factors now returned log(BF) (column name log_BF). Printing unaffected. retrieve raw BFs, can run exp(result$log_BF).","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"new-functions-0-10-0","dir":"Changelog","previous_headings":"","what":"New functions","title":"bayestestR 0.10.0","text":"bci() (alias bcai()) compute bias-corrected accelerated bootstrap intervals. Along new function, ci() describe_posterior() gain new ci_method type, \"bci\".","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"changes-0-10-0","dir":"Changelog","previous_headings":"","what":"Changes","title":"bayestestR 0.10.0","text":"contr.bayes renamed contr.orthonorm explicit function.","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bayestestr-090","dir":"Changelog","previous_headings":"","what":"bayestestR 0.9.0","title":"bayestestR 0.9.0","text":"CRAN release: 2021-04-08","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"breaking-0-9-0","dir":"Changelog","previous_headings":"","what":"Breaking","title":"bayestestR 0.9.0","text":"default ci width changed 0.95 instead 0.89 (see ). come surprise long-time users bayestestR warning impending change now :) Column names bayesfactor_restricted() now p_prior p_posterior (Prior_prob Posterior_prob), consistent bayesfactor_inclusion() output. Removed experimental function mhdior.","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"general-0-9-0","dir":"Changelog","previous_headings":"","what":"General","title":"bayestestR 0.9.0","text":"Support blavaan models. Support blrm models (rmsb). Support BGGM models (BGGM). check_prior() describe_prior() now also work ways prior definition models rstanarm brms.","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bug-fixes-0-9-0","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"bayestestR 0.9.0","text":"Fixed bug print() method mediation() function. Fixed remaining inconsistencies CI values, reported fraction rope(). Fixed issues special prior definitions check_prior(), describe_prior() simulate_prior().","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bayestestr-082","dir":"Changelog","previous_headings":"","what":"bayestestR 0.8.2","title":"bayestestR 0.8.2","text":"CRAN release: 2021-01-26","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"general-0-8-2","dir":"Changelog","previous_headings":"","what":"General","title":"bayestestR 0.8.2","text":"Support bamlss models. Roll-back R dependency R >= 3.4.","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"changes-to-functions-0-8-2","dir":"Changelog","previous_headings":"","what":"Changes to functions","title":"bayestestR 0.8.2","text":".stanreg methods gain component argument, also include auxiliary parameters.","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bug-fixes-0-8-2","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"bayestestR 0.8.2","text":"bayesfactor_parameters() longer errors reason computing extremely un/likely direction hypotheses. bayesfactor_pointull() / bf_pointull() now bayesfactor_pointnull() / bf_pointnull() (can spot difference? #363 ).","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bayestestr-080","dir":"Changelog","previous_headings":"","what":"bayestestR 0.8.0","title":"bayestestR 0.8.0","text":"CRAN release: 2020-12-05","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"new-functions-0-8-0","dir":"Changelog","previous_headings":"","what":"New functions","title":"bayestestR 0.8.0","text":"sexit(), function sequential effect existence significance testing (SEXIT).","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"general-0-8-0","dir":"Changelog","previous_headings":"","what":"General","title":"bayestestR 0.8.0","text":"Added startup-message warn users default ci-width might change future update. Added support mcmc.list objects.","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bug-fixes-0-8-0","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"bayestestR 0.8.0","text":"unupdate() gains newdata argument work brmsfit_multiple models. Fixed issue Bayes factor vignette (don’t evaluate code chunks packages available).","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bayestestr-075","dir":"Changelog","previous_headings":"","what":"bayestestR 0.7.5","title":"bayestestR 0.7.5","text":"CRAN release: 2020-10-22","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"new-functions-0-7-5","dir":"Changelog","previous_headings":"","what":"New functions","title":"bayestestR 0.7.5","text":"Added .matrix() function bayesfactor_model arrays. unupdate(), utility function get Bayesian models un-fitted data, representing priors .","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"changes-to-functions-0-7-5","dir":"Changelog","previous_headings":"","what":"Changes to functions","title":"bayestestR 0.7.5","text":"ci() supports emmeans - Bayesian frequentist ( #312 - cross fix parameters)","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bug-fixes-0-7-5","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"bayestestR 0.7.5","text":"Fixed issue default rope range BayesFactor models. Fixed issue collinearity-check rope() models less two parameters. Fixed issue print-method mediation() stanmvreg-models, displays wrong name response-value. Fixed issue effective_sample() models one parameter. rope_range() BayesFactor models returns non-NA values ( #343 )","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bayestestr-072","dir":"Changelog","previous_headings":"","what":"bayestestR 0.7.2","title":"bayestestR 0.7.2","text":"CRAN release: 2020-07-20","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"new-functions-0-7-2","dir":"Changelog","previous_headings":"","what":"New functions","title":"bayestestR 0.7.2","text":"mediation(), compute average direct average causal mediation effects multivariate response models (brmsfit, stanmvreg).","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bug-fixes-0-7-2","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"bayestestR 0.7.2","text":"bayesfactor_parameters() works R<3.6.0.","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bayestestr-070","dir":"Changelog","previous_headings":"","what":"bayestestR 0.7.0","title":"bayestestR 0.7.0","text":"CRAN release: 2020-06-19","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"general-0-7-0","dir":"Changelog","previous_headings":"","what":"General","title":"bayestestR 0.7.0","text":"Preliminary support stanfit objects. Added support bayesQR objects.","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"changes-to-functions-0-7-0","dir":"Changelog","previous_headings":"","what":"Changes to functions","title":"bayestestR 0.7.0","text":"weighted_posteriors() can now used data frames. Revised print() describe_posterior(). Improved value formatting Bayesfactor functions.","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bug-fixes-0-7-0","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"bayestestR 0.7.0","text":"Link transformation now taken account emmeans objets. E.g., describe_posterior(). Fix diagnostic_posterior() algorithm “sampling”. Minor revisions documentations. Fix CRAN check issues win-old-release.","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bayestestr-060","dir":"Changelog","previous_headings":"","what":"bayestestR 0.6.0","title":"bayestestR 0.6.0","text":"CRAN release: 2020-04-20","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"changes-to-functions-0-6-0","dir":"Changelog","previous_headings":"","what":"Changes to functions","title":"bayestestR 0.6.0","text":"describe_posterior() now also works effectsize::standardize_posteriors(). p_significance() now also works parameters::simulate_model(). rope_range() supports (frequentis) models.","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bug-fixes-0-6-0","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"bayestestR 0.6.0","text":"Fixed issue plot() data.frame-methods p_direction() equivalence_test(). Fix check issues forthcoming insight-update.","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bayestestr-053","dir":"Changelog","previous_headings":"","what":"bayestestR 0.5.3","title":"bayestestR 0.5.3","text":"CRAN release: 2020-03-26","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"general-0-5-3","dir":"Changelog","previous_headings":"","what":"General","title":"bayestestR 0.5.3","text":"Support bcplm objects (package cplm)","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"changes-to-functions-0-5-3","dir":"Changelog","previous_headings":"","what":"Changes to functions","title":"bayestestR 0.5.3","text":"estimate_density() now also works grouped data frames.","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bug-fixes-0-5-3","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"bayestestR 0.5.3","text":"Fixed bug weighted_posteriors() properly weight Intercept-BFBayesFactor models. Fixed bug weighted_posteriors() models low posterior probability ( #286 ). Fixed bug describe_posterior(), rope() equivalence_test() brmsfit models monotonic effect. Fixed issues related latest changes .data.frame.brmsfit() brms package.","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bayestestr-050","dir":"Changelog","previous_headings":"","what":"bayestestR 0.5.0","title":"bayestestR 0.5.0","text":"CRAN release: 2020-01-18","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"general-0-5-0","dir":"Changelog","previous_headings":"","what":"General","title":"bayestestR 0.5.0","text":"Added p_pointnull() alias p_MAP(). Added si() function compute support intervals. Added weighted_posteriors() generating posterior samples averaged across models. Added plot()-method p_significance(). p_significance() now also works brmsfit-objects. estimate_density() now also works MCMCglmm-objects. equivalence_test() gets effects component arguments stanreg brmsfit models, print specific model components. Support mcmc objects (package coda) Provide distributions via distribution(). Added distribution_tweedie(). Better handling stanmvreg models describe_posterior(), diagnostic_posterior() describe_prior().","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"breaking-changes-0-5-0","dir":"Changelog","previous_headings":"","what":"Breaking changes","title":"bayestestR 0.5.0","text":"point_estimate(): argument centrality default value changed ‘median’ ‘’. p_rope(), previously exploratory index, renamed mhdior() (Max HDI inside/outside ROPE), p_rope() refer rope(..., ci =   1) ( #258 )","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bug-fixes-0-5-0","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"bayestestR 0.5.0","text":"Fixed mistake description p_significance(). Fixed error computing BFs emmGrid based non-linear models ( #260 ). Fixed wrong output percentage-values print.equivalence_test(). Fixed issue describe_posterior() BFBayesFactor-objects one model.","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bayestestr-040","dir":"Changelog","previous_headings":"","what":"bayestestR 0.4.0","title":"bayestestR 0.4.0","text":"CRAN release: 2019-10-20","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"new-functions--features-0-4-0","dir":"Changelog","previous_headings":"","what":"New functions / features","title":"bayestestR 0.4.0","text":"convert_bayesian_to_frequentist() Convert (refit) Bayesian model frequentist distribution_binomial() perfect binomial distributions simulate_ttest() Simulate data mean difference simulate_correlation() Simulate correlated datasets p_significance() Compute probability Practical Significance (ps) overlap() Compute overlap two empirical distributions estimate_density(): method = \"mixture\" argument added mixture density estimation","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bug-fixes-0-4-0","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"bayestestR 0.4.0","text":"Fixed bug simulate_prior() stanreg-models autoscale set FALSE","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bayestestr-030","dir":"Changelog","previous_headings":"","what":"bayestestR 0.3.0","title":"bayestestR 0.3.0","text":"CRAN release: 2019-09-22","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"general-0-3-0","dir":"Changelog","previous_headings":"","what":"General","title":"bayestestR 0.3.0","text":"revised print()-methods functions like rope(), p_direction(), describe_posterior() etc., particular model objects random effects /zero-inflation component","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"new-functions--features-0-3-0","dir":"Changelog","previous_headings":"","what":"New functions / features","title":"bayestestR 0.3.0","text":"check_prior() check prior informative simulate_prior() simulate model’s priors distributions distribution_gamma() generate (near-perfect random) Gamma distribution contr.bayes function orthogonal factor coding (implementation Singmann & Gronau’s bfrms, used proper prior estimation factor 3 levels . See Bayes factor vignette ## Changes functions Added support sim, sim.merMod (arm::sim()) MCMCglmm-objects many functions (like hdi(), ci(), eti(), rope(), p_direction(), point_estimate(), …) describe_posterior() gets effects component argument, include description posterior samples random effects /zero-inflation component. user-friendly warning non-supported models bayesfactor()-methods","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bug-fixes-0-3-0","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"bayestestR 0.3.0","text":"Fixed bug bayesfactor_inclusion() interaction sometimes appeared (#223) Fixed bug describe_posterior() stanreg models fitted fullrank-algorithm","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bayestestr-025","dir":"Changelog","previous_headings":"","what":"bayestestR 0.2.5","title":"bayestestR 0.2.5","text":"CRAN release: 2019-08-06","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"breaking-changes-0-2-5","dir":"Changelog","previous_headings":"","what":"Breaking changes","title":"bayestestR 0.2.5","text":"rope_range() binomial model now different default (-.18; .18 ; instead -.055; .055) rope(): returns proportion (0 1) instead value 0 100 p_direction(): returns proportion (0.5 1) instead value 50 100 (#168) bayesfactor_savagedickey(): hypothesis argument replaced null part new bayesfactor_parameters() function","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"new-functions--features-0-2-5","dir":"Changelog","previous_headings":"","what":"New functions / features","title":"bayestestR 0.2.5","text":"density_at(), p_map() map_estimate(): method argument added rope(): ci_method argument added eti(): Computes equal-tailed intervals reshape_ci(): Reshape CIs wide/long bayesfactor_parameters(): New function, replacing bayesfactor_savagedickey(), allows computing Bayes factors point-null interval-null bayesfactor_restricted(): Function computing Bayes factors order restricted models","code":""},{"path":[]},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bug-fixes-0-2-5","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"bayestestR 0.2.5","text":"bayesfactor_inclusion() now works R < 3.6.","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bayestestr-022","dir":"Changelog","previous_headings":"","what":"bayestestR 0.2.2","title":"bayestestR 0.2.2","text":"CRAN release: 2019-06-20","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"breaking-changes-0-2-2","dir":"Changelog","previous_headings":"","what":"Breaking changes","title":"bayestestR 0.2.2","text":"equivalence_test(): returns capitalized output (e.g., Rejected instead rejected) describe_posterior.numeric(): dispersion defaults FALSE consistency methods","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"new-functions--features-0-2-2","dir":"Changelog","previous_headings":"","what":"New functions / features","title":"bayestestR 0.2.2","text":"pd_to_p() p_to_pd(): Functions convert probability direction (pd) p-value Support emmGrid objects: ci(), rope(), bayesfactor_savagedickey(), describe_posterior(), …","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"minor-changes-0-2-2","dir":"Changelog","previous_headings":"","what":"Minor changes","title":"bayestestR 0.2.2","text":"Improved tutorial 2","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bug-fixes-0-2-2","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"bayestestR 0.2.2","text":"describe_posterior(): Fixed column order restoration bayesfactor_inclusion(): Inclusion BFs matched models inline JASP results.","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bayestestr-020","dir":"Changelog","previous_headings":"","what":"bayestestR 0.2.0","title":"bayestestR 0.2.0","text":"CRAN release: 2019-05-29","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"breaking-changes-0-2-0","dir":"Changelog","previous_headings":"","what":"Breaking changes","title":"bayestestR 0.2.0","text":"plotting functions now require installation see package estimate argument name describe_posterior() point_estimate() changed centrality hdi(), ci(), rope() equivalence_test() default ci 0.89 rnorm_perfect() deprecated favour distribution_normal() map_estimate() now returns single value instead dataframe density parameter removed. MAP density value now accessible via attributes(map_output)$MAP_density","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"new-functions--features-0-2-0","dir":"Changelog","previous_headings":"","what":"New functions / features","title":"bayestestR 0.2.0","text":"describe_posterior(), describe_prior(), diagnostic_posterior(): added wrapper function point_estimate() added function compute point estimates p_direction(): new argument method compute pd based AUC area_under_curve(): compute AUC distribution() functions added bayesfactor_savagedickey(), bayesfactor_models() bayesfactor_inclusion() functions added Started adding plotting methods (currently see package) p_direction() hdi() probability_at() alias density_at() effective_sample() return effective sample size Stan-models mcse() return Monte Carlo standard error Stan-models","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"minor-changes-0-2-0","dir":"Changelog","previous_headings":"","what":"Minor changes","title":"bayestestR 0.2.0","text":"Improved documentation Improved testing p_direction(): improved printing rope() model-objects now returns HDI values parameters attribute consistent way Changes legend-labels plot.equivalence_test() align plots output print()-method (#78)","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bug-fixes-0-2-0","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"bayestestR 0.2.0","text":"hdi() returned multiple class attributes (#72) Printing results hdi() failed ci-argument fractional parts percentage values (e.g. ci = 0.995). plot.equivalence_test() work properly brms-models (#76).","code":""},{"path":"https://easystats.github.io/bayestestR/news/index.html","id":"bayestestr-010","dir":"Changelog","previous_headings":"","what":"bayestestR 0.1.0","title":"bayestestR 0.1.0","text":"CRAN release: 2019-04-08 CRAN initial publication 0.1.0 release Added NEWS.md file track changes package","code":""}]
